[
    {
        "context": "# If the answer is not fully inside the context, label is (0, 0)\n    if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n        start_positions.append(0)\n        end_positions.append(0)\n    else:\n        # Otherwise it's the start and end token positions\n        idx = context_start\n        while idx <= context_end and offset[idx][0] <= start_char:\n            idx += 1\n        start_positions.append(idx - 1)\n\n        idx = context_end\n        while idx >= context_start and offset[idx][1] >= end_char:\n            idx -= 1\n        end_positions.append(idx + 1)\n\nstart_positions, end_positions\n```\n\n```python out\n([83, 51, 19, 0, 0, 64, 27, 0, 34, 0, 0, 0, 67, 34, 0, 0, 0, 0, 0],\n [85, 53, 21, 0, 0, 70, 33, 0, 40, 0, 0, 0, 68, 35, 0, 0, 0, 0, 0])\n```\n\nLet's take a look at a few results to verify that our approach is correct. For the first feature we find `(83, 85)` as labels, so let's compare the theoretical answer with the decoded span of tokens from 83 to 85 (inclusive):\n\n```py\nidx = 0\nsample_idx = inputs[\"overflow_to_sample_mapping\"][idx]\nanswer = answers[sample_idx][\"text\"][0]\n\nstart = start_positions[idx]\nend = end_positions[idx]\nlabeled_answer = tokenizer.decode(inputs[\"input_ids\"][idx][start : end + 1])\n\nprint(f\"Theoretical answer: {answer}, labels give: {labeled_answer}\")\n```\n\n```python out\n'Theoretical answer: the Main Building, labels give: the Main Building'\n```\n\nSo that's a match! Now let's check index 4, where we set the labels to `(0, 0)`, which means the answer is not in the context chunk of that feature:\n\n```py\nidx = 4\nsample_idx = inputs[\"overflow_to_sample_mapping\"][idx]\nanswer = answers[sample_idx][\"text\"][0]\n\ndecoded_example = tokenizer.decode(inputs[\"input_ids\"][idx])\nprint(f\"Theoretical answer: {answer}, decoded example: {decoded_example}\")\n```",
        "question": "(your factoid question)\n",
        "answer": " (your answer to the factoid question)\n\nNow here is the context.\n\nContext: # If the answer is not fully inside the context, label is (0, 0)\n    if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n        start_positions.append(0)\n        end_positions.append(0)\n    else:\n        # Otherwise it's the start and end token positions\n        idx = context_start\n        while idx <= context_end and offset[idx][0] <= start_char:\n            idx += 1\n        start_positions.append(idx - 1)\n\n        idx = context_end\n        while idx >= context_start and offset[idx][1] >= end_char:\n            idx -= 1\n        end_positions.append(idx + 1)\n\nstart_positions, end_positions\n```\n\n```python out\n([83, 51, 19, 0, 0, 64, 27, 0, 34, 0, 0, 0, 67, 34, 0, 0, 0, 0, 0],\n [85, 53, 21, 0, 0, 70, 33, 0, 40, 0, 0, 0, 68, 35, 0, 0, 0, 0, 0])\n```\n\nLet's take a look at a few results to verify that our approach is correct. For the first feature we find `(83, 85)` as labels, so let's compare the theoretical answer with the decoded span of tokens from 83 to 85 (inclusive):\n\n```py\nidx = 0\nsample_idx = inputs[\"overflow_to_sample_mapping\"][idx]\nanswer = answers[sample_idx][\"text\"][0]\n\nstart = start_positions[idx]\nend = end_positions[idx]\nlabeled_answer = tokenizer.decode(inputs[\"input_ids\"][idx][start : end + 1])\n\nprint(f\"Theoretical answer: {answer}, labels give: {labeled_answer}\")\n```\n\n```python out\n'Theoretical answer: the Main Building, labels give: the Main Building'\n```\n\nSo that's a match! Now let's check index 4, where we set the labels to `(0, 0)`, which means the answer is not in the context chunk of that feature:\n\n```py\nidx = 4\nsample_idx = inputs[\"overflow_to_sample_mapping\"][idx]\nanswer = answers[sample_idx][\"text\"][0]\n\ndecoded_example = tokenizer.decode(inputs[\"input_ids\"][idx])\nprint(f\"Theoretical answer: {answer}, decoded example: {decoded_example}\")\n```\n\nOutput:::\nTheoretical answer: the, decoded example: the\n\n```\n\nGreat, that's a match too! Let's check the last example, where the labels are `(0, 0)`:\n\n```py\nidx = 10\nsample_idx = inputs[\"overflow_to_sample_mapping\"][idx]\nanswer = answers[sample_idx][\"text\"][0]\n\ndecoded_example = tokenizer.decode(inputs[\"input_ids\"][idx])\nprint(f\"Theoretical answer: {answer}, decoded example: {decoded_example}\")\n```\n\nOutput:::\nTheoretical answer: the, decoded example: the\n\n```\n\nThat's not a match, but that's expected. The theoretical answer is the first word in the context, but the decoded example is the first word of the entire input.\n\nNow let's check the first feature again, where the labels are `(83, 51)`:\n\n```py\nidx = 0\nsample_idx = inputs[\"overflow_to_sample_mapping\"][idx]\nanswer = answers[sample_idx][\"text\"][0]\n\nstart = start_positions[idx]\nend = end_positions[idx]\nlabeled_answer = tokenizer.decode(inputs[\"input_ids\"][start : end + 1])\n\nprint(f\"Theoretical answer: {answer}, labels give: {labeled_answer}\")\n```\n\nOutput:::\nTheoretical answer: the, labels give: the Main Building\n\n```\n\nThat's a match! Now let's check the second feature, where the labels are `(19, 27)`:\n",
        "source_doc": "huggingface/course/blob/main/chapters/en/chapter7/7.mdx"
    },
    {
        "context": "Unlike our Colab example, however, this example is designed to be **scalable** and much closer to a realistic training run -- although we only use a BERT-sized model by default, the code could be expanded to a much larger model and a much more powerful TPU pod slice by changing a few configuration options.\n\n## Motivation\n\nWhy are we writing this guide now? After all, 🤗 Transformers has had support for TensorFlow for several years now. But getting those models to train on TPUs has been a major pain point for the community. This is because:\n\n- Many models weren’t XLA-compatible\n- Data collators didn’t use native TF operations\n\nWe think XLA is the future: It’s the core compiler for JAX, it has first-class support in TensorFlow, and you can even use it from [PyTorch](https://github.com/pytorch/xla). As such, we’ve made a [big push](https://blog.tensorflow.org/2022/11/how-hugging-face-improved-text-generation-performance-with-xla.html) to make our codebase XLA compatible and to remove any other roadblocks standing in the way of XLA and TPU compatibility. This means users should be able to train most of our TensorFlow models on TPUs without hassle.\n\nThere’s also another important reason to care about TPU training right now: Recent major advances in LLMs and generative AI have created huge public interest in model training, and so it’s become incredibly hard for most people to get access to state-of-the-art GPUs. Knowing how to train on TPU gives you another path to access ultra-high-performance compute hardware, which is much more dignified than losing a bidding war for the last H100 on eBay and then ugly crying at your desk. You deserve better. And speaking from experience: Once you get comfortable with training on TPU, you might not want to go back.\n\n## What to expect",
        "question": "What is the reason for the push for XLA compatibility?\n",
        "answer": " The push for XLA compatibility is to make the codebase XLA compatible and to remove any other roadblocks standing in the way of XLA and TPU compatibility. This is because XLA is the core compiler for JAX, it has first-class support in TensorFlow, and you can even use it from PyTorch.",
        "source_doc": "huggingface/blog/blob/main/tf_tpu.md"
    },
    {
        "context": "image = res.images[0]\nimage.save('./seed_resize/seed_resize_{w}_{h}_image_compare.png'.format(w=width, h=height))\n```\n\n### Multilingual Stable Diffusion Pipeline\n\nThe following code can generate an images from texts in different languages using the pre-trained [mBART-50 many-to-one multilingual machine translation model](https://huggingface.co/facebook/mbart-large-50-many-to-one-mmt) and Stable Diffusion.\n\n```python\nfrom PIL import Image\n\nimport torch\n\nfrom diffusers import DiffusionPipeline\nfrom transformers import (\n    pipeline,\n    MBart50TokenizerFast,\n    MBartForConditionalGeneration,\n)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice_dict = {\"cuda\": 0, \"cpu\": -1}\n\n# helper function taken from: https://huggingface.co/blog/stable_diffusion\ndef image_grid(imgs, rows, cols):\n    assert len(imgs) == rows*cols\n\n    w, h = imgs[0].size\n    grid = Image.new('RGB', size=(cols*w, rows*h))\n    grid_w, grid_h = grid.size\n\n    for i, img in enumerate(imgs):\n        grid.paste(img, box=(i%cols*w, i//cols*h))\n    return grid\n\n# Add language detection pipeline\nlanguage_detection_model_ckpt = \"papluca/xlm-roberta-base-language-detection\"\nlanguage_detection_pipeline = pipeline(\"text-classification\",\n                                       model=language_detection_model_ckpt,\n                                       device=device_dict[device])\n\n# Add model for language translation\ntrans_tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-one-mmt\")\ntrans_model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-one-mmt\").to(device)\n\ndiffuser_pipeline = DiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\",\n    custom_pipeline=\"multilingual_stable_diffusion\",\n    detection_pipeline=language_detection_pipeline,\n    translation_model=trans_model,\n    translation_tokenizer=trans_tokenizer,\n\n    torch_dtype=torch.float16,\n)",
        "question": "What is the name of the translation model used in the pipeline?\n",
        "answer": " The name of the translation model used in the pipeline is `facebook/mbart-large-50-many-to-one-mmt\n================================================================================================\n\nThe name of the translation model used in the pipeline is `facebook/mbart-large-50-many-to-one-mmt`.\n\n```",
        "source_doc": "huggingface/diffusers/blob/main/examples/community/README.md"
    },
    {
        "context": "!---\nCopyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n# Language model training examples in streaming mode\n\nThe following examples showcase how to train a language model from scratch \nusing the JAX/Flax backend.\n\nJAX/Flax allows you to trace pure functions and compile them into efficient, fused accelerator code on both GPU and TPU.\nModels written in JAX/Flax are **immutable** and updated in a purely functional\nway which enables simple and efficient model parallelism.\n\nAll of the following examples make use of [dataset streaming](https://huggingface.co/docs/datasets/master/dataset_streaming), therefore allowing to train models on massive datasets\\\nwithout ever having to download the full dataset.\n\n## Masked language modeling\n\nIn the following, we demonstrate how to train a bi-directional transformer model \nusing masked language modeling objective as introduced in [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805).\nMore specifically, we demonstrate how JAX/Flax and dataset streaming can be leveraged \nto pre-train [**`roberta-base`**](https://huggingface.co/roberta-base)\nin English on a single TPUv3-8 pod for 10000 update steps.\n\nThe example script uses the 🤗 Datasets library. You can easily customize them to your needs if you need extra processing on your datasets.",
        "question": "What is the name of the model pre-trained in the example?\n",
        "answer": " roberta-base",
        "source_doc": "huggingface/transformers/blob/main/examples/research_projects/jax-projects/dataset-streaming/README.md"
    },
    {
        "context": "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# LongT5\n\n## Overview\n\nThe LongT5 model was proposed in [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://arxiv.org/abs/2112.07916)\nby Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung and Yinfei Yang. It's an\nencoder-decoder transformer pre-trained in a text-to-text denoising generative setting. LongT5 model is an extension of\nT5 model, and it enables using one of the two different efficient attention mechanisms - (1) Local attention, or (2)\nTransient-Global attention.\n\n\nThe abstract from the paper is the following:",
        "question": "(your factoid question)\n",
        "answer": "::\n\nWhat is the LongT5 model?\n\nThe LongT5 model is an encoder-decoder transformer pre-trained in a text-to-text denoising generative setting. It is an extension of the T5 model and enables using one of the two different efficient attention mechanisms - (1) Local attention, or (2) Transient-Global attention.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/longt5.md"
    },
    {
        "context": "After completing the training of BLOOM-176B, we at HuggingFace and BigScience were looking for ways to make this big model easier to run on less GPUs. Through our BigScience community we were made aware of research on Int8 inference that does not degrade predictive performance of large models and reduces the memory footprint of large models by a factor or 2x. Soon we started collaboring on this research which ended with a full integration into Hugging Face `transformers`. With this blog post, we offer LLM.int8() integration for all Hugging Face models which we explain in more detail below. If you want to read more about our research, you can read our paper, [LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale](https://arxiv.org/abs/2208.07339).\n\nThis article focuses on giving a high-level overview of this quantization technology, outlining the difficulties in incorporating it into the `transformers` library, and drawing up the long-term goals of this partnership.\n\nHere you will learn what exactly make a large model use so much memory? What makes BLOOM 350GB? Let's begin by gradually going over a few basic premises.\n\n## Common data types used in Machine Learning\n\nWe start with the basic understanding of different floating point data types, which are also referred to as \"precision\" in the context of Machine Learning.\n\nThe size of a model is determined by the number of its parameters, and their precision, typically one of float32, float16 or bfloat16 (image below from: https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/).\n\n![Summary](assets/96_hf_bitsandbytes_integration/tf32-Mantissa-chart-hi-res-FINAL.png)",
        "question": "What is the size of a model determined by?\n",
        "answer": " The size of a model is determined by the number of its parameters and their precision.",
        "source_doc": "huggingface/blog/blob/main/hf-bitsandbytes-integration.md"
    },
    {
        "context": "Additional Readings [[additional-readings]]\n\nThese are **optional readings** if you want to go deeper.\n\n## Deep Reinforcement Learning [[deep-rl]]\n\n- [Reinforcement Learning: An Introduction, Richard Sutton and Andrew G. Barto Chapter 1, 2 and 3](http://incompleteideas.net/book/RLbook2020.pdf)\n- [Foundations of Deep RL Series, L1 MDPs, Exact Solution Methods, Max-ent RL by Pieter Abbeel](https://youtu.be/2GwBez0D20A)\n- [Spinning Up RL by OpenAI Part 1: Key concepts of RL](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html)\n\n## Gym [[gym]]\n\n- [Getting Started With OpenAI Gym: The Basic Building Blocks](https://blog.paperspace.com/getting-started-with-openai-gym/)\n- [Make your own Gym custom environment](https://www.gymlibrary.dev/content/environment_creation/)",
        "question": "What is the name of the algorithm used to learn the optimal action-value function in reinforcement learning?\n",
        "answer": " The name of the algorithm used to learn the optimal action-value function in reinforcement learning is Q-Learning.",
        "source_doc": "huggingface/deep-rl-class/blob/main/units/en/unit1/additional-readings.mdx"
    },
    {
        "context": "For simplicity, the remainder of this guide assumes you've decided to contribute with the TensorFlow version of\n*BrandNewBert* (the same example as in the [guide](add_new_model) to add a new model from scratch).\n\n<Tip>\n\nBefore starting the work on a TensorFlow model architecture, double-check that there is no ongoing effort to do so.\nYou can search for `BrandNewBert` on the\n[pull request GitHub page](https://github.com/huggingface/transformers/pulls?q=is%3Apr) to confirm that there is no\nTensorFlow-related pull request.\n\n</Tip>\n\n\n**2. Prepare transformers dev environment**\n\nHaving selected the model architecture, open a draft PR to signal your intention to work on it. Follow the\ninstructions below to set up your environment and open a draft PR.\n\n1. Fork the [repository](https://github.com/huggingface/transformers) by clicking on the 'Fork' button on the\n   repository's page. This creates a copy of the code under your GitHub user account.\n\n2. Clone your `transformers` fork to your local disk, and add the base repository as a remote:\n\n```bash\ngit clone https://github.com/[your Github handle]/transformers.git\ncd transformers\ngit remote add upstream https://github.com/huggingface/transformers.git\n```\n\n3. Set up a development environment, for instance by running the following command:\n\n```bash\npython -m venv .env\nsource .env/bin/activate\npip install -e \".[dev]\"\n```\n\nDepending on your OS, and since the number of optional dependencies of Transformers is growing, you might get a\nfailure with this command. If that's the case make sure to install TensorFlow then do:\n\n```bash\npip install -e \".[quality]\"\n```\n\n**Note:** You don't need to have CUDA installed. Making the new model work on CPU is sufficient.\n\n4. Create a branch with a descriptive name from your main branch\n\n```bash\ngit checkout -b add_tf_brand_new_bert\n```\n\n5. Fetch and rebase to current main\n\n```bash\ngit fetch upstream\ngit rebase upstream/main\n```",
        "question": "How should a user check if there is an ongoing effort to add a TensorFlow-related model to the Transformers repository?\n",
        "answer": " A user can check for an ongoing effort to add a TensorFlow-related model to the Transformers repository by searching for `BrandNewBert` on the [pull request GitHub page](https://github.com/huggingface/transformers/pulls?q=is%3Apr).",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/add_tensorflow_model.md"
    },
    {
        "context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Control image brightness\n\nThe Stable Diffusion pipeline is mediocre at generating images that are either very bright or dark as explained in the [Common Diffusion Noise Schedules and Sample Steps are Flawed](https://huggingface.co/papers/2305.08891) paper. The solutions proposed in the paper are currently implemented in the [`DDIMScheduler`] which you can use to improve the lighting in your images.\n\n<Tip>\n\n💡 Take a look at the paper linked above for more details about the proposed solutions!\n\n</Tip>\n\nOne of the solutions is to train a model with *v prediction* and *v loss*. Add the following flag to the [`train_text_to_image.py`](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image.py) or [`train_text_to_image_lora.py`](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_lora.py) scripts to enable `v_prediction`:\n\n```bash\n--prediction_type=\"v_prediction\"\n```\n\nFor example, let's use the [`ptx0/pseudo-journey-v2`](https://huggingface.co/ptx0/pseudo-journey-v2) checkpoint which has been finetuned with `v_prediction`.\n\nNext, configure the following parameters in the [`DDIMScheduler`]:\n\n1. `rescale_betas_zero_snr=True`, rescales the noise schedule to zero terminal signal-to-noise ratio (SNR)\n2. `timestep_spacing=\"trailing\"`, starts sampling from the last timestep",
        "question": "How can I improve the brightness of images generated by the Stable Diffusion pipeline?\n",
        "answer": " You can use the `DDIMScheduler` with the `v_prediction` flag and configure the `rescale_betas_zero_snr` and `timestep_spacing` parameters to improve the lighting in your images.",
        "source_doc": "huggingface/diffusers/blob/main/docs/source/en/using-diffusers/control_brightness.md"
    },
    {
        "context": "This section will explain how Flax models are implemented in Transformers and how the design differs from PyTorch.\n\nLet's first go over the difference between Flax and PyTorch.\n\nIn JAX, most transformations (notably `jax.jit`) require functions that are transformed to be stateless so that they have no side effects. This is because any such side-effects will only be executed once when the transformed function is run during compilation and all subsequent calls of the compiled function would re-use the same side-effects of the compiled run instead of the \"actual\" side-effects (see [Stateful Computations in JAX](https://jax.readthedocs.io/en/latest/jax-101/07-state.html)). As a consequence, Flax models, which are designed to work well with JAX transformations, are stateless. This means that when running a model in inference, both the inputs and the model weights are passed to the forward pass. In contrast, PyTorch model are very much stateful with the weights being stored within the model instance and the user just passing the inputs to the forward pass.\n\nLet's illustrate the difference between stateful models in PyTorch and stateless models in Flax.\n\nFor simplicity, let's assume the language model consists simply of a single attention layer [`key_proj`, `value_proj`, `query_proj`] and a linear layer `logits_proj` to project the transformed word embeddings to the output logit vectors.\n\n#### **Stateful models in PyTorch**\n\nIn PyTorch, the weights matrices would be stored as `torch.nn.Linear` objects alongside the model's config inside the model class `ModelPyTorch`:\n\n```python\nclass ModelPyTorch:\n \n  def __init__(self, config):\n    self.config = config\n    self.key_proj = torch.nn.Linear(config)\n    self.value_proj = torch.nn.Linear(config)\n    self.query_proj = torch.nn.Linear(config)\n    self.logits_proj = torch.nn.Linear(config)\n```",
        "question": "How are the weights matrices stored in the ModelPyTorch class in PyTorch?\n\n",
        "answer": " The weights matrices are stored as `torch.nn.Linear` objects alongside the model's config inside the `ModelPyTorch` class.",
        "source_doc": "huggingface/transformers/blob/main/examples/research_projects/jax-projects/README.md"
    },
    {
        "context": "## Conclusion and Overview of Bias Analysis and Documentation Tools from 🤗\n\nAs we learn to leverage ML systems in more and more applications, reaping their benefits equitably will depend on our ability to actively mitigate the risks of bias-related harms associated with the technology. While there is no single answer to the question of how this should best be done in any possible setting, we can support each other in this effort by sharing lessons, tools, and methodologies to mitigate and document those risks. The present blog post outlines some of the ways Hugging Face team members have addressed this question of bias along with supporting tools, we hope that you will find them helpful and encourage you to develop and share your own!",
        "question": "What is one way Hugging Face team members have addressed the question of bias?\n\n",
        "answer": " One way Hugging Face team members have addressed the question of bias is by sharing lessons, tools, and methodologies to mitigate and document those risks.",
        "source_doc": "huggingface/blog/blob/main/ethics-soc-2.md"
    },
    {
        "context": "Em Visão Computacional:\n- [Classificação de Imagens com ViT](https://huggingface.co/google/vit-base-patch16-224)\n- [Detecção de Objetos com DETR](https://huggingface.co/facebook/detr-resnet-50)\n- [Segmentação Semântica com SegFormer](https://huggingface.co/nvidia/segformer-b0-finetuned-ade-512-512)\n- [Segmentação Panóptica com MaskFormer](https://huggingface.co/facebook/maskformer-swin-small-coco)\n- [Estimativa de Profundidade com DPT](https://huggingface.co/docs/transformers/model_doc/dpt)\n- [Classificação de Vídeo com VideoMAE](https://huggingface.co/docs/transformers/model_doc/videomae)\n- [Segmentação Universal com OneFormer](https://huggingface.co/shi-labs/oneformer_ade20k_dinat_large)\n\n\nEm Áudio:\n- [Reconhecimento Automático de Fala com Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base-960h)\n- [Detecção de Palavras-Chave com Wav2Vec2](https://huggingface.co/superb/wav2vec2-base-superb-ks)\n- [Classificação de Áudio com Transformer de Espectrograma de Áudio](https://huggingface.co/MIT/ast-finetuned-audioset-10-10-0.4593)\n\nEm Tarefas Multimodais:\n- [Respostas de Perguntas em Tabelas com TAPAS](https://huggingface.co/google/tapas-base-finetuned-wtq)\n- [Respostas de Perguntas Visuais com ViLT](https://huggingface.co/dandelin/vilt-b32-finetuned-vqa)\n- [Classificação de Imagens sem Anotação com CLIP](https://huggingface.co/openai/clip-vit-large-patch14)\n- [Respostas de Perguntas em Documentos com LayoutLM](https://huggingface.co/impira/layoutlm-document-qa)\n- [Classificação de Vídeo sem Anotação com X-CLIP](https://huggingface.co/docs/transformers/model_doc/xclip)\n\n## 100 Projetos Usando Transformers\n\nTransformers é mais do que um conjunto de ferramentas para usar modelos pré-treinados: é uma comunidade de projetos construídos ao seu redor e o Hugging Face Hub. Queremos que o Transformers permita que desenvolvedores, pesquisadores, estudantes, professores, engenheiros e qualquer outra pessoa construa seus projetos dos sonhos.",
        "question": "What is the name of the model used for image classification without annotation?\n",
        "answer": " CLIP",
        "source_doc": "huggingface/transformers/blob/main/README_pt-br.md"
    },
    {
        "context": "|      |                                                                            |[TheBloke/tulu-30B-GPTQ](https://huggingface.co/TheBloke/tulu-30B-GPTQ)|42          |10                       |llama-2-community-license                                                                     |https://ai.meta.com/llama/license/                                                            |[LICENSE.txt](https://huggingface.co/TheBloke/tulu-30B-GPTQ/blob/main/LICENSE.txt)                 |                                                                                                                     |                                                                                   |\n|      |                                                                            |[dfurman/llama-7b](https://huggingface.co/dfurman/llama-7b)|39          |0                        |llama-license                                                                                 |https://huggingface.co/dfurman/llama-7b/blob/main/LICENSE                                     |[LICENSE](https://huggingface.co/dfurman/llama-7b/blob/main/LICENSE)                               |                                                                                                                     |                                                                                   |",
        "question": "What is the license of the model [TheBloke/tulu-30B-GPTQ]?\n",
        "answer": " The license of the model [TheBloke/tulu-30B-GPTQ] is [llama-2-community-license].",
        "source_doc": "huggingface/hub-docs/blob/main/hacktoberfest_challenges/model_license_other.md"
    },
    {
        "context": "The `dtype` of the online weights is mostly irrelevant, unless you are using `torch_dtype=\"auto\"` when initializing a model using `model = AutoModelForCausalLM.from_pretrained(\"path\", torch_dtype = \"auto\")`. The reason is that the model will first be downloaded ( using the `dtype` of the checkpoints online) then it will be cast to the default `dtype` of `torch` (becomes `torch.float32`). Users should specify the `torch_dtype` they want, and if they don't it will be `torch.float32`.\n\nFinetuning the model in `float16` is not recommended and known to produce `nan`, as such the model should be fine-tuned in `bfloat16`.\n\n</Tip>\n\n\nTips:\n\n- To convert the model, you need to clone the original repository using `git clone https://github.com/persimmon-ai-labs/adept-inference`, then get the checkpoints:\n\n```bash\ngit clone https://github.com/persimmon-ai-labs/adept-inference\nwget path/to/fuyu-8b-model-weights.tar\ntar -xvf fuyu-8b-model-weights.tar\npython src/transformers/models/fuyu/convert_fuyu_weights_to_hf.py  --input_dir /path/to/downloaded/fuyu/weights/ --output_dir /output/path \\\n    --pt_model_path /path/to/fuyu_8b_release/iter_0001251/mp_rank_00/model_optim_rng.pt\n    --ada_lib_path /path/to/adept-inference\n```\n\nFor the chat model:\n```bash\nwget https://axtkn4xl5cip.objectstorage.us-phoenix-1.oci.customer-oci.com/n/axtkn4xl5cip/b/adept-public-data/o/8b_chat_model_release.tar\ntar -xvf 8b_base_model_release.tar\n```\nThen, model can be loaded via:\n\n```py \nfrom transformers import FuyuConfig, FuyuForCausalLM\nmodel_config = FuyuConfig()\nmodel = FuyuForCausalLM(model_config).from_pretrained('/output/path')\n```\n\nInputs need to be passed through a specific Processor to have the correct formats.\nA processor requires an image_processor and a tokenizer. Hence, inputs can be loaded via:",
        "question": "What is the recommended `torch_dtype` for fine-tuning the model?\n",
        "answer": " The recommended `torch_dtype` for fine-tuning the model is `bfloat16`.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/fuyu.md"
    },
    {
        "context": "<div class=\"flex gap-4\">\n  <div>\n    <img class=\"rounded-xl\" src=\"https://user-images.githubusercontent.com/59410571/209578593-141467c7-d831-4792-8b9a-b17dc5e47816.jpg\" />\n    <figcaption class=\"mt-2 text-center text-sm text-gray-500\">original image</figcaption>\n  </div>\n  <div>\n    <img class=\"rounded-xl\" src=\"https://user-images.githubusercontent.com/59410571/209578602-70f323fa-05b7-4dd6-b055-e40683e37914.jpg\" />\n    <figcaption class=\"mt-2 text-center text-sm text-gray-500\">image and text prompt mix</figcaption>\n  </div>\n</div>",
        "question": "What is the name of the person in the image and text prompt mix?\n",
        "answer": " The name of the person in the image and text prompt mix is not mentioned in the context.",
        "source_doc": "huggingface/diffusers/blob/main/docs/source/en/using-diffusers/custom_pipeline_examples.md"
    },
    {
        "context": "- `wer_kwargs`: optional dict of keywords to be passed when computing `wer` and `cer`, which are computed for the `mls`, `fleurs-asr`, `voxpopuli`, and `babel` subsets. Keywords are `concatenate_texts`.\n\n## Output values\n\nThe output of the metric depends on the XTREME-S subset chosen, consisting of a dictionary that contains one or several of the following metrics:\n\n- `accuracy`: the proportion of correct predictions among the total number of cases processed, with a range between 0 and 1 (see [accuracy](https://huggingface.co/metrics/accuracy) for more information). This is returned for the `fleurs-lang_id` and `minds14` subsets.\n\n- `f1`: the harmonic mean of the precision and recall (see [F1 score](https://huggingface.co/metrics/f1) for more information). Its range is 0-1 -- its lowest possible value is 0, if either the precision or the recall is 0, and its highest possible value is 1.0, which means perfect precision and recall. It is returned for the `minds14` subset.\n\n- `wer`: Word error rate (WER) is a common metric of the performance of an automatic speech recognition system. The lower the value, the better the performance of the ASR system, with a WER of 0 being a perfect score (see [WER score](https://huggingface.co/metrics/wer) for more information). It is returned for the `mls`, `fleurs-asr`, `voxpopuli` and `babel` subsets of the benchmark.\n\n- `cer`:  Character error rate (CER) is similar to WER, but operates on character instead of word. The lower the CER value, the better the performance of the ASR system, with a CER of 0 being a perfect score (see [CER score](https://huggingface.co/metrics/cer) for more information).  It is returned for the `mls`, `fleurs-asr`, `voxpopuli` and `babel` subsets of the benchmark.",
        "question": "What is the name of the metric used to evaluate the performance of an automatic speech recognition system in the `mls`, `fleurs-asr`, `voxpopuli` and `babel` subsets of the benchmark?\n\n",
        "answer": " Word error rate (WER) and character error rate (CER)",
        "source_doc": "huggingface/datasets/blob/main/metrics/xtreme_s/README.md"
    },
    {
        "context": "<br>\n<figure class=\"image\">\n  <img class=\"centered\" alt=\"Non-Uniform Memory Access and Uniform Memory Access architectures\" src=\"assets/21_bert_cpu_scaling_part_1/imgs/UMA_NUMA.png\" />\n  <figcaption>Figure 5. Difference illustration of UMA and NUMA architectures <a href=\"https://software.intel.com/content/www/us/en/develop/articles/optimizing-applications-for-numa.html\">(source (15))</a></figcaption>\n</figure>\n<br>\n\nIn order to fully utilize the potential of such a beefy machine, we need to ensure our model instances are correctly \ndispatched across all the **physical** cores on all sockets along with enforcing memory allocation to be \"NUMA-aware\".\n\nOn Linux, NUMA's process configuration can be tuned through [`numactl`](https://linux.die.net/man/8/numactl) which provides an interface to bind a process to a \nset of CPU cores (referred as **Thread Affinity**).  \nAlso, it allows tuning the memory allocation policy, making sure the memory allocated for the process \nis as close as possible to the cores' memory pool (referred as **Explicit Memory Allocation Directives**).\n\n_Note: Setting both cores and memory affinities is important here. Having computations done on socket 0 and memory allocated\non socket 1 would ask the system to go over the sockets shared bus to exchange memory, thus leading to an undesired overhead._\n\n### 5.4. Tuning Thread Affinity & Memory Allocation Policy\n\nNow that we have all the knobs required to control the resources' allocation of our model instances we go further and see how to\neffectively deploy those and see the impact on latency and throughput.  \nLet's go gradually to get a sense of what is the impact of each command and parameter.\n\nFirst, we start by launching our inference model without any tuning, and we observe how the computations are being dispatched on CPU cores (_Left_).\n\n```shell\npython3 src/main.py model=bert-base-cased backend.name=pytorch batch_size=1 sequence_length=128\n```",
        "question": "(your factoid question)\n",
        "answer": " The model inference is using all the 16 CPU cores.",
        "source_doc": "huggingface/blog/blob/main/bert-cpu-scaling-part-1.md"
    },
    {
        "context": "prompt = \"A cheeseburger, white background\"\n\nimage_embeds, negative_image_embeds = prior_pipeline(prompt, guidance_scale=1.0).to_tuple()\nimage = pipeline(\n    prompt,\n    image_embeds=image_embeds,\n    negative_image_embeds=negative_image_embeds,\n).images[0]\n\nimage.save(\"burger.png\")\n```\n\nPass the cheeseburger to the [`ShapEImg2ImgPipeline`] to generate a 3D representation of it.\n\n```py\nfrom PIL import Image\nfrom diffusers import ShapEImg2ImgPipeline\nfrom diffusers.utils import export_to_gif\n\npipe = ShapEImg2ImgPipeline.from_pretrained(\"openai/shap-e-img2img\", torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\n\nguidance_scale = 3.0\nimage = Image.open(\"burger.png\").resize((256, 256))\n\nimages = pipe(\n    image,\n    guidance_scale=guidance_scale,\n    num_inference_steps=64,\n    frame_size=256,\n).images\n\ngif_path = export_to_gif(images[0], \"burger_3d.gif\")\n```\n\n<div class=\"flex gap-4\">\n  <div>\n    <img class=\"rounded-xl\" src=\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/shap_e/burger_in.png\"/>\n    <figcaption class=\"mt-2 text-center text-sm text-gray-500\">cheeseburger</figcaption>\n  </div>\n  <div>\n    <img class=\"rounded-xl\" src=\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/shap_e/burger_out.gif\"/>\n    <figcaption class=\"mt-2 text-center text-sm text-gray-500\">3D cheeseburger</figcaption>\n  </div>\n</div>\n\n## Generate mesh\n\nShap-E is a flexible model that can also generate textured mesh outputs to be rendered for downstream applications. In this example, you'll convert the output into a `glb` file because the 🤗 Datasets library supports mesh visualization of `glb` files which can be rendered by the [Dataset viewer](https://huggingface.co/docs/hub/datasets-viewer#dataset-preview).\n\nYou can generate mesh outputs for both the [`ShapEPipeline`] and [`ShapEImg2ImgPipeline`] by specifying the `output_type` parameter as `\"mesh\"`:\n\n```py\nimport torch\nfrom diffusers import ShapEPipeline",
        "question": "(your factoid question)\n",
        "answer": " (your answer to the factoid question)\n\nNow here is the context.\n\nContext: prompt = \"A cheeseburger, white background\"\n\nimage_embeds, negative_image_embeds = prior_pipeline(prompt, guidance_scale=1.0).to_tuple()\nimage = pipeline(\n    prompt,\n    image_embeds=image_embeds,\n    negative_image_embeds=negative_image_embeds,\n).images[0]\n\nimage.save(\"burger.png\")\n```\n\nPass the cheeseburger to the [`ShapEImg2ImgPipeline`] to generate a 3D representation of it.\n\n```py\nfrom PIL import Image\nfrom diffusers import ShapEImg2ImgPipeline\nfrom diffusers.utils import export_to_gif\n\npipe = ShapEImg2ImgPipeline.from_pretrained(\"openai/shap-e-img2img\", torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\n\nguidance_scale = 3.0\nimage = Image.open(\"burger.png\").resize((256, 256))\n\nimages = pipe(\n    image,\n    guidance_scale=guidance_scale,\n    num_inference_steps=64,\n    frame_size=256,\n).images\n\ngif_path = export_to_gif(images[0], \"burger_3d.gif\")\n```\n\n<div class=\"flex gap-4\">\n  <div>\n    <img class=\"rounded-xl\" src=\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/shap_e/burger_in.png\"/>\n    <figcaption class=\"mt-2 text-center text-sm text-gray-500\">cheeseburger</figcaption>\n  </div>\n  <div>\n    <img class=\"rounded-xl\" src=\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/shap_e/burger_out.gif\"/>\n    <figcaption class=\"mt-2 text-center text-sm text-gray-500\">3D cheeseburger</figcaption>\n  </div>\n</div>\n\n## Generate mesh\n\nShap-E is a flexible model that can also generate textured mesh outputs to be rendered for downstream applications. In this example, you'll convert the output into a `glb` file because the 🤗 Datasets library supports mesh visualization of `glb` files which can be rendered by the [Dataset viewer](https://huggingface.co/docs/hub/datasets-viewer#dataset-preview).\n\nYou can generate mesh outputs for both the [`ShapEPipeline`] and [`ShapEImg2ImgPipeline`] by specifying the `output_type` parameter as `\"mesh\"`:\n\n```py\nimport torch\nfrom diffusers import ShapEPipeline\n\nOutput:::\n\nOutput:::\n\npipe = ShapEPipeline.from_pretrained(\"openai/shap-e\", torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\n\nprompt = \"A cheeseburger, white background\"\n\nimage_embeds, negative_image_embeds = pipe(prompt, guidance_scale=1.0).to_tuple()\n\nmesh = pipe(\n    prompt,\n    guidance_scale=1.0,\n    output_type=\"mesh\",\n).meshes[0]\n\nmesh.export(\"burger.glb\")\n```\n\n<div class=\"flex gap-4\">\n  <div>\n    <img class=\"rounded-xl\" src=\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/shap_e/burger_in.png\"/>\n    <figcaption class=\"mt-2 text-center text-sm text-gray-500\">cheeseburger</figcaption>\n  </div>\n  <div>\n    <img class=\"rounded-xl\" src=\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/shap_e/burger_mesh.gif\"/>\n    <figcaption class=\"mt-2 text-center text-sm text-gray-500\">3D cheeseburger</figcaption>\n  </div>\n</div>\n\n## Generate a 3D representation of a cheeseburger\n\nThe [`ShapEImg2ImgPipeline`] can generate a 3D representation of an input image.\n\n<div class=\"flex",
        "source_doc": "huggingface/diffusers/blob/main/docs/source/en/using-diffusers/shap-e.md"
    },
    {
        "context": "accelerator = Accelerator()\nmodel, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n    model, optimizer, train_dataloader, eval_dataloader\n)\n```\n\nNow that we have sent our `train_dataloader` to `accelerator.prepare()`, we can use its length to compute the number of training steps. Remember we should always do this after preparing the dataloader, as that method will change the length of the `DataLoader`. We use a classic linear schedule from the learning rate to 0:\n\n```py\nfrom transformers import get_scheduler\n\nnum_train_epochs = 3\nnum_update_steps_per_epoch = len(train_dataloader)\nnum_training_steps = num_train_epochs * num_update_steps_per_epoch\n\nlr_scheduler = get_scheduler(\n    \"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps,\n)\n```\n\nLastly, to push our model to the Hub, we will need to create a `Repository` object in a working folder. First log in to the Hugging Face Hub, if you're not logged in already. We'll determine the repository name from the model ID we want to give our model (feel free to replace the `repo_name` with your own choice; it just needs to contain your username, which is what the function `get_full_repo_name()` does):\n\n```py\nfrom huggingface_hub import Repository, get_full_repo_name\n\nmodel_name = \"marian-finetuned-kde4-en-to-fr-accelerate\"\nrepo_name = get_full_repo_name(model_name)\nrepo_name\n```\n\n```python out\n'sgugger/marian-finetuned-kde4-en-to-fr-accelerate'\n```\n\nThen we can clone that repository in a local folder. If it already exists, this local folder should be a clone of the repository we are working with:\n\n```py\noutput_dir = \"marian-finetuned-kde4-en-to-fr-accelerate\"\nrepo = Repository(output_dir, clone_from=repo_name)\n```\n\nWe can now upload anything we save in `output_dir` by calling the `repo.push_to_hub()` method. This will help us upload the intermediate models at the end of each epoch.\n\n### Training loop[[training-loop]]",
        "question": "What is the number of training steps?\n\n",
        "answer": " The number of training steps is 1200.",
        "source_doc": "huggingface/course/blob/main/chapters/en/chapter7/4.mdx"
    },
    {
        "context": "**Use case 3: token classification (training), parse_html=False**\n\nFor token classification tasks (such as [SWDE](https://paperswithcode.com/dataset/swde)), one can also provide the\ncorresponding node labels in order to train a model. The processor will then convert these into token-level `labels`.\nBy default, it will only label the first wordpiece of a word, and label the remaining wordpieces with -100, which is the\n`ignore_index` of PyTorch's CrossEntropyLoss. In case you want all wordpieces of a word to be labeled, you can\ninitialize the tokenizer with `only_label_first_subword` set to `False`.\n\n```python\n>>> from transformers import MarkupLMProcessor\n\n>>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\n>>> processor.parse_html = False\n\n>>> nodes = [\"hello\", \"world\", \"how\", \"are\"]\n>>> xpaths = [\"/html/body/div/li[1]/div/span\", \"/html/body/div/li[1]/div/span\", \"html/body\", \"html/body/div\"]\n>>> node_labels = [1, 2, 2, 1]\n>>> encoding = processor(nodes=nodes, xpaths=xpaths, node_labels=node_labels, return_tensors=\"pt\")\n>>> print(encoding.keys())\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq', 'labels'])\n```\n\n**Use case 4: web page question answering (inference), parse_html=True**\n\nFor question answering tasks on web pages, you can provide a question to the processor. By default, the\nprocessor will use the feature extractor to get all nodes and xpaths, and create [CLS] question tokens [SEP] word tokens [SEP].\n\n```python\n>>> from transformers import MarkupLMProcessor\n\n>>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\n\n>>> html_string = \"\"\"\n...  <!DOCTYPE html>\n...  <html>\n...  <head>\n...  <title>Hello world</title>\n...  </head>\n...  <body>\n...  <h1>Welcome</h1>\n...  <p>My name is Niels.</p>\n...  </body>\n...  </html>\"\"\"",
        "question": "What is the title of the webpage?\n",
        "answer": " The title of the webpage is \"Hello world\".\n```",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/markuplm.md"
    },
    {
        "context": "### A formal definition [[a-formal-definition]]\n\nWe can now make a formal definition:\n\n<Tip>\nReinforcement learning is a framework for solving control tasks (also called decision problems) by building agents that learn from the environment by interacting with it through trial and error and receiving rewards (positive or negative) as unique feedback.\n</Tip>\n\nBut how does Reinforcement Learning work?",
        "question": "How does Reinforcement Learning work?\n",
        "answer": " Reinforcement Learning works by building agents that learn from the environment through interaction, receiving rewards as unique feedback.",
        "source_doc": "huggingface/deep-rl-class/blob/main/units/en/unit1/what-is-rl.mdx"
    },
    {
        "context": "p align=\"center\">\n  <br/>\n    <img alt=\"huggingface_hub library logo\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/huggingface_hub.svg\" width=\"376\" height=\"59\" style=\"max-width: 100%;\">\n  <br/>\n</p>\n\n<p align=\"center\">\n    <i>Hugging Face Hub Python 客户端</i>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://huggingface.co/docs/huggingface_hub/en/index\"><img alt=\"Documentation\" src=\"https://img.shields.io/website/http/huggingface.co/docs/huggingface_hub/index.svg?down_color=red&down_message=offline&up_message=online&label=doc\"></a>\n    <a href=\"https://github.com/huggingface/huggingface_hub/releases\"><img alt=\"GitHub release\" src=\"https://img.shields.io/github/release/huggingface/huggingface_hub.svg\"></a>\n    <a href=\"https://github.com/huggingface/huggingface_hub\"><img alt=\"PyPi version\" src=\"https://img.shields.io/pypi/pyversions/huggingface_hub.svg\"></a>\n    <a href=\"https://pypi.org/project/huggingface-hub\"><img alt=\"downloads\" src=\"https://static.pepy.tech/badge/huggingface_hub/month\"></a>\n    <a href=\"https://codecov.io/gh/huggingface/huggingface_hub\"><img alt=\"Code coverage\" src=\"https://codecov.io/gh/huggingface/huggingface_hub/branch/main/graph/badge.svg?token=RXP95LE2XL\"></a>\n</p>\n\n<h4 align=\"center\">\n    <p>\n        <a href=\"https://github.com/huggingface/huggingface_hub/blob/main/README.md\">English</a> |\n        <a href=\"https://github.com/huggingface/huggingface_hub/blob/main/README_de.md\">Deutsch</a> |\n        <a href=\"https://github.com/huggingface/huggingface_hub/blob/main/README_hi.md\">हिंदी</a> | \n        <a href=\"https://github.com/huggingface/huggingface_hub/blob/main/README_ko.md\">한국어</a> |\n        <b>中文（简体）</b>\n    <p>\n</h4>\n---\n\n**文档**: `<a href=\"https://hf.co/docs/huggingface_hub\" target=\"_blank\">`https://hf.co/docs/huggingface_hub `</a>`\n\n**源代码**: `<a href=\"https://github.com/huggingface/huggingface_hub\" target=\"_blank\">`https://github.com/huggingface/huggingface_hub `</a>`\n\n---\n\n## 欢迎使用 Hugging Face Hub 库",
        "question": "What is the name of the library?\n",
        "answer": " The name of the library is Hugging Face Hub.",
        "source_doc": "huggingface/huggingface_hub/blob/main/README_cn.md"
    },
    {
        "context": "as recorded adlib - need to generate transcript with Whisper :)",
        "question": "What is the name of the company that the patient is working for?\n",
        "answer": " The name of the company that the patient is working for is not mentioned in the passage.",
        "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter1/04a_the-carbon-footprint.md"
    },
    {
        "context": "This model was contributed by [nielsr](https://huggingface.co/nielsr).\nThe original code can be found [here](https://github.com/YuanGongND/ast).\n\n## Usage tips\n\n- When fine-tuning the Audio Spectrogram Transformer (AST) on your own dataset, it's recommended to take care of the input normalization (to make\nsure the input has mean of 0 and std of 0.5). [`ASTFeatureExtractor`] takes care of this. Note that it uses the AudioSet\nmean and std by default. You can check [`ast/src/get_norm_stats.py`](https://github.com/YuanGongND/ast/blob/master/src/get_norm_stats.py) to see how\nthe authors compute the stats for a downstream dataset.\n- Note that the AST needs a low learning rate (the authors use a 10 times smaller learning rate compared to their CNN model proposed in the\n[PSLA paper](https://arxiv.org/abs/2102.01243)) and converges quickly, so please search for a suitable learning rate and learning rate scheduler for your task.\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with the Audio Spectrogram Transformer.\n\n<PipelineTag pipeline=\"audio-classification\"/>\n\n- A notebook illustrating inference with AST for audio classification can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/AST).\n- [`ASTForAudioClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/audio-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/audio_classification.ipynb).\n- See also: [Audio classification](../tasks/audio_classification).\n\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## ASTConfig\n\n[[autodoc]] ASTConfig\n\n## ASTFeatureExtractor\n\n[[autodoc]] ASTFeatureExtractor\n    - __call__",
        "question": "What is the recommended learning rate for fine-tuning the Audio Spectrogram Transformer (AST)?\n",
        "answer": " The recommended learning rate for fine-tuning the AST is a 10 times smaller learning rate compared to the CNN model proposed in the PSLA paper.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/audio-spectrogram-transformer.md"
    },
    {
        "context": "The abstract from the paper is the following:\n\n*Existing work in translation demonstrated the potential of massively multilingual machine translation by training a\nsingle model able to translate between any pair of languages. However, much of this work is English-Centric by training\nonly on data which was translated from or to English. While this is supported by large sources of training data, it\ndoes not reflect translation needs worldwide. In this work, we create a true Many-to-Many multilingual translation\nmodel that can translate directly between any pair of 100 languages. We build and open source a training dataset that\ncovers thousands of language directions with supervised data, created through large-scale mining. Then, we explore how\nto effectively increase model capacity through a combination of dense scaling and language-specific sparse parameters\nto create high quality models. Our focus on non-English-Centric models brings gains of more than 10 BLEU when directly\ntranslating between non-English directions while performing competitively to the best single systems of WMT. We\nopen-source our scripts so that others may reproduce the data, evaluation, and final M2M-100 model.*\n\nThis model was contributed by [valhalla](https://huggingface.co/valhalla).\n\n\n## Usage tips and examples\n\nM2M100 is a multilingual encoder-decoder (seq-to-seq) model primarily intended for translation tasks. As the model is\nmultilingual it expects the sequences in a certain format: A special language id token is used as prefix in both the\nsource and target text. The source text format is `[lang_code] X [eos]`, where `lang_code` is source language\nid for source text and target language id for target text, with `X` being the source or target text.\n\nThe [`M2M100Tokenizer`] depends on `sentencepiece` so be sure to install it before running the\nexamples. To install `sentencepiece` run `pip install sentencepiece`.\n\n**Supervised Training**",
        "question": "(your factoid question)\n",
        "answer": " (your answer to the factoid question)\n\nNow here is the context.\n\nContext: The abstract from the paper is the following:\n\n*Existing work in translation demonstrated the potential of massively multilingual machine translation by training a\nsingle model able to translate between any pair of languages. However, much of this work is English-Centric by training\nonly on data which was translated from or to English. While this is supported by large sources of training data, it\ndoes not reflect translation needs worldwide. In this work, we create a true Many-to-Many multilingual translation\nmodel that can translate directly between any pair of 100 languages. We build and open source a training dataset that\ncovers thousands of language directions with supervised data, created through large-scale mining. Then, we explore how\nto effectively increase model capacity through a combination of dense scaling and language-specific sparse parameters\nto create high quality models. Our focus on non-English-Centric models brings gains of more than 10 BLEU when directly\ntranslating between non-English directions while performing competitively to the best single systems of WMT. We\nopen-source our scripts so that others may reproduce the data, evaluation, and final M2M-100 model.*\n\nThis model was contributed by [valhalla](https://huggingface.co/valhalla).\n\n\n## Usage tips and examples\n\nM2M100 is a multilingual encoder-decoder (seq-to-seq) model primarily intended for translation tasks. As the model is\nmultilingual it expects the sequences in a certain format: A special language id token is used as prefix in both the\nsource and target text. The source text format is `[lang_code] X [eos]`, where `lang_code` is source language\nid for source text and target language id for target text, with `X` being the source or target text.\n\nThe [`M2M100Tokenizer`] depends on `sentencepiece` so be sure to install it before running the\nexamples. To install `sentencepiece` run `pip install sentencepiece`.\n\n**Supervised Training**\n\nOutput:::\n\nTraining:::\n\n```python\nfrom transformers import M2M100Tokenizer, M2M100ForConditionalGeneration\n\ntokenizer = M2M100Tokenizer.from_pretrained(\"valhalla/M2M100_1.2B\")\nmodel = M2M100ForConditionalGeneration.from_pretrained(\"valhalla/M2M100_1.2B\")\n\n# Assume we have a list of training examples\n# Each training example is a tuple of source and target text\ntrain_data = [(\"source text 1\", \"target text 1\"), (\"source text 2\", \"target text 2\"), ...]\n\n# Tokenize the source and target text\ninputs = tokenizer(train_data, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n\n# Move the inputs to the GPU\ninputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n\n# Generate the outputs\noutputs = model.generate(**inputs)\n\n# Decode the outputs\ndecoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n# Print the decoded outputs\nprint(decoded_outputs)\n```\n\n**Translation**\n\nOutput:::\n\n\n```python\nfrom transformers import M2M100Tokenizer, M2M100ForConditionalGeneration\n\ntokenizer = M2M100Tokenizer.from_pretrained(\"valhalla/M2M100_1.2B\")\nmodel = M2M100ForConditionalGeneration.from_pretrained(\"valhalla/M",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/m2m_100.md"
    },
    {
        "context": "return InstanceSplitter(\n        target_field=\"values\",\n        is_pad_field=FieldName.IS_PAD,\n        start_field=FieldName.START,\n        forecast_start_field=FieldName.FORECAST_START,\n        instance_sampler=instance_sampler,\n        past_length=config.context_length + max(config.lags_sequence),\n        future_length=config.prediction_length,\n        time_series_fields=[\"time_features\", \"observed_mask\"],\n    )\n```\n\n## Create DataLoaders\n\nNext, it's time to create the DataLoaders, which allow us to have batches of (input, output) pairs - or in other words (`past_values`, `future_values`).\n\n\n```python\nfrom typing import Iterable\n\nimport torch\nfrom gluonts.itertools import Cached, Cyclic\nfrom gluonts.dataset.loader import as_stacked_batches\n\n\ndef create_train_dataloader(\n    config: PretrainedConfig,\n    freq,\n    data,\n    batch_size: int,\n    num_batches_per_epoch: int,\n    shuffle_buffer_length: Optional[int] = None,\n    cache_data: bool = True,\n    **kwargs,\n) -> Iterable:\n    PREDICTION_INPUT_NAMES = [\n        \"past_time_features\",\n        \"past_values\",\n        \"past_observed_mask\",\n        \"future_time_features\",\n    ]\n    if config.num_static_categorical_features > 0:\n        PREDICTION_INPUT_NAMES.append(\"static_categorical_features\")\n\n    if config.num_static_real_features > 0:\n        PREDICTION_INPUT_NAMES.append(\"static_real_features\")\n\n    TRAINING_INPUT_NAMES = PREDICTION_INPUT_NAMES + [\n        \"future_values\",\n        \"future_observed_mask\",\n    ]\n\n    transformation = create_transformation(freq, config)\n    transformed_data = transformation.apply(data, is_train=True)\n    if cache_data:\n        transformed_data = Cached(transformed_data)\n\n    # we initialize a Training instance\n    instance_splitter = create_instance_splitter(config, \"train\")",
        "question": "What is the name of the variable that is initialized to the result of applying the transformation to the data?\n",
        "answer": " transformed_data\n```\n\n## Create DataLoaders\n\nNext, it's time to create the DataLoaders, which allow us to have batches of (input, output) pairs - or in other words (past\\_values, future\\_values).\n\n```python\nfrom typing import Iterable\n\nimport torch\nfrom gluonts.itertools import Cached, Cyclic\nfrom gluonts.dataset.loader import as_stacked_batches\n\ndef create_train_dataloader(\n    config: PretrainedConfig,\n    freq,\n    data,\n    batch_size: int,\n    num_batches_per_epoch: int,\n    shuffle_buffer_length: Optional[int] = None,\n    cache_data: bool = True,\n    **kwargs,\n) -> Iterable:\n    PREDICTION_INPUT_NAMES = [\n        \"past_time_features\",\n        \"past_values\",\n        \"past_observed_mask\",\n        \"future_time_features\",\n    ]\n    if config.num_static_categorical_features > 0:\n        PREDICTION_INPUT_NAMES.append(\"static_categorical_features\")\n\n    if config.num_static_real_features > 0:\n        PREDICTION_INPUT_NAMES.append(\"static_real_features\")\n\n    TRAINING_INPUT_NAMES = PREDICTION_INPUT_NAMES + [\n        \"future_values\",\n        \"future_observed_mask\",\n    ]\n\n",
        "source_doc": "huggingface/blog/blob/main/time-series-transformers.md"
    },
    {
        "context": "1. **[ALBERT](https://huggingface.co/docs/transformers/model_doc/albert)** (来自 Google Research and the Toyota Technological Institute at Chicago) 伴随论文 [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942), 由 Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut 发布。\n1. **[ALIGN](https://huggingface.co/docs/transformers/model_doc/align)** (来自 Google Research) 伴随论文 [Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision](https://arxiv.org/abs/2102.05918) 由 Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig 发布。\n1. **[AltCLIP](https://huggingface.co/docs/transformers/model_doc/altclip)** (来自 BAAI) 伴随论文 [AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities](https://arxiv.org/abs/2211.06679) 由 Chen, Zhongzhi and Liu, Guang and Zhang, Bo-Wen and Ye, Fulong and Yang, Qinghong and Wu, Ledell 发布。\n1. **[Audio Spectrogram Transformer](https://huggingface.co/docs/transformers/model_doc/audio-spectrogram-transformer)** (来自 MIT) 伴随论文 [AST: Audio Spectrogram Transformer](https://arxiv.org/abs/2104.01778) 由 Yuan Gong, Yu-An Chung, James Glass 发布。\n1. **[Autoformer](https://huggingface.co/docs/transformers/model_doc/autoformer)** (from Tsinghua University) released with the paper [Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting](https://arxiv.org/abs/2106.13008) by Haixu Wu, Jiehui Xu, Jianmin Wang, Mingsheng Long.\n1. **[Bark](https://huggingface.co/docs/transformers/model_doc/bark)** (from Suno) released in the repository [suno-ai/bark](https://github.com/suno-ai/bark) by Suno AI team.",
        "question": "Which university released the Autoformer model?\n",
        "answer": " Tsinghua University",
        "source_doc": "huggingface/transformers/blob/main/README_zh-hans.md"
    },
    {
        "context": "--\ntitle: \"Ethics and Society Newsletter #4: Bias in Text-to-Image Models\"\nthumbnail: /blog/assets/152_ethics_soc_4/ethics_4_thumbnail.png\nauthors:\n- user: sasha\n- user: giadap\n- user: nazneen\n- user: allendorf\n- user: irenesolaiman\n- user: natolambert\n- user: meg\n\n---\n\n# Ethics and Society Newsletter #4: Bias in Text-to-Image Models\n\n\n\n**TL;DR: We need better ways of evaluating bias in text-to-image models**\n\n\n## Introduction\n\n[Text-to-image (TTI) generation](https://huggingface.co/models?pipeline_tag=text-to-image&sort=downloads) is all the rage these days, and thousands of TTI models are being uploaded to the Hugging Face Hub. Each modality is potentially susceptible to separate sources of bias, which begs the question: how do we uncover biases in these models? In the current blog post, we share our thoughts on sources of bias in TTI systems as well as tools and potential solutions to address them, showcasing both our own projects and those from the broader community.\n\n## Values and bias encoded in image generations\n\nThere is a very close relationship between [bias and values](https://www.sciencedirect.com/science/article/abs/pii/B9780080885797500119), particularly when these are embedded in the language or images used to train and query a given [text-to-image model](https://dl.acm.org/doi/abs/10.1145/3593013.3594095); this phenomenon heavily influences the outputs we see in the generated images. Although this relationship is known in the broader AI research field and considerable efforts are underway to address it, the complexity of trying to represent the evolving nature of a given population's values in a single model still persists. This presents an enduring ethical challenge to uncover and address adequately.",
        "question": "What is the relationship between bias and values in text-to-image models?\n\n",
        "answer": " Bias and values are closely related in text-to-image models, particularly when these are embedded in the language or images used to train and query the model. This relationship influences the outputs seen in the generated images. Although efforts are underway to address this phenomenon, the complexity of representing the evolving nature of a given population's values in a single model still presents an enduring ethical challenge.",
        "source_doc": "huggingface/blog/blob/main/ethics-soc-4.md"
    },
    {
        "context": "1. **[mBART](https://huggingface.co/docs/transformers/model_doc/mbart)** (from Facebook) released with the paper [Multilingual Denoising Pre-training for Neural Machine Translation](https://arxiv.org/abs/2001.08210) by Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer.\n1. **[mBART-50](https://huggingface.co/docs/transformers/model_doc/mbart)** (from Facebook) released with the paper [Multilingual Translation with Extensible Multilingual Pretraining and Finetuning](https://arxiv.org/abs/2008.00401) by Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, Angela Fan.\n1. **[MEGA](https://huggingface.co/docs/transformers/model_doc/mega)** (from Facebook) released with the paper [Mega: Moving Average Equipped Gated Attention](https://arxiv.org/abs/2209.10655) by Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer.\n1. **[Megatron-BERT](https://huggingface.co/docs/transformers/model_doc/megatron-bert)** (from NVIDIA) released with the paper [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053) by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper and Bryan Catanzaro.\n1. **[Megatron-GPT2](https://huggingface.co/docs/transformers/model_doc/megatron_gpt2)** (from NVIDIA) released with the paper [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053) by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper and Bryan Catanzaro.\n1. **[MGP-STR](https://huggingface.co/docs/transformers/model_doc/mgp-str)** (from Alibaba Research) released with the paper [Multi-Granularity Prediction for Scene Text Recognition](https://arxiv.org/abs/2209.03592) by Peng Wang, Cheng Da, and Cong Yao.",
        "question": "Which model was released with the paper \"Multilingual Denoising Pre-training for Neural Machine Translation\"?\n",
        "answer": " mBART",
        "source_doc": "huggingface/transformers/blob/main/README_zh-hant.md"
    },
    {
        "context": "- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`85ba6de13`](https://github.com/gradio-app/gradio/commit/85ba6de136a45b3e92c74e410bb27e3cbe7138d7) - Adds the ability to build the frontend and backend of custom components in preparation for publishing to pypi using `gradio_component build`.  Thanks [@pngwn](https://github.com/pngwn)!\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`85ba6de13`](https://github.com/gradio-app/gradio/commit/85ba6de136a45b3e92c74e410bb27e3cbe7138d7) - Fix deployed demos on v4 branch.  Thanks [@pngwn](https://github.com/pngwn)!\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`85ba6de13`](https://github.com/gradio-app/gradio/commit/85ba6de136a45b3e92c74e410bb27e3cbe7138d7) - Set api=False for cancel events.  Thanks [@pngwn](https://github.com/pngwn)!\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`85ba6de13`](https://github.com/gradio-app/gradio/commit/85ba6de136a45b3e92c74e410bb27e3cbe7138d7) - Use full path to executables in CLI.  Thanks [@pngwn](https://github.com/pngwn)!\n- [#5949](https://github.com/gradio-app/gradio/pull/5949) [`1c390f101`](https://github.com/gradio-app/gradio/commit/1c390f10199142a41722ba493a0c86b58245da15) - Merge main again.  Thanks [@pngwn](https://github.com/pngwn)!\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`85ba6de13`](https://github.com/gradio-app/gradio/commit/85ba6de136a45b3e92c74e410bb27e3cbe7138d7) - Simplify how files are handled in components in 4.0.  Thanks [@pngwn](https://github.com/pngwn)!\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`85ba6de13`](https://github.com/gradio-app/gradio/commit/85ba6de136a45b3e92c74e410bb27e3cbe7138d7) - Name Endpoints if api_name is None.  Thanks [@pngwn](https://github.com/pngwn)!",
        "question": "What is the name of the commit that simplifies how files are handled in components in 4.0?\n",
        "answer": " The name of the commit is 'Simplify how files are handled in components in 4.0'.",
        "source_doc": "gradio-app/gradio/blob/main/gradio/CHANGELOG.md"
    },
    {
        "context": "The abstract from the paper is:\n\n*There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.*\n\n## UNetMotionModel\n[[autodoc]] UNetMotionModel\n\n## UNet3DConditionOutput\n[[autodoc]] models.unet_3d_condition.UNet3DConditionOutput",
        "question": "What is the architecture of the network presented in the paper?\n",
        "answer": " The architecture of the network presented in the paper consists of a contracting path to capture context and a symmetric expanding path that enables precise localization.",
        "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/models/unet-motion.md"
    },
    {
        "context": "Label Smoothing: 0.1\n    BatchNorm Momentum: 0.99\n  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/efficientnet.py#L1261\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b2_aa-60c94f97.pth\n  Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 80.07%\n      Top 5 Accuracy: 94.9%\n- Name: tf_efficientnet_b3\n  In Collection: TF EfficientNet\n  Metadata:\n    FLOPs: 2275247568\n    Parameters: 12230000\n    File Size: 49381362\n    Architecture:\n    - 1x1 Convolution\n    - Average Pooling\n    - Batch Normalization\n    - Convolution\n    - Dense Connections\n    - Dropout\n    - Inverted Residual Block\n    - Squeeze-and-Excitation Block\n    - Swish\n    Tasks:\n    - Image Classification\n    Training Techniques:\n    - AutoAugment\n    - Label Smoothing\n    - RMSProp\n    - Stochastic Depth\n    - Weight Decay\n    Training Data:\n    - ImageNet\n    ID: tf_efficientnet_b3\n    LR: 0.256\n    Epochs: 350\n    Crop Pct: '0.904'\n    Momentum: 0.9\n    Batch Size: 2048\n    Image Size: '300'\n    Weight Decay: 1.0e-05\n    Interpolation: bicubic\n    RMSProp Decay: 0.9\n    Label Smoothing: 0.1\n    BatchNorm Momentum: 0.99\n  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/efficientnet.py#L1271\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b3_aa-84b4657e.pth\n  Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 81.65%\n      Top 5 Accuracy: 95.72%\n- Name: tf_efficientnet_b4\n  In Collection: TF EfficientNet\n  Metadata:\n    FLOPs: 5749638672\n    Parameters: 19340000\n    File Size: 77989689\n    Architecture:\n    - 1x1 Convolution\n    - Average Pooling\n    - Batch Normalization\n    - Convolution\n    - Dense Connections\n    - Dropout\n    - Inverted Residual Block",
        "question": "What is the FLOPs of tf_efficientnet_b3?\n",
        "answer": " The FLOPs of tf_efficientnet_b3 is 2275247568.",
        "source_doc": "huggingface/pytorch-image-models/blob/main/docs/models/tf-efficientnet.md"
    },
    {
        "context": "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# GPU inference\n\nGPUs are the standard choice of hardware for machine learning, unlike CPUs, because they are optimized for memory bandwidth and parallelism. To keep up with the larger sizes of modern models or to run these large models on existing and older hardware, there are several optimizations you can use to speed up GPU inference. In this guide, you'll learn how to use FlashAttention-2 (a more memory-efficient attention mechanism), BetterTransformer (a PyTorch native fastpath execution), and bitsandbytes to quantize your model to a lower precision. Finally, learn how to use 🤗 Optimum to accelerate inference with ONNX Runtime on Nvidia and AMD GPUs.\n\n<Tip>\n\nThe majority of the optimizations described here also apply to multi-GPU setups!\n\n</Tip>\n\n## FlashAttention-2\n\n<Tip>\n\nFlashAttention-2 is experimental and may change considerably in future versions.\n\n</Tip>\n\n[FlashAttention-2](https://huggingface.co/papers/2205.14135) is a faster and more efficient implementation of the standard attention mechanism that can significantly speedup inference by:\n\n1. additionally parallelizing the attention computation over sequence length\n2. partitioning the work between GPU threads to reduce communication and shared memory reads/writes between them",
        "question": "(your factoid question)\n",
        "answer": "\n\nBitsAndBytes is a library that can quantize your model to a lower precision.\n\n## ONNX Runtime\n\n<Tip>",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/perf_infer_gpu_one.md"
    },
    {
        "context": "This model was contributed by [sayakpaul](https://huggingface.co/sayakpaul). The original code can be found [here](https://github.com/facebookresearch/msn). \n\n## Usage tips\n\n- MSN (masked siamese networks) is a method for self-supervised pre-training of Vision Transformers (ViTs). The pre-training\nobjective is to match the prototypes assigned to the unmasked views of the images to that of the masked views of the same images.\n- The authors have only released pre-trained weights of the backbone (ImageNet-1k pre-training). So, to use that on your own image classification dataset,\nuse the [`ViTMSNForImageClassification`] class which is initialized from [`ViTMSNModel`]. Follow\n[this notebook](https://github.com/huggingface/notebooks/blob/main/examples/image_classification.ipynb) for a detailed tutorial on fine-tuning.\n- MSN is particularly useful in the low-shot and extreme low-shot regimes. Notably, it achieves 75.7% top-1 accuracy with only 1% of ImageNet-1K\nlabels when fine-tuned.\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with ViT MSN.\n\n<PipelineTag pipeline=\"image-classification\"/>\n\n- [`ViTMSNForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\n- See also: [Image classification task guide](../tasks/image_classification)\n\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n## ViTMSNConfig\n\n[[autodoc]] ViTMSNConfig\n\n## ViTMSNModel\n\n[[autodoc]] ViTMSNModel\n    - forward\n\n## ViTMSNForImageClassification\n\n[[autodoc]] ViTMSNForImageClassification\n    - forward",
        "question": "What is the pre-training objective of MSN?\n",
        "answer": " The pre-training objective of MSN is to match the prototypes assigned to the unmasked views of the images to that of the masked views of the same images.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/vit_msn.md"
    },
    {
        "context": "This tutorial shows you how to use an `AutoPipeline` to automatically infer the pipeline class to load for a specific task, given the pretrained weights.\n\n## Choose an AutoPipeline for your task\n\nStart by picking a checkpoint. For example, if you're interested in text-to-image with the [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) checkpoint, use [`AutoPipelineForText2Image`]:\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"peasant and dragon combat, wood cutting style, viking era, bevel with rune\"\n\nimage = pipeline(prompt, num_inference_steps=25).images[0]\nimage\n```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-text2img.png\" alt=\"generated image of peasant fighting dragon in wood cutting style\"/>\n</div>\n\nUnder the hood, [`AutoPipelineForText2Image`]:\n\n1. automatically detects a `\"stable-diffusion\"` class from the [`model_index.json`](https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/model_index.json) file\n2. loads the corresponding text-to-image [`StableDiffusionPipeline`] based on the `\"stable-diffusion\"` class name\n\nLikewise, for image-to-image, [`AutoPipelineForImage2Image`] detects a `\"stable-diffusion\"` checkpoint from the `model_index.json` file and it'll load the corresponding [`StableDiffusionImg2ImgPipeline`] behind the scenes. You can also pass any additional arguments specific to the pipeline class such as `strength`, which determines the amount of noise or variation added to an input image:\n\n```py\nfrom diffusers import AutoPipelineForImage2Image\nimport torch\nimport requests\nfrom PIL import Image\nfrom io import BytesIO",
        "question": "(your factoid question)\n",
        "answer": " (your answer to the factoid question)\n\nNow here is the context.\n\nContext: This tutorial shows you how to use an `AutoPipeline` to automatically infer the pipeline class to load for a specific task, given the pretrained weights.\n\n## Choose an AutoPipeline for your task\n\nStart by picking a checkpoint. For example, if you're interested in text-to-image with the [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) checkpoint, use [`AutoPipelineForText2Image`]:\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"peasant and dragon combat, wood cutting style, viking era, bevel with rune\"\n\nimage = pipeline(prompt, num_inference_steps=25).images[0]\nimage\n```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-text2img.png\" alt=\"generated image of peasant fighting dragon in wood cutting style\"/>\n</div>\n\nUnder the hood, [`AutoPipelineForText2Image`]:\n\n1. automatically detects a `\"stable-diffusion\"` class from the [`model_index.json`](https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/model_index.json) file\n2. loads the corresponding text-to-image [`StableDiffusionPipeline`] based on the `\"stable-diffusion\"` class name\n\nLikewise, for image-to-image, [`AutoPipelineForImage2Image`] detects a `\"stable-diffusion\"` checkpoint from the `model_index.json` file and it'll load the corresponding [`StableDiffusionImg2ImgPipeline`] behind the scenes. You can also pass any additional arguments specific to the pipeline class such as `strength`, which determines the amount of noise or variation added to an input image:\n\n```py\nfrom diffusers import AutoPipelineForImage2Image\nimport torch\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\n\nOutput:::\n\npipeline = AutoPipelineForImage2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\n\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-img2img.png\"\nresponse = requests.get(url)\nimage = Image.open(BytesIO(response.content))\n\nstrength = 0.7\nimage = pipeline(image, strength=strength).images[0]\nimage\n```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-img2img.png\" alt=\"generated image of a cat with a hat\"/>\n</div>\n\n## AutoPipeline for other tasks\n\n`AutoPipeline` can also be used for other tasks such as [`AutoPipelineForConditionalGeneration`] for text generation, [`AutoPipelineForImageGeneration`] for image generation, and [`AutoPipelineForImageSegmentation`] for image segmentation.\n\nFor example, to use [`AutoPipelineForConditionalGeneration`] for text generation, you can use the following code:\n\n```py\nfrom diffusers import AutoPipelineForConditionalGeneration\nimport torch\n\npipeline = AutoPipelineForConditionalGeneration.from_pretrained(\n    \"EleutherAI/gpt-neo-1.3B\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\n\nprompt = \"",
        "source_doc": "huggingface/diffusers/blob/main/docs/source/en/tutorials/autopipeline.md"
    },
    {
        "context": "```python out\nValueError: No gradients provided for any variable: ['tf_distil_bert_for_sequence_classification/distilbert/embeddings/word_embeddings/weight:0', '...']\n```\n\nWhat does that mean? We tried to train on our data, but we got no gradient? This is pretty perplexing; how do we even begin to debug something like that? When the error you get doesn't immediately suggest where the problem is, the best solution is often to walk through things in sequence, making sure at each stage that everything looks right. And of course, the place to start is always to...\n\n### Check your data[[check-your-data]]\n\nThis goes without saying, but if your data is corrupted, Keras is not going to be able to fix it for you. So first things first, you need to have a look at what is inside your training set.\n\nAlthough it's tempting to look inside `raw_datasets` and `tokenized_datasets`, we highly recommend you go to the data right at the point where it's going to enter the model. That means reading an output from the `tf.data.Dataset` you created with the `to_tf_dataset()` function! So how do we do that? `tf.data.Dataset` objects give us whole batches at a time and don't support indexing, so we can't just ask for `train_dataset[0]`. We can, however, ask it politely for a batch:\n\n```py\nfor batch in train_dataset:\n    break\n```\n\n`break` ends the loop after one iteration, so this grabs the first batch that comes out of `train_dataset` and saves it as `batch`. Now, let's take a look at what's inside:",
        "question": "(your factoid question)\n",
        "answer": " (your answer to the factoid question)\n\nNow here is the context.\n\nContext: ```python out\nValueError: No gradients provided for any variable: ['tf_distil_bert_for_sequence_classification/distilbert/embeddings/word_embeddings/weight:0', '...']\n```\n\nWhat does that mean? We tried to train on our data, but we got no gradient? This is pretty perplexing; how do we even begin to debug something like that? When the error you get doesn't immediately suggest where the problem is, the best solution is often to walk through things in sequence, making sure at each stage that everything looks right. And of course, the place to start is always to...\n\n### Check your data[[check-your-data]]\n\nThis goes without saying, but if your data is corrupted, Keras is not going to be able to fix it for you. So first things first, you need to have a look at what is inside your training set.\n\nAlthough it's tempting to look inside `raw_datasets` and `tokenized_datasets`, we highly recommend you go to the data right at the point where it's going to enter the model. That means reading an output from the `tf.data.Dataset` you created with the `to_tf_dataset()` function! So how do we do that? `tf.data.Dataset` objects give us whole batches at a time and don't support indexing, so we can't just ask for `train_dataset[0]`. We can, however, ask it politely for a batch:\n\n```py\nfor batch in train_dataset:\n    break\n```\n\n`break` ends the loop after one iteration, so this grabs the first batch that comes out of `train_dataset` and saves it as `batch`. Now, let's take a look at what's inside:\n\nOutput:::\n\n```\n<ipython-input-1-1234567890abcdef> in <module>\n      1 for batch in train_dataset:\n----> 2     break\n\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n```\n\nOh no! We got an error! But this error is not about our data, it's about the `break` statement. The `for` loop is expecting a boolean value, and `break` is not a boolean. We can fix this by wrapping the `break` statement in an `if` statement, like this:\n\n```py\nfor batch in train_dataset:\n    if True:\n        break\n```\n\nNow, let's take a look at what's inside:\n\nOutput:::\n\n```\n<ipython-input-1-1234567890abcdef> in <module>\n      1 for batch in train_dataset:\n      2     if True:\n----> 3         break\n\nTypeError: 'BatchDataset' object is not iterable\n```\n\nOh no! We got another error! But this error is not about our `break` statement. The `for` loop is expecting an iterable, and `train_dataset` is not an iterable. We can fix this by wrapping `train_dataset` in a `list` function, like this:\n\n```py\nlist(train_dataset)[0]\n```\n\nNow, let's take a look at what's inside:\n\nOutput:::\n\n```\n<ipython-input-1-1234567890abcdef",
        "source_doc": "huggingface/course/blob/main/chapters/en/chapter8/4_tf.mdx"
    },
    {
        "context": "## Create a model dictionary 💾 and publish our trained model to the Hub 🔥\n\n- We create a model dictionary that will contain all the training hyperparameters for reproducibility and the Q-Table.\n\n\n```python\nmodel = {\n    \"env_id\": env_id,\n    \"max_steps\": max_steps,\n    \"n_training_episodes\": n_training_episodes,\n    \"n_eval_episodes\": n_eval_episodes,\n    \"eval_seed\": eval_seed,\n    \"learning_rate\": learning_rate,\n    \"gamma\": gamma,\n    \"max_epsilon\": max_epsilon,\n    \"min_epsilon\": min_epsilon,\n    \"decay_rate\": decay_rate,\n    \"qtable\": Qtable_taxi,\n}\n```\n\n```python\nusername = \"\"  # FILL THIS\nrepo_name = \"\"  # FILL THIS\npush_to_hub(repo_id=f\"{username}/{repo_name}\", model=model, env=env)\n```\n\nNow that it's on the Hub, you can compare the results of your Taxi-v3 with your classmates using the leaderboard 🏆 👉 https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard\n\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/taxi-leaderboard.png\" alt=\"Taxi Leaderboard\">\n\n# Part 3: Load from Hub 🔽\n\nWhat's amazing with Hugging Face Hub 🤗 is that you can easily load powerful models from the community.\n\nLoading a saved model from the Hub is really easy:\n\n1. You go https://huggingface.co/models?other=q-learning to see the list of all the q-learning saved models.\n2. You select one and copy its repo_id\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/copy-id.png\" alt=\"Copy id\">\n\n3. Then we just need to use `load_from_hub` with:\n- The repo_id\n- The filename: the saved model inside the repo.\n\n#### Do not modify this code\n\n```python\nfrom urllib.error import HTTPError\n\nfrom huggingface_hub import hf_hub_download",
        "question": "What is the filename of the saved model on the Hugging Face Hub?\n",
        "answer": " The filename of the saved model on the Hugging Face Hub is \"model.pt\".",
        "source_doc": "huggingface/deep-rl-class/blob/main/units/en/unit2/hands-on.mdx"
    },
    {
        "context": "`@gradio/button`\n\n```html\n<script>\n\timport { Button } from \"@gradio/button\";\n</script>\n\n<button type=\"primary|secondary\" href=\"string\" on:click=\"{e.detail === href}\">\n\tcontent\n</button>\n```",
        "question": "What is the type of the button in the given code snippet?\n",
        "answer": " The type of the button in the given code snippet is \"primary\" or \"secondary\".",
        "source_doc": "gradio-app/gradio/blob/main/js/tootils/README.md"
    },
    {
        "context": "def bar_plot_fn(display):\n    if display == \"simple\":\n        return gr.BarPlot(\n            simple,\n            x=\"a\",\n            y=\"b\",\n            title=\"Simple Bar Plot with made up data\",\n            tooltip=[\"a\", \"b\"],\n            y_lim=[20, 100],\n        )\n    elif display == \"stacked\":\n        return gr.BarPlot(\n            fake_barley,\n            x=\"variety\",\n            y=\"yield\",\n            color=\"site\",\n            title=\"Barley Yield Data\",\n            tooltip=[\"variety\", \"site\"],\n        )\n    elif display == \"grouped\":\n        return gr.BarPlot(\n            fake_barley.astype({\"year\": str}),\n            x=\"year\",\n            y=\"yield\",\n            color=\"year\",\n            group=\"site\",\n            title=\"Barley Yield by Year and Site\",\n            group_title=\"\",\n            tooltip=[\"yield\", \"site\", \"year\"],\n        )\n    elif display == \"simple-horizontal\":\n        return gr.BarPlot(\n            simple,\n            x=\"a\",\n            y=\"b\",\n            x_title=\"Variable A\",\n            y_title=\"Variable B\",\n            title=\"Simple Bar Plot with made up data\",\n            tooltip=[\"a\", \"b\"],\n            vertical=False,\n            y_lim=[20, 100],\n        )\n    elif display == \"stacked-horizontal\":\n        return gr.BarPlot(\n            fake_barley,\n            x=\"variety\",\n            y=\"yield\",\n            color=\"site\",\n            title=\"Barley Yield Data\",\n            vertical=False,\n            tooltip=[\"variety\", \"site\"],\n        )\n    elif display == \"grouped-horizontal\":\n        return gr.BarPlot(\n            fake_barley.astype({\"year\": str}),\n            x=\"year\",\n            y=\"yield\",\n            color=\"year\",\n            group=\"site\",\n            title=\"Barley Yield by Year and Site\",\n            group_title=\"\",\n            tooltip=[\"yield\", \"site\", \"year\"],\n            vertical=False,\n        )",
        "question": "What is the name of the first function argument in the `bar_plot_fn` function?\n",
        "answer": " display",
        "source_doc": "gradio-app/gradio/blob/main/demo/bar_plot/run.ipynb"
    },
    {
        "context": "!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# Command Line Interface (CLI)\n\nThe `huggingface_hub` Python package comes with a built-in CLI called `huggingface-cli`. This tool allows you to interact with the Hugging Face Hub directly from a terminal. For example, you can login to your account, create a repository, upload and download files, etc. It also comes with handy features to configure your machine or manage your cache. In this guide, we will have a look at the main features of the CLI and how to use them.\n\n## Getting started\n\nFirst of all, let's install the CLI:\n\n```\n>>> pip install -U \"huggingface_hub[cli]\"\n```\n\n<Tip>\n\nIn the snippet above, we also installed the `[cli]` extra dependencies to make the user experience better, especially when using the `delete-cache` command.\n\n</Tip>\n\nOnce installed, you can check that the CLI is correctly setup:\n\n```\n>>> huggingface-cli --help\nusage: huggingface-cli <command> [<args>]\n\npositional arguments:\n  {env,login,whoami,logout,repo,upload,download,lfs-enable-largefiles,lfs-multipart-upload,scan-cache,delete-cache}\n                        huggingface-cli command helpers\n    env                 Print information about the environment.\n    login               Log in using a token from huggingface.co/settings/tokens\n    whoami              Find out which huggingface.co account you are logged in as.\n    logout              Log out\n    repo                {create} Commands to interact with your huggingface.co repos.\n    upload              Upload a file or a folder to a repo on the Hub\n    download            Download files from the Hub\n    lfs-enable-largefiles\n                        Configure your repository to enable upload of files > 5GB.\n    scan-cache          Scan cache directory.\n    delete-cache        Delete revisions from the cache directory.",
        "question": "(your factoid question)\n",
        "answer": " (your answer to the factoid question)\n\nNow here is the context.\n\nContext: !--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# Command Line Interface (CLI)\n\nThe `huggingface_hub` Python package comes with a built-in CLI called `huggingface-cli`. This tool allows you to interact with the Hugging Face Hub directly from a terminal. For example, you can login to your account, create a repository, upload and download files, etc. It also comes with handy features to configure your machine or manage your cache. In this guide, we will have a look at the main features of the CLI and how to use them.\n\n## Getting started\n\nFirst of all, let's install the CLI:\n\n```\n>>> pip install -U \"huggingface_hub[cli]\"\n```\n\n<Tip>\n\nIn the snippet above, we also installed the `[cli]` extra dependencies to make the user experience better, especially when using the `delete-cache` command.\n\n</Tip>\n\nOnce installed, you can check that the CLI is correctly setup:\n\n```\n>>> huggingface-cli --help\nusage: huggingface-cli <command> [<args>]\n\npositional arguments:\n  {env,login,whoami,logout,repo,upload,download,lfs-enable-largefiles,lfs-multipart-upload,scan-cache,delete-cache}\n                        huggingface-cli command helpers\n    env                 Print information about the environment.\n    login               Log in using a token from huggingface.co/settings/tokens\n    whoami              Find out which huggingface.co account you are logged in as.\n    logout              Log out\n    repo                {create} Commands to interact with your huggingface.co repos.\n    upload              Upload a file or a folder to a repo on the Hub\n    download            Download files from the Hub\n    lfs-enable-largefiles\n                        Configure your repository to enable upload of files > 5GB.\n    scan-cache          Scan cache directory.\n    delete-cache        Delete revisions from the cache directory.\n\nOutput:::\n\nThe output above shows the list of available commands.\n\n</Tip>\n\n## Environment\n\nThe `env` command allows you to check the configuration of the CLI.\n\n```\n>>> huggingface-cli env\nhuggingface-cli version: 0.1.0\nhuggingface.co token: <not set>\ncache directory: /home/user/.cache/huggingface/hub\n```\n\n<Tip>\n\nThe `cache directory` is the directory where the CLI stores the files you downloaded from the Hub.\n\n</Tip>\n\n## Login\n\nTo login to your Hugging Face account, you can use the `login` command:\n\n```\n>>> huggingface-cli login\nLogging in to huggingface.co...\nLogin successful\nYour token is saved in /home/user/.cache/huggingface/hub/token\n```\n\n<Tip>\n\nThe token is saved in the cache directory.\n\n</Tip>\n\n## Whoami\n\nTo check which account you are logged in, you can use the `whoami` command:\n\n```\n>>> huggingface-cli whoami\n<username>\n```\n\n## Logout\n\nTo logout from your Hugging Face account, you can use the `logout` command:\n\n```\n>>> huggingface-cli logout\nLogged out from huggingface.co\n```\n\n## Repository\n\nThe `repo` command allows you to interact with your Hugging Face repositories.\n\n### Create\n\nTo create a new repository, you can use the `create` command:\n\n```\n>>> huggingface-cli repo create <repo-name>\nCreating new repository <repo-name",
        "source_doc": "huggingface/huggingface_hub/blob/main/docs/source/en/guides/cli.md"
    },
    {
        "context": "From Q-Learning to Deep Q-Learning [[from-q-to-dqn]]\n\nWe learned that **Q-Learning is an algorithm we use to train our Q-Function**, an **action-value function** that determines the value of being at a particular state and taking a specific action at that state.\n\n<figure>\n  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function.jpg\" alt=\"Q-function\"/>\n</figure>\n\nThe **Q comes from \"the Quality\" of that action at that state.**\n\nInternally, our Q-function is encoded by **a Q-table, a table where each cell corresponds to a state-action pair value.** Think of this Q-table as **the memory or cheat sheet of our Q-function.**\n\nThe problem is that Q-Learning is a *tabular method*. This becomes a problem if the states and actions spaces **are not small enough to be represented efficiently by arrays and tables**. In other words: it is **not scalable**.\nQ-Learning worked well with small state space environments like:\n\n- FrozenLake, we had 16 states.\n- Taxi-v3, we had 500 states.\n\nBut think of what we're going to do today: we will train an agent to learn to play Space Invaders, a more complex game, using the frames as input.\n\nAs **[Nikita Melkozerov mentioned](https://twitter.com/meln1k), Atari environments** have an observation space with a shape of (210, 160, 3)*, containing values ranging from 0 to 255 so that gives us \\\\(256^{210 \\times 160 \\times 3} = 256^{100800}\\\\) possible observations (for comparison, we have approximately \\\\(10^{80}\\\\) atoms in the observable universe).\n\n* A single frame in Atari is composed of an image of 210x160 pixels. Given that the images are in color (RGB), there are 3 channels. This is why the shape is (210, 160, 3). For each pixel, the value can go from 0 to 255.\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/atari.jpg\" alt=\"Atari State Space\"/>",
        "question": "How many possible observations are there in Atari environments?\n",
        "answer": " There are 256^100800 possible observations in Atari environments.",
        "source_doc": "huggingface/deep-rl-class/blob/main/units/en/unit3/from-q-to-dqn.mdx"
    },
    {
        "context": ">>> model_id = \"roberta-base\"\n>>> model = AutoModel.from_pretrained(model_id, device_map=\"auto\")\n```\n\n## Step 2: Set your model on your preferred device\n\nIf you did not used `device_map=\"auto\"` to load your model (or if your model does not support `device_map=\"auto\"`), you can manually set your model to a GPU:\n```python\n>>> model = model.to(0) # or model.to(\"cuda:0\")\n```\n\n## Step 3: Convert your model to BetterTransformer!\n\nNow time to convert your model using `BetterTransformer` API! You can run the commands below:\n\n```python\n>>> from optimum.bettertransformer import BetterTransformer\n\n>>> model = BetterTransformer.transform(model)\n```\nBy default, `BetterTransformer.transform` will overwrite your model, which means that your previous native model cannot be used anymore. If you want to keep it for some reasons, just add the flag `keep_original_model=True`!\n```python\n>>> from optimum.bettertransformer import BetterTransformer\n\n>>> model_bt = BetterTransformer.transform(model, keep_original_model=True)\n```\nIf your model does not support the `BetterTransformer` API, this will be displayed on an error trace. Note also that decoder-based models (OPT, BLOOM, etc.) are not supported yet but this is in the roadmap of PyTorch for the future.\n\n## Pipeline compatibility\n\n[Transformer's pipeline](https://huggingface.co/docs/transformers/main_classes/pipelines) is also compatible with this integration and you can use `BetterTransformer` as an accelerator for your pipelines. The code snippet below shows how:\n```python\n>>> from optimum.pipelines import pipeline\n\n>>> pipe = pipeline(\"fill-mask\", \"distilbert-base-uncased\", accelerator=\"bettertransformer\")\n>>> pipe(\"I am a student at [MASK] University.\")\n```\nIf you want to run a pipeline on a GPU device, run:\n```python\n>>> from optimum.pipelines import pipeline\n\n>>> pipe = pipeline(\"fill-mask\", \"distilbert-base-uncased\", accelerator=\"bettertransformer\", device=0)\n>>> ...\n```",
        "question": "What is the model id used to load the model?\n",
        "answer": " The model id used to load the model is \"roberta-base\".",
        "source_doc": "huggingface/optimum/blob/main/docs/source/bettertransformer/tutorials/convert.mdx"
    },
    {
        "context": "In this guide, we will go over the effective techniques for efficient LLM deployment:\n\n1.  **Lower Precision:** Research has shown that operating at reduced numerical precision, namely [8-bit and 4-bit](./main_classes/quantization.md) can achieve computational advantages without a considerable decline in model performance.\n\n2.  **Flash Attention:** Flash Attention is a variation of the attention algorithm that not only provides a more memory-efficient approach but also realizes increased efficiency due to optimized GPU memory utilization.\n\n3.  **Architectural Innovations:** Considering that LLMs are always deployed in the same way during inference, namely autoregressive text generation with a long input context, specialized model architectures have been proposed that allow for more efficient inference. The most important advancement in model architectures hereby are [Alibi](https://arxiv.org/abs/2108.12409), [Rotary embeddings](https://arxiv.org/abs/2104.09864), [Multi-Query Attention (MQA)](https://arxiv.org/abs/1911.02150) and [Grouped-Query-Attention (GQA)]((https://arxiv.org/abs/2305.13245)).\n\nThroughout this guide, we will offer an analysis of auto-regressive generation from a tensor's perspective. We delve into the pros and cons of adopting lower precision, provide a comprehensive exploration of the latest attention algorithms, and discuss improved LLM architectures. While doing so, we run practical examples showcasing each of the feature improvements.\n\n## 1. Lower Precision\n\nMemory requirements of LLMs can be best understood by seeing the LLM as a set of weight matrices and vectors and the text inputs as a sequence of vectors. In the following, the definition *weights* will be used to signify all model weight matrices and vectors.",
        "question": "(your factoid question)\n",
        "answer": " One technique for efficient LLM deployment that involves operating at reduced numerical precision is Lower Precision, which can achieve computational advantages without a considerable decline in model performance. This can be done using 8-bit and 4-bit numerical precision.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/llm_tutorial_optimization.md"
    },
    {
        "context": "The second part (the bullet points below *\"Tools\"*) is dynamically added upon calling `run` or `chat`. There are \nexactly as many bullet points as there are tools in `agent.toolbox` and each bullet point consists of the name \nand description of the tool:\n\n```text\n- <tool.name>: <tool.description>\n```\n\nLet's verify this quickly by loading the document_qa tool and printing out the name and description.\n\n```py\nfrom transformers import load_tool\n\ndocument_qa = load_tool(\"document-question-answering\")\nprint(f\"- {document_qa.name}: {document_qa.description}\")\n```\n\nwhich gives:\n```text\n- document_qa: This is a tool that answers a question about a document (pdf). It takes an input named `document` which should be the document containing the information, as well as a `question` that is the question about the document. It returns a text that contains the answer to the question.\n```\n\nWe can see that the tool name is short and precise. The description includes two parts, the first explaining \nwhat the tool does and the second states what input arguments and return values are expected.\n\nA good tool name and tool description are very important for the agent to correctly use it. Note that the only\ninformation the agent has about the tool is its name and description, so one should make sure that both \nare precisely written and match the style of the existing tools in the toolbox. In particular make sure the description\nmentions all the arguments expected by name in code-style, along with the expected type and a description of what they\nare.\n\n<Tip>\n\nCheck the naming and description of the curated Transformers tools to better understand what name and \ndescription a tool is expected to have. You can see all tools with the [`Agent.toolbox`] property.\n\n</Tip>",
        "question": "(your factoid question)\n",
        "answer": " The name of the tool is 'document-question-answering'.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/custom_tools.md"
    },
    {
        "context": "A language model trained for [causal language modeling](tasks/language_modeling) takes a sequence of text tokens as input and returns the probability distribution for the next token.\n\n<!-- [GIF 1 -- FWD PASS] -->\n<figure class=\"image table text-center m-0 w-full\">\n    <video\n        style=\"max-width: 90%; margin: auto;\"\n        autoplay loop muted playsinline\n        src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_1_1080p.mov\"\n    ></video>\n    <figcaption>\"Forward pass of an LLM\"</figcaption>\n</figure>\n\nA critical aspect of autoregressive generation with LLMs is how to select the next token from this probability distribution. Anything goes in this step as long as you end up with a token for the next iteration. This means it can be as simple as selecting the most likely token from the probability distribution or as complex as applying a dozen transformations before sampling from the resulting distribution.\n\n<!-- [GIF 2 -- TEXT GENERATION] -->\n<figure class=\"image table text-center m-0 w-full\">\n    <video\n        style=\"max-width: 90%; margin: auto;\"\n        autoplay loop muted playsinline\n        src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_2_1080p.mov\"\n    ></video>\n    <figcaption>\"Autoregressive generation iteratively selects the next token from a probability distribution to generate text\"</figcaption>\n</figure>\n\nThe process depicted above is repeated iteratively until some stopping condition is reached. Ideally, the stopping condition is dictated by the model, which should learn when to output an end-of-sequence (`EOS`) token. If this is not the case, generation stops when some predefined maximum length is reached.",
        "question": "What is the stopping condition for autoregressive generation?\n",
        "answer": " The stopping condition for autoregressive generation is either when the model outputs an end-of-sequence (`EOS`) token or when a predefined maximum length is reached.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/llm_tutorial.md"
    },
    {
        "context": "### Fixes\n\n- [#6368](https://github.com/gradio-app/gradio/pull/6368) [`8a3f45c26`](https://github.com/gradio-app/gradio/commit/8a3f45c2612a36112d797465e14cd6f1801ccbd9) - Fix component update bug.  Thanks [@freddyaboulton](https://github.com/freddyaboulton)!\n- [#6322](https://github.com/gradio-app/gradio/pull/6322) [`6204ccac5`](https://github.com/gradio-app/gradio/commit/6204ccac5967763e0ebde550d04d12584243a120) - Fixes `gr.load()` so it works properly with Images and Examples.  Thanks [@abidlabs](https://github.com/abidlabs)!\n- [#6323](https://github.com/gradio-app/gradio/pull/6323) [`55fda81fa`](https://github.com/gradio-app/gradio/commit/55fda81fa5918b48952729232d6e2fc55af9351d) - Textbox and Code Component Blur/Focus Fixes.  Thanks [@dawoodkhan82](https://github.com/dawoodkhan82)!\n\n## 4.1.2\n\n### Features\n\n- [#6318](https://github.com/gradio-app/gradio/pull/6318) [`d3b53a457`](https://github.com/gradio-app/gradio/commit/d3b53a4577ea05cd27e37ce7fec952028c18ed45) - Fix for stylized DataFrame.  Thanks [@abidlabs](https://github.com/abidlabs)!\n- [#6326](https://github.com/gradio-app/gradio/pull/6326) [`ed546f2e1`](https://github.com/gradio-app/gradio/commit/ed546f2e13915849b0306d017c40933b856bb792) - Fix Model3D template.  Thanks [@freddyaboulton](https://github.com/freddyaboulton)!\n\n### Fixes",
        "question": "What is the commit hash for the fix of the component update bug?\n",
        "answer": " The commit hash for the fix of the component update bug is `8a3f45c2612a36112d797465e14cd6f1801ccbd9`.",
        "source_doc": "gradio-app/gradio/blob/main/gradio/CHANGELOG.md"
    },
    {
        "context": "- [用 DistilBERT 做问答](https://huggingface.co/distilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%28Portuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+forest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belonging+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees+divided+into+16%2C000+species)\n- [用 T5 做翻译](https://huggingface.co/t5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)",
        "question": "What percentage of the Amazon rainforest is contained within Brazil?\n",
        "answer": " 60%",
        "source_doc": "huggingface/transformers/blob/main/README_zh-hans.md"
    },
    {
        "context": "> We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimizers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.\n\n## Resources\n\nThis blogpost and release come with several resources to get started with 4bit models and QLoRA:",
        "question": "What is the name of the best model family introduced in the study?\n",
        "answer": " The name of the best model family introduced in the study is Guanaco.",
        "source_doc": "huggingface/blog/blob/main/4bit-transformers-bitsandbytes.md"
    },
    {
        "context": "This metric outputs a float representing the character error rate.\n\n```\nprint(cer_score)\n0.34146341463414637\n```\n\nThe **lower** the CER value, the **better** the performance of the ASR system, with a CER of 0 being a perfect score. \n\nHowever, CER's output is not always a number between 0 and 1, in particular when there is a high number of insertions (see [Examples](#Examples) below).\n\n### Values from popular papers\n\nThis metric is highly dependent on the content and quality of the dataset, and therefore users can expect very different values for the same model but on different datasets.\n\nMultilingual datasets such as [Common Voice](https://huggingface.co/datasets/common_voice) report different CERs depending on the language, ranging from 0.02-0.03 for languages such as French and Italian, to 0.05-0.07 for English (see [here](https://github.com/speechbrain/speechbrain/tree/develop/recipes/CommonVoice/ASR/CTC) for more values).\n\n## Examples \n\nPerfect match between prediction and reference:\n\n```python\nfrom evaluate import load\ncer = load(\"cer\")\npredictions = [\"hello world\", \"good night moon\"]\nreferences = [\"hello world\", \"good night moon\"]\ncer_score = cer.compute(predictions=predictions, references=references)\nprint(cer_score)\n0.0\n```\n\nPartial match between prediction and reference:\n\n```python\nfrom evaluate import load\ncer = load(\"cer\")\npredictions = [\"this is the prediction\", \"there is an other sample\"]\nreferences = [\"this is the reference\", \"there is another one\"]\ncer_score = cer.compute(predictions=predictions, references=references)\nprint(cer_score)\n0.34146341463414637\n```\n\nNo match between prediction and reference:\n\n```python\nfrom evaluate import load\ncer = load(\"cer\")\npredictions = [\"hello\"]\nreferences = [\"gracias\"]\ncer_score = cer.compute(predictions=predictions, references=references)\nprint(cer_score)\n1.0\n```\n\nCER above 1 due to insertion errors:",
        "question": "What is the character error rate of the ASR system?\n",
        "answer": " The character error rate of the ASR system is 0.34146341463414637.",
        "source_doc": "huggingface/evaluate/blob/main/metrics/cer/README.md"
    },
    {
        "context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# LMSDiscreteScheduler\n\n`LMSDiscreteScheduler` is a linear multistep scheduler for discrete beta schedules. The scheduler is ported from and created by [Katherine Crowson](https://github.com/crowsonkb/), and the original implementation can be found at [crowsonkb/k-diffusion](https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L181).\n\n## LMSDiscreteScheduler\n[[autodoc]] LMSDiscreteScheduler\n\n## LMSDiscreteSchedulerOutput\n[[autodoc]] schedulers.scheduling_lms_discrete.LMSDiscreteSchedulerOutput",
        "question": "What is the name of the scheduler ported from Katherine Crowson's implementation?\n",
        "answer": " The name of the scheduler ported from Katherine Crowson's implementation is LMSDiscreteScheduler.",
        "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/schedulers/lms_discrete.md"
    },
    {
        "context": "To combine datasets, as in Section 6.2, try something like:\n```bash\ncurl -S https://cdn-datasets.huggingface.co/pseudo/xsum/bart_xsum_pl.tgz | tar -xvz -C .\ncurl -S https://cdn-datasets.huggingface.co/pseudo/xsum/pegasus_xsum.tgz | tar -xvz -C .\ncurl -S https://cdn-datasets.huggingface.co/summarization/xsum.tar.gz | tar -xvz -C .\nmkdir all_pl\ncat bart_xsum_pl/train.source pegasus_xsum/train.source xsum/train.source > all_pl/train.source\ncat bart_xsum_pl/train.target pegasus_xsum/train.target xsum/train.target > all_pl/train.target\ncp xsum/val* all_pl\ncp xsum/test* all_pl\n```\nthen use `all_pl` as DATA in the command above.\n\n#### Direct Knowledge Distillation (KD)\n+ In this method, we use try to enforce that the student and teacher produce similar encoder_outputs, logits, and hidden_states using `SummarizationDistiller`.\n+ This method was used for `sshleifer/distilbart-xsum-12-6`, `6-6`, and `9-6` checkpoints were produced.\n+ You must use [`distillation.py`](./distillation.py). Note that this command initializes the student for you.\n\nThe command that produced `sshleifer/distilbart-xsum-12-6` is at [./train_distilbart_xsum.sh](train_distilbart_xsum.sh)\n```bash\n./train_distilbart_xsum.sh --logger_name wandb --gpus 1\n```\n\n+ Expected ROUGE-2 between 21.3 and 21.6, run time ~13H.\n+ direct KD + Pegasus is VERY slow and works best with `--supervise_forward --normalize_hidden`.\n\n<!--- runtime: 13H on V-100 16GB GPU. -->\n\n### Citation",
        "question": "What is the expected ROUGE-2 score for the direct KD + Pegasus model?\n",
        "answer": " The expected ROUGE-2 score for the direct KD + Pegasus model is between 21.3 and 21.6.",
        "source_doc": "huggingface/transformers/blob/main/examples/research_projects/seq2seq-distillation/README.md"
    },
    {
        "context": "[[autodoc]] FlavaImageCodebook\n    - forward\n    - get_codebook_indices\n    - get_codebook_probs\n\n## FlavaTextModel\n\n[[autodoc]] FlavaTextModel\n    - forward\n\n## FlavaImageModel\n\n[[autodoc]] FlavaImageModel\n    - forward\n\n## FlavaMultimodalModel\n\n[[autodoc]] FlavaMultimodalModel\n    - forward",
        "question": "(your factoid question)\n",
        "answer": " (your answer to the factoid question)\n\nNow here is the context.\n\nContext: [[autodoc]] FlavaImageCodebook\n    - forward\n    - get_codebook_indices\n    - get_codebook_probs\n\n## FlavaTextModel\n\n[[autodoc]] FlavaTextModel\n    - forward\n\n## FlavaImageModel\n\n[[autodoc]] FlavaImageModel\n    - forward\n\n## FlavaMultimodalModel\n\n[[autodoc]] FlavaMultimodalModel\n    - forward\n\nOutput:::\n\nFlavaImageModel is a class in the Flava library that inherits from the PyTorch Module class. It has a forward method that takes in an image tensor and returns a tensor of shape (batch\\_size, num\\_classes) containing the predicted probabilities for each class.\n\nFlavaTextModel is a class in the Flava library that inherits from the PyTorch Module class. It has a forward method that takes in a text tensor and returns a tensor of shape (batch\\_size, num\\_classes) containing the predicted probabilities for each class.\n\nFlavaMultimodalModel is a class in the Flava library that inherits from the PyTorch Module class. It has a forward method that takes in a tuple of image and text tensors and returns a tensor of shape (batch\\_size, num\\_classes) containing the predicted probabilities for each class.\n\nFlavaImageCodebook is a function in the Flava library that takes in a string of text and returns a tuple of two tensors. The first tensor is of shape (batch\\_size, num\\_classes) and contains the predicted probabilities for each class. The second tensor is of shape (batch\\_size, num\\_classes, num\\_classes) and contains the codebook probabilities for each class.\n\nget\\_codebook\\_indices is a function in the Flava library that takes in a tensor of shape (batch\\_size, num\\_classes) and returns a tensor of shape (batch\\_size, num\\_classes) containing the indices of the maximum probability for each sample.\n\nget\\_codebook\\_probs is a function in the Flava library that takes in a tensor of shape (batch\\_size, num\\_classes) and returns a tensor of shape (batch\\_size, num\\_classes) containing the codebook probabilities for each class.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/flava.md"
    },
    {
        "context": "Res2NeXt\n\n**Res2NeXt** is an image model that employs a variation on [ResNeXt](https://paperswithcode.com/method/resnext) bottleneck residual blocks. The motivation is to be able to represent features at multiple scales. This is achieved through a novel building block for CNNs that constructs hierarchical residual-like connections within one single residual block. This represents multi-scale features at a granular level and increases the range of receptive fields for each network layer.\n\n## How do I use this model on an image?\n\nTo load a pretrained model:\n\n```py\n>>> import timm\n>>> model = timm.create_model('res2next50', pretrained=True)\n>>> model.eval()\n```\n\nTo load and preprocess the image:\n\n```py \n>>> import urllib\n>>> from PIL import Image\n>>> from timm.data import resolve_data_config\n>>> from timm.data.transforms_factory import create_transform\n\n>>> config = resolve_data_config({}, model=model)\n>>> transform = create_transform(**config)\n\n>>> url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n>>> urllib.request.urlretrieve(url, filename)\n>>> img = Image.open(filename).convert('RGB')\n>>> tensor = transform(img).unsqueeze(0) # transform and add batch dimension\n```\n\nTo get the model predictions:\n\n```py\n>>> import torch\n>>> with torch.no_grad():\n...     out = model(tensor)\n>>> probabilities = torch.nn.functional.softmax(out[0], dim=0)\n>>> print(probabilities.shape)\n>>> # prints: torch.Size([1000])\n```\n\nTo get the top-5 predictions class names:\n\n```py\n>>> # Get imagenet class mappings\n>>> url, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\n>>> urllib.request.urlretrieve(url, filename) \n>>> with open(\"imagenet_classes.txt\", \"r\") as f:\n...     categories = [s.strip() for s in f.readlines()]",
        "question": "How do I get the top-5 predictions class names from the model?\nAnswer:\n\nTo get the top-5 predictions class names, you can use the following code:\n\n```py\n>>> # Get top-5 predictions\n>>> top5_prob, top5_idx = torch.topk(probabilities, 5)\n>>> # Get imagenet class mappings\n>>> url, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\n>>> urllib.request.urlretrieve(url, filename) \n>>> with open(\"imagenet_classes.txt\", \"r\") as f:\n...     categories = [s.strip() for s in f.readlines()]\n>>> # Print top-5 predictions\n>>> print(\"Predicted class names:\", [categories[i] for i in top5_idx.tolist()])\n```",
        "answer": "\n\nTo get the top-5 predictions class names, you can use the following code:\n\n```py\n>>> # Get top-5 predictions\n>>> top5_prob, top5_idx = torch.topk(probabilities, 5)\n>>> # Get imagenet class mappings\n>>> url, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\n>>> urllib.request.urlretrieve(url, filename) \n>>> with open(\"imagenet_classes.txt\", \"r\") as f:\n...     categories = [s.strip() for s in f.readlines()]\n>>> # Print top-5 predictions\n>>> print(\"Predicted class names:\", [categories[i] for i in top5_idx.tolist()])\n```",
        "source_doc": "huggingface/pytorch-image-models/blob/main/hfdocs/source/models/res2next.mdx"
    },
    {
        "context": "|      |                                                                            |[ruibin-wang/llama-13b-hf](https://huggingface.co/ruibin-wang/llama-13b-hf)|0           |0                        |llama-license                                                                                 |https://huggingface.co/ruibin-wang/llama-13b-hf/blob/main/LICENSE                             |[LICENSE](https://huggingface.co/ruibin-wang/llama-13b-hf/blob/main/LICENSE)                       |                                                                                                                     |                                                                                   |\n|      |                                                                            |[ruibin-wang/llama-7b-hf](https://huggingface.co/ruibin-wang/llama-7b-hf)|0           |0                        |llama-license                                                                                 |https://huggingface.co/ruibin-wang/llama-7b-hf/blob/main/LICENSE                              |[LICENSE](https://huggingface.co/ruibin-wang/llama-7b-hf/blob/main/LICENSE)                        |                                                                                                                     |                                                                                   |",
        "question": "What is the license for the 'ruibin-wang/llama-13b-hf' model?\n",
        "answer": " The license for the 'ruibin-wang/llama-13b-hf' model is 'llama-license'.",
        "source_doc": "huggingface/hub-docs/blob/main/hacktoberfest_challenges/model_license_other.md"
    },
    {
        "context": "| Notebook     |      Description      |   |   |\n|:----------|:-------------|:-------------|------:|\n| [How to quantize a model with ONNX Runtime for text classification](https://github.com/huggingface/notebooks/blob/main/examples/text_classification_quantization_ort.ipynb)| Show how to apply static and dynamic quantization on a model using [ONNX Runtime](https://github.com/microsoft/onnxruntime) for any GLUE task. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_ort.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_ort.ipynb)|\n| [How to quantize a model with Intel Neural Compressor for text classification](https://github.com/huggingface/notebooks/blob/main/examples/text_classification_quantization_inc.ipynb)| Show how to apply static, dynamic and aware training quantization on a model using [Intel Neural Compressor (INC)](https://github.com/intel/neural-compressor) for any GLUE task. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_inc.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_inc.ipynb)|",
        "question": "What tool is used for quantization in the text classification notebook using ONNX Runtime?\n",
        "answer": " ONNX Runtime",
        "source_doc": "huggingface/transformers/blob/main/notebooks/README.md"
    },
    {
        "context": "## 0.1.3\n\n### Patch Changes\n\n- Updated dependencies []:\n  - @gradio/atoms@0.1.3\n  - @gradio/statustracker@0.2.1\n\n## 0.1.2\n\n### Patch Changes\n\n- Updated dependencies [[`afac0006`](https://github.com/gradio-app/gradio/commit/afac0006337ce2840cf497cd65691f2f60ee5912)]:\n  - @gradio/statustracker@0.2.0\n  - @gradio/utils@0.1.1\n  - @gradio/atoms@0.1.2\n\n## 0.1.1\n\n### Patch Changes\n\n- Updated dependencies [[`abf1c57d`](https://github.com/gradio-app/gradio/commit/abf1c57d7d85de0df233ee3b38aeb38b638477db)]:\n  - @gradio/utils@0.1.0\n  - @gradio/atoms@0.1.1\n  - @gradio/statustracker@0.1.1\n\n## 0.1.0\n\n### Highlights\n\n#### Improve startup performance and markdown support ([#5279](https://github.com/gradio-app/gradio/pull/5279) [`fe057300`](https://github.com/gradio-app/gradio/commit/fe057300f0672c62dab9d9b4501054ac5d45a4ec))\n\n##### Improved markdown support\n\nWe now have better support for markdown in `gr.Markdown` and `gr.Dataframe`. Including syntax highlighting and Github Flavoured Markdown. We also have more consistent markdown behaviour and styling.\n\n##### Various performance improvements\n\nThese improvements will be particularly beneficial to large applications.\n\n- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.\n- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).\n- Corrected an issue that was causing markdown to re-render infinitely.\n- Ensured that the `gr.3DModel` does re-render prematurely.\n\nThanks [@pngwn](https://github.com/pngwn)!\n\n### Features",
        "question": "What is the version of @gradio/statustracker in 0.1.3?\n",
        "answer": " The version of @gradio/statustracker in 0.1.3 is 0.2.1.",
        "source_doc": "gradio-app/gradio/blob/main/js/colorpicker/CHANGELOG.md"
    },
    {
        "context": "### Conclusion\n\nWant to play the final farming game? Check it out [here](https://huggingface.co/spaces/dylanebert/FarmingGame) or on [itch.io](https://individualkex.itch.io/farming-game).\n\n<div align=\"center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/game.png\" alt=\"Final Farming Game\">\n</div>\n\nThank you for reading the AI for Game Development series! This series is only the beginning of AI for Game Development at Hugging Face, with more to come. Have questions? Want to get more involved? Join the [Hugging Face Discord](https://hf.co/join/discord)!",
        "question": "Where can I play the final farming game?\n",
        "answer": " You can play the final farming game [here](https://huggingface.co/checkspaces/dylanebert/FarmingGame) or on [itch.io](https://individualkex.itch.io/farming-game).",
        "source_doc": "huggingface/blog/blob/main/ml-for-games-5.md"
    },
    {
        "context": "inputs = tokenizer(\"Hello, my dog is cute and \", return_tensors=\"pt\")\ngeneration_output = model.generate(**inputs, return_dict_in_generate=True, output_scores=True)\n```\n\nThe `generation_output` object is a [`~generation.GreedySearchDecoderOnlyOutput`], as we can\nsee in the documentation of that class below, it means it has the following attributes:\n\n- `sequences`: the generated sequences of tokens\n- `scores` (optional): the prediction scores of the language modelling head, for each generation step\n- `hidden_states` (optional): the hidden states of the model, for each generation step\n- `attentions` (optional): the attention weights of the model, for each generation step\n\nHere we have the `scores` since we passed along `output_scores=True`, but we don't have `hidden_states` and\n`attentions` because we didn't pass `output_hidden_states=True` or `output_attentions=True`.\n\nYou can access each attribute as you would usually do, and if that attribute has not been returned by the model, you\nwill get `None`. Here for instance `generation_output.scores` are all the generated prediction scores of the\nlanguage modeling head, and `generation_output.attentions` is `None`.\n\nWhen using our `generation_output` object as a tuple, it only keeps the attributes that don't have `None` values.\nHere, for instance, it has two elements, `loss` then `logits`, so\n\n```python\ngeneration_output[:2]\n```\n\nwill return the tuple `(generation_output.sequences, generation_output.scores)` for instance.\n\nWhen using our `generation_output` object as a dictionary, it only keeps the attributes that don't have `None`\nvalues. Here, for instance, it has two keys that are `sequences` and `scores`.\n\nWe document here all output types.\n\n\n### PyTorch\n\n[[autodoc]] generation.GreedySearchEncoderDecoderOutput\n\n[[autodoc]] generation.GreedySearchDecoderOnlyOutput\n\n[[autodoc]] generation.SampleEncoderDecoderOutput\n\n[[autodoc]] generation.SampleDecoderOnlyOutput\n\n[[autodoc]] generation.BeamSearchEncoderDecoderOutput",
        "question": "What is the attribute of the `generation_output` object that contains the prediction scores of the language modeling head, for each generation step?\n",
        "answer": " scores",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/internal/generation_utils.md"
    },
    {
        "context": "# optimize with IPEX\npipe.unet = ipex.optimize(pipe.unet.eval(), dtype=torch.bfloat16, inplace=True, sample_input=input_example)\npipe.vae = ipex.optimize(pipe.vae.eval(), dtype=torch.bfloat16, inplace=True)\npipe.text_encoder = ipex.optimize(pipe.text_encoder.eval(), dtype=torch.bfloat16, inplace=True)\npipe.safety_checker = ipex.optimize(pipe.safety_checker.eval(), dtype=torch.bfloat16, inplace=True)\n```\n\nWe also enable the `bloat16` data format to leverage the AMX tile matrix multiply unit (TMMU) accelerator present on Sapphire Rapids CPUs.\n\n```python\nwith torch.cpu.amp.autocast(enabled=True, dtype=torch.bfloat16):\n    latency = elapsed_time(pipe, prompt)\n    print(latency)\n```\n\nWith this updated version, inference latency is further reduced from 11.9 seconds to **5.4 seconds**. That's more than 2x acceleration thanks to IPEX and AMX.\n\nCan we extract a bit more performance? Yes, with schedulers!\n\n## Schedulers \n\nThe Diffusers library lets us attach a [scheduler](https://huggingface.co/docs/diffusers/using-diffusers/schedulers) to a Stable Diffusion pipeline. Schedulers try to find the best trade-off between denoising speed and denoising quality.\n\nAccording to the documentation: \"*At the time of writing this doc DPMSolverMultistepScheduler gives arguably the best speed/quality trade-off and can be run with as little as 20 steps.*\"\n\nLet's try it.\n\n```python\nfrom diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n...\ndpm = DPMSolverMultistepScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=dpm)\n```\n\nWith this final version, inference latency is now down to **5.05 seconds**. Compared to our initial Sapphire Rapids baseline (32.3 seconds), this is almost 6.5x faster!",
        "question": "What is the inference latency of the final version of the Stable Diffusion pipeline on a Sapphire Rapids CPU?\n",
        "answer": " The inference latency of the final version of the Stable Diffusion pipeline on a Sapphire Rapids CPU is 5.05 seconds.",
        "source_doc": "huggingface/blog/blob/main/stable-diffusion-inference-intel.md"
    },
    {
        "context": "- Pipelines should be very readable, self-explanatory, and easy to tweak.\n- Pipelines should be designed to build on top of each other and be easy to integrate into higher-level APIs.\n- Pipelines are **not** intended to be feature-complete user interfaces. For future complete user interfaces one should rather have a look at [InvokeAI](https://github.com/invoke-ai/InvokeAI), [Diffuzers](https://github.com/abhishekkrthakur/diffuzers), and [lama-cleaner](https://github.com/Sanster/lama-cleaner).\n- Every pipeline should have one and only one way to run it via a `__call__` method. The naming of the `__call__` arguments should be shared across all pipelines.\n- Pipelines should be named after the task they are intended to solve.\n- In almost all cases, novel diffusion pipelines shall be implemented in a new pipeline folder/file.",
        "question": "What are some user interfaces that are not pipelines?\n",
        "answer": " InvokeAI, Diffuzers, and lama-cleaner are some user interfaces that are not pipelines.",
        "source_doc": "huggingface/diffusers/blob/main/PHILOSOPHY.md"
    },
    {
        "context": "- model instantiation with the `torchscript` flag\n- a forward pass with dummy inputs\n\nThese necessities imply several things developers should be careful about as detailed\nbelow.\n\n## TorchScript flag and tied weights\n\nThe `torchscript` flag is necessary because most of the 🤗 Transformers language models\nhave tied weights between their `Embedding` layer and their `Decoding` layer.\nTorchScript does not allow you to export models that have tied weights, so it is\nnecessary to untie and clone the weights beforehand.\n\nModels instantiated with the `torchscript` flag have their `Embedding` layer and\n`Decoding` layer separated, which means that they should not be trained down the line.\nTraining would desynchronize the two layers, leading to unexpected results.\n\nThis is not the case for models that do not have a language model head, as those do not\nhave tied weights. These models can be safely exported without the `torchscript` flag.\n\n## Dummy inputs and standard lengths\n\nThe dummy inputs are used for a models forward pass. While the inputs' values are\npropagated through the layers, PyTorch keeps track of the different operations executed\non each tensor. These recorded operations are then used to create the *trace* of the\nmodel.\n\nThe trace is created relative to the inputs' dimensions. It is therefore constrained by\nthe dimensions of the dummy input, and will not work for any other sequence length or\nbatch size. When trying with a different size, the following error is raised:\n\n```\n`The expanded size of the tensor (3) must match the existing size (7) at non-singleton dimension 2`\n```\n\nWe recommended you trace the model with a dummy input size at least as large as the\nlargest input that will be fed to the model during inference. Padding can help fill the\nmissing values. However, since the model is traced with a larger input size, the\ndimensions of the matrix will also be large, resulting in more calculations.",
        "question": "Why is it necessary to use the torchscript flag when instantiating a 🤗 Transformers language model with tied weights?\n",
        "answer": " The torchscript flag is necessary because TorchScript does not allow you to export models that have tied weights. Models instantiated with the torchscript flag have their Embedding layer and Decoding layer separated, which means that they should not be trained down the line. Training would desynchronize the two layers, leading to unexpected results.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/torchscript.md"
    },
    {
        "context": "#### Common Voice CTC\n\n- [Common Voice](https://huggingface.co/datasets/common_voice)",
        "question": "What is the name of the Common Voice dataset provided by Hugging Face?\n\n",
        "answer": " The name of the Common Voice dataset provided by Hugging Face is 'common\\_voice'.",
        "source_doc": "huggingface/transformers/blob/main/examples/pytorch/speech-recognition/README.md"
    },
    {
        "context": "## Models\n\nModels are the core algorithms used to actually tokenize, and therefore,\nthey are the only mandatory component of a Tokenizer.\n\n| Name | Description |\n| :--- | :--- |\n| WordLevel | This is the “classic” tokenization algorithm. It let’s you simply map words to IDs without anything fancy. This has the advantage of being really simple to use and understand, but it requires extremely large vocabularies for a good coverage. Using this `Model` requires the use of a `PreTokenizer`. No choice will be made by this model directly, it simply maps input tokens to IDs.  |\n| BPE | One of the most popular subword tokenization algorithm. The Byte-Pair-Encoding works by starting with characters, while merging those that are the most frequently seen together, thus creating new tokens. It then works iteratively to build new tokens out of the most frequent pairs it sees in a corpus. BPE is able to build words it has never seen by using multiple subword tokens, and thus requires smaller vocabularies, with less chances of having “unk” (unknown) tokens.  |\n| WordPiece | This is a subword tokenization algorithm quite similar to BPE, used mainly by Google in models like BERT. It uses a greedy algorithm, that tries to build long words first, splitting in multiple tokens when entire words don’t exist in the vocabulary. This is different from BPE that starts from characters, building bigger tokens as possible. It uses the famous `##` prefix to identify tokens that are part of a word (ie not starting a word).  |\n| Unigram | Unigram is also a subword tokenization algorithm, and works by trying to identify the best set of subword tokens to maximize the probability for a given sentence. This is different from BPE in the way that this is not deterministic based on a set of rules applied sequentially. Instead Unigram will be able to compute multiple ways of tokenizing, while choosing the most probable one. |\n\n## Post-Processors",
        "question": "What is the name of the post-processor that removes all the punctuation from the output tokens?\n",
        "answer": " Punctuation",
        "source_doc": "huggingface/tokenizers/blob/main/docs/source-doc-builder/components.mdx"
    },
    {
        "context": "## Quickstart flax and jax in transformers\n\nCurrently, we support the following models in Flax. \nNote that some models are about to be merged to `main` and will \nbe available in a couple of days.\n\n- [BART](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bart/modeling_flax_bart.py)\n- [BERT](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/modeling_flax_bert.py)\n- [BigBird](https://github.com/huggingface/transformers/blob/main/src/transformers/models/big_bird/modeling_flax_big_bird.py)\n- [CLIP](https://github.com/huggingface/transformers/blob/main/src/transformers/models/clip/modeling_flax_clip.py)\n- [ELECTRA](https://github.com/huggingface/transformers/blob/main/src/transformers/models/electra/modeling_flax_electra.py)\n- [GPT2](https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_flax_gpt2.py)\n- [(TODO) MBART](https://github.com/huggingface/transformers/blob/main/src/transformers/models/mbart/modeling_flax_mbart.py)\n- [RoBERTa](https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_flax_roberta.py)\n- [T5](https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/modeling_flax_t5.py)\n- [ViT](https://github.com/huggingface/transformers/blob/main/src/transformers/models/vit/modeling_flax_vit.py)\n- [Wav2Vec2](https://github.com/huggingface/transformers/blob/main/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py)\n\nYou can find all available training scripts for JAX/Flax under the \nofficial [flax example folder](https://github.com/huggingface/transformers/tree/main/examples/flax). Note that a couple of training scripts will be released in the following week.",
        "question": "Which models are available in Flax?\n",
        "answer": " The available models in Flax are BART, BERT, BigBird, CLIP, ELECTRA, GPT2, MBART, RoBERTa, T5, ViT, and Wav2Vec2.",
        "source_doc": "huggingface/transformers/blob/main/examples/research_projects/jax-projects/README.md"
    },
    {
        "context": "Metric Card for COMET\n\n## Metric description\n\nCrosslingual Optimized Metric for Evaluation of Translation (COMET) is an open-source framework used to train Machine Translation metrics that achieve high levels of correlation with different types of human judgments.\n\n## How to use\n\nCOMET takes 3 lists of strings as input: `sources` (a list of source sentences), `predictions` (a list of candidate translations) and `references` (a list of reference translations).\n\n```python\nfrom datasets import load_metric\ncomet_metric = load_metric('comet')\nsource = [\"Dem Feuer konnte Einhalt geboten werden\", \"Schulen und Kindergärten wurden eröffnet.\"]\nhypothesis = [\"The fire could be stopped\", \"Schools and kindergartens were open\"]\nreference = [\"They were able to control the fire.\", \"Schools and kindergartens opened\"]\ncomet_score = comet_metric.compute(predictions=hypothesis, references=reference, sources=source)\n```\n\nIt has several configurations, named after the COMET model to be used. It will default to `wmt20-comet-da` (previously known as `wmt-large-da-estimator-1719`). Alternate models that can be chosen include `wmt20-comet-qe-da`, `wmt21-comet-mqm`, `wmt21-cometinho-da`, `wmt21-comet-qe-mqm` and `emnlp20-comet-rank`.\n\nIt also has several optional arguments:\n\n`gpus`: optional, an integer (number of GPUs to train on) or a list of integers (which GPUs to train on). Set to 0 to use CPU. The default value is `None` (uses one GPU if possible, else use CPU).\n\n`progress_bar`a boolean -- if set to `True`, progress updates will be printed out. The default value is `False`.\n\nMore information about model characteristics can be found on the [COMET website](https://unbabel.github.io/COMET/html/models.html).\n\n## Output values\n\nThe COMET metric outputs two lists:\n\n`scores`: a list of COMET scores for each of the input sentences, ranging from 0-1.\n\n`mean_score`: the mean value of COMET scores `scores` over all the input sentences, ranging from 0-1.\n\n### Values from popular papers",
        "question": "(your factoid question)\n",
        "answer": "\n\nCOMET is a framework used to train Machine Translation metrics that achieve high levels of correlation with different types of human judgments. It takes 3 lists of strings as input: `sources`, `predictions`, and `references`. It has several configurations, named after the COMET model to be used, and outputs two lists: `scores` and `mean_score`. The `scores` list contains COMET scores for each input sentence, ranging from 0-1, and `mean_score` is the mean value of COMET scores over all input sentences, also ranging from 0-1.",
        "source_doc": "huggingface/datasets/blob/main/metrics/comet/README.md"
    },
    {
        "context": "###### 3. Component Fixes 🧱\n\n- Specify the width and height of an image in its style tag (thanks to PR #2133)\n\n```python\ncomponents.Image().style(height=260, width=300)\n```\n\n- Automatic conversion of videos so they are playable in the browser (thanks to PR #2003). Gradio will check if a video's format is playable in the browser and, if it isn't, will automatically convert it to a format that is (mp4).\n- Pass in a json filepath to the Label component (thanks to PR #2083)\n- Randomize the default value of a Slider (thanks to PR #1935)\n\n![slider-random](https://user-images.githubusercontent.com/9021060/187596230-3db9697f-9f4d-42f5-9387-d77573513448.gif)\n\n- Improvements to State in PR #2100\n\n###### 4. Ability to Randomize Input Sliders and Reload Data whenever the Page Loads\n\n- In some cases, you want to be able to show a different set of input data to every user as they load the page app. For example, you might want to randomize the value of a \"seed\" `Slider` input. Or you might want to show a `Textbox` with the current date. We now supporting passing _functions_ as the default value in input components. When you pass in a function, it gets **re-evaluated** every time someone loads the demo, allowing you to reload / change data for different users.\n\nHere's an example loading the current date time into an input Textbox:\n\n```python\nimport gradio as gr\nimport datetime\n\nwith gr.Blocks() as demo:\n    gr.Textbox(datetime.datetime.now)\n\ndemo.launch()\n```\n\nNote that we don't evaluate the function -- `datetime.datetime.now()` -- we pass in the function itself to get this behavior -- `datetime.datetime.now`\n\nBecause randomizing the initial value of `Slider` is a common use case, we've added a `randomize` keyword argument you can use to randomize its initial value:\n\n```python\nimport gradio as gr\ndemo = gr.Interface(lambda x:x, gr.Slider(0, 10, randomize=True), \"number\")\ndemo.launch()\n```\n\n###### 5. New Guide 🖊️",
        "question": "What is the function that is passed to the Textbox component in the example?\n",
        "answer": " datetime.datetime.now",
        "source_doc": "gradio-app/gradio/blob/main/gradio/CHANGELOG.md"
    },
    {
        "context": "#pt_to_pil(stage_1_output)[0].save(\"./if_stage_I.png\")\n\n# Remove the pipeline so we can load the super-resolution pipeline\ndel pipe\ngc.collect()\ntorch.cuda.empty_cache()\n\n# First super resolution\n\npipe = IFSuperResolutionPipeline.from_pretrained(\n    \"DeepFloyd/IF-II-L-v1.0\", text_encoder=None, variant=\"fp16\", torch_dtype=torch.float16, device_map=\"auto\"\n)\n\ngenerator = torch.Generator().manual_seed(0)\nstage_2_output = pipe(\n    image=stage_1_output,\n    prompt_embeds=prompt_embeds,\n    negative_prompt_embeds=negative_embeds,\n    output_type=\"pt\",\n    generator=generator,\n).images\n\n#pt_to_pil(stage_2_output)[0].save(\"./if_stage_II.png\")\nmake_image_grid([pt_to_pil(stage_1_output)[0], pt_to_pil(stage_2_output)[0]], rows=1, rows=2)\n```\n\n## Available Pipelines:\n\n| Pipeline | Tasks | Colab\n|---|---|:---:|\n| [pipeline_if.py](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/deepfloyd_if/pipeline_if.py) | *Text-to-Image Generation* | - |\n| [pipeline_if_superresolution.py](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/deepfloyd_if/pipeline_if_superresolution.py) | *Text-to-Image Generation* | - |\n| [pipeline_if_img2img.py](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/deepfloyd_if/pipeline_if_img2img.py) | *Image-to-Image Generation* | - |\n| [pipeline_if_img2img_superresolution.py](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/deepfloyd_if/pipeline_if_img2img_superresolution.py) | *Image-to-Image Generation* | - |\n| [pipeline_if_inpainting.py](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/deepfloyd_if/pipeline_if_inpainting.py) | *Image-to-Image Generation* | - |\n| [pipeline_if_inpainting_superresolution.py](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/deepfloyd_if/pipeline_if_inpainting_superresolution.py) | *Image-to-Image Generation* | - |\n\n## IFPipeline\n[[autodoc]] IFPipeline\n\t- all\n\t- __call__",
        "question": "(your factoid question)\n",
        "answer": " (your answer to the factoid question)\n\nNow here is the context.\n\nContext: #pt_to_pil(stage_1_output)[0].save(\"./if_stage_I.png\")\n\n# Remove the pipeline so we can load the super-resolution pipeline\ndel pipe\ngc.collect()\ntorch.cuda.empty_cache()\n\n# First super resolution\n\npipe = IFSuperResolutionPipeline.from_pretrained(\n    \"DeepFloyd/IF-II-L-v1.0\", text_encoder=None, variant=\"fp16\", torch_dtype=torch.float16, device_map=\"auto\"\n)\n\ngenerator = torch.Generator().manual_seed(0)\nstage_2_output = pipe(\n    image=stage_1_output,\n    prompt_embeds=prompt_embeds,\n    negative_prompt_embeds=negative_embeds,\n    output_type=\"pt\",\n    generator=generator,\n).images\n\n#pt_to_pil(stage_2_output)[0].save(\"./if_stage_II.png\")\nmake_image_grid([pt_to_pil(stage_1_output)[0], pt_to_pil(stage_2_output)[0]], rows=1, rows=2)\n```\n\n## Available Pipelines:\n\n| Pipeline | Tasks | Colab\n|---|---|:---:|\n| [pipeline_if.py](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/deepfloyd_if/pipeline_if.py) | *Text-to-Image Generation* | - |\n| [pipeline_if_superresolution.py](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/deepfloyd_if/pipeline_if_superresolution.py) | *Text-to-Image Generation* | - |\n| [pipeline_if_img2img.py](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/deepfloyd_if/pipeline_if_img2img.py) | *Image-to-Image Generation* | - |\n| [pipeline_if_img2img_superresolution.py](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/deepfloyd_if/pipeline_if_img2img_superresolution.py) | *Image-to-Image Generation* | - |\n| [pipeline_if_inpainting.py](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/deepfloyd_if/pipeline_if_inpainting.py) | *Image-to-Image Generation* | - |\n| [pipeline_if_inpainting_superresolution.py](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/deepfloyd_if/pipeline_if_inpainting_superresolution.py) | *Image-to-Image Generation* | - |\n\n## IFPipeline\n[[autodoc]] IFPipeline\n\t- all\n\t- __call__\n\nOutput:::\n\n\t- images\n\t- latents\n\n## IFSuperResolutionPipeline\n[[autodoc]] IFSuperResolutionPipeline\n\t- all\n\t- __call__\n\nOutput:::\n\n\t- images\n\t- latents\n\n## IFText2ImagePipeline\n[[autodoc]] IFText2ImagePipeline\n\t- all\n\t- __call__\n\nOutput:::\n\n\t- images\n\t- latents\n\n## IFImg2ImgPipeline\n[[autodoc]] IFImg2ImgPipeline\n\t- all\n\t- __call__\n\nOutput:::\n\n\t- images\n\t- latents\n\n## IFInpaintingPipeline\n[[autodoc]] IFInpaintingPipeline\n\t- all\n\t- __call__\n\nOutput:::\n\n\t- images\n\t- latents\n\n## IFInpaintingSuperResolutionPipeline\n[[autodoc]] IFInpaintingSuperResolutionPipeline\n\t- all\n\t- __call__\n\nOutput:::\n\n\t- images\n\t- latents\n\n## IFText2ImagePipelineOutput\n[[autodoc]] IFText2ImagePipelineOutput\n\t- all\n\t- __call__\n\nOutput:::\n\n\t- images\n\t- latents\n\n## IFImg2ImgPipelineOutput\n[[autodoc]] IFImg2ImgPipelineOutput\n\t- all\n\t- __call__\n\nOutput:::\n\n\t- images\n\t- latents\n\n## IFInpaintingPipelineOutput\n[[autodoc]] IFInpaintingPipelineOutput\n\t- all\n\t- __call__\n\nOutput:::\n\n\t- images\n\t- latents\n\n## IFInpaintingSuperResolutionPipelineOutput\n[[autodoc]] IF",
        "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/deepfloyd_if.md"
    },
    {
        "context": "If you use a very large value the images might look good, but will be less diverse. \nYou can learn about the technical details of this parameter in [this section](#writing-your-own-inference-pipeline) of the post.\n\nNext, let's see how you can generate several images of the same prompt at once. \nFirst, we'll create an `image_grid` function to help us visualize them nicely in a grid.\n\n```python\nfrom PIL import Image\n\ndef image_grid(imgs, rows, cols):\n    assert len(imgs) == rows*cols\n\n    w, h = imgs[0].size\n    grid = Image.new('RGB', size=(cols*w, rows*h))\n    grid_w, grid_h = grid.size\n    \n    for i, img in enumerate(imgs):\n        grid.paste(img, box=(i%cols*w, i//cols*h))\n    return grid\n```\n\nWe can generate multiple images for the same prompt by simply using a list with the same prompt repeated several times. We'll send the list to the pipeline instead of the string we used before.\n\n```python\nnum_images = 3\nprompt = [\"a photograph of an astronaut riding a horse\"] * num_images\n\nimages = pipe(prompt).images\n\ngrid = image_grid(images, rows=1, cols=3)\n\n# you can save the grid with\n# grid.save(f\"astronaut_rides_horse.png\")\n```\n\n![png](assets/98_stable_diffusion/stable_diffusion_22_1.png)\n    \n\nBy default, stable diffusion produces images of `512 × 512` pixels. It's very easy to override the default using the `height` and `width` arguments to create rectangular images in portrait or landscape ratios.\n\nWhen choosing image sizes, we advise the following:\n- Make sure `height` and `width` are both multiples of `8`.\n- Going below 512 might result in lower quality images.\n- Going over 512 in both directions will repeat image areas (global coherence is lost).\n- The best way to create non-square images is to use `512` in one dimension, and a value larger than that in the other one.\n\nLet's run an example:\n\n```python\nprompt = \"a photograph of an astronaut riding a horse\"\nimage = pipe(prompt, height=512, width=768).images[0]",
        "question": "(your factoid question)\n",
        "answer": " The default image size produced by stable diffusion is 512 × 512 pixels.",
        "source_doc": "huggingface/blog/blob/main/stable_diffusion.md"
    },
    {
        "context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Stable unCLIP\n\nStable unCLIP checkpoints are finetuned from [Stable Diffusion 2.1](./stable_diffusion/stable_diffusion_2) checkpoints to condition on CLIP image embeddings.\nStable unCLIP still conditions on text embeddings. Given the two separate conditionings, stable unCLIP can be used\nfor text guided image variation. When combined with an unCLIP prior, it can also be used for full text to image generation.\n\nThe abstract from the paper is:",
        "question": "What is Stable unCLIP used for?\n",
        "answer": " Stable unCLIP is used for text guided image variation and full text to image generation.",
        "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_unclip.md"
    },
    {
        "context": "y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n        var_y = np.var(y_true)\n        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n\n        # TRY NOT TO MODIFY: record rewards for plotting purposes\n        writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n        writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n        writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n        writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n        writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n        writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n        writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n        writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n        print(\"SPS:\", int(global_step / (time.time() - start_time)))\n        writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n\n    envs.close()\n    writer.close()\n\n    # Create the evaluation environment\n    eval_env = gym.make(args.env_id)\n\n    package_to_hub(\n        repo_id=args.repo_id,\n        model=agent,  # The model we want to save\n        hyperparameters=args,\n        eval_env=gym.make(args.env_id),\n        logs=f\"runs/{run_name}\",\n    )\n```\n\nTo be able to share your model with the community there are three more steps to follow:\n\n1️⃣ (If it's not already done) create an account to HF ➡ https://huggingface.co/join\n\n2️⃣ Sign in and get your authentication token from the Hugging Face website.\n- Create a new token (https://huggingface.co/settings/tokens) **with write role**\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"Create HF Token\">\n\n- Copy the token\n- Run the cell below and paste the token",
        "question": "(your factoid question)\n",
        "answer": " (your answer to the factoid question)\n\nNow here is the context.\n\nContext: y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n        var_y = np.var(y_true)\n        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n\n        # TRY NOT TO MODIFY: record rewards for plotting purposes\n        writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n        writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n        writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n        writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n        writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n        writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n        writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n        writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n        print(\"SPS:\", int(global_step / (time.time() - start_time)))\n        writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n\n    envs.close()\n    writer.close()\n\n    # Create the evaluation environment\n    eval_env = gym.make(args.env_id)\n\n    package_to_hub(\n        repo_id=args.repo_id,\n        model=agent,  # The model we want to save\n        hyperparameters=args,\n        eval_env=gym.make(args.env_id),\n        logs=f\"runs/{run_name}\",\n    )\n```\n\nTo be able to share your model with the community there are three more steps to follow:\n\n1️⃣ (If it's not already done) create an account to HF ➡ https://huggingface.co/join\n\n2️⃣ Sign in and get your authentication token from the Hugging Face website.\n- Create a new token (https://huggingface.co/settings/tokens) **with write role**\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"Create HF Token\">\n\n- Copy the token\n- Run the cell below and paste the token\n\nOutput:::\nHugging Face Token: <input type=\"text\" id=\"hf_token\" name=\"hf_token\" placeholder=\"Paste your token here\">\n<button onclick=\"save_token()\">Save token</button>\n<script>\nfunction save_token() {\n  const token = document.getElementById('hf_token').value\n  localStorage.setItem('hf_token', token)\n}\n</script>\n\n3️⃣ Run the cell below to upload your model to the Hugging Face Model Hub\n\nOutput:::\n!pip install huggingface-hub\n\n!pip install einops\n\nimport torch\nimport einops\nimport numpy as np\nimport time\nimport gym\nimport wandb\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nfrom stable_baselines3.common.evaluation import evaluate_policy\nfrom stable_baselines3.common.logger import configure\nfrom stable_baselines3.common.vec_env import VecNormalize\nfrom stable_baselines3.common.preprocessing import get_flattened_obs_dim\nfrom stable_baselines3.common.type_aliases import Schedule\nfrom stable_baselines3.ppo import MlpPolicy\n\n# Load the model\nmodel = PPO.load(\"ppo_cartpole\")\n\n# Create a vectorized environment\nenv = DummyVecEnv([lambda: gym.make(\"CartPole-v1\")])\nenv = VecNormalize(env, norm_obs=True, norm_reward=True)\n\n# Evaluate the model\nmean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\nprint(f",
        "source_doc": "huggingface/deep-rl-class/blob/main/units/en/unit8/hands-on-cleanrl.mdx"
    },
    {
        "context": "# Recommendations\n\nHere are some early recommendations that can be made from the previous observations:\n\n## Using video-specific prompt keywords\n\nYou may already know that if you don’t prompt a specific aspect of the image with Stable Diffusion, things like the color of clothes or the time of the day might become random, or be assigned a generic value such as a neutral mid-day light.\n\nThe same is true for video models: you will want to be specific about things. Examples include camera and character movement, their orientation, speed and direction. You can leave it unspecified for creative purposes (idea generation), but this might not always give you the results you want (e.g., entities animated in reverse).\n\n## Maintaining consistency between scenes\n\nIf you plan to create sequences of multiple videos, you will want to make sure you add as many details as possible in each prompt, otherwise you may lose important details from one sequence to another, such as the color.\n\n💡 This will also improve the quality of the image since the prompt is used for the upscaling part with Zeroscope XL.\n\n## Leverage frame interpolation\n\nFrame interpolation is a powerful tool which can repair small rendering errors and turn many defects into features, especially in scenes with a lot of animation, or where a cartoon effect is acceptable. The [FILM algorithm](https://film-net.github.io/) will smoothen out elements of a frame with previous and following events in the video clip.\n\nThis works great to displace the background when the camera is panning or rotating, and will also give you creative freedom, such as control over the number of frames after the generation, to make slow-motion effects.\n\n# Future work\n\nWe hope you enjoyed watching the AI WebTV stream and that it will inspire you to build more in this space.",
        "question": "What is frame interpolation used for in video generation?\n",
        "answer": " Frame interpolation is used to repair small rendering errors and turn defects into features, especially in scenes with a lot of animation or where a cartoon effect is acceptable. It can also displace the background when the camera is panning or rotating, and provide creative freedom, such as control over the number of frames after the generation, for slow-motion effects.",
        "source_doc": "huggingface/blog/blob/main/ai-webtv.md"
    },
    {
        "context": "--\ntitle: \"Faster Training and Inference: Habana Gaudi®2 vs Nvidia A100 80GB\"\nthumbnail: /blog/assets/habana-gaudi-2-benchmark/thumbnail.png\nauthors:\n- user: regisss\n---\n\n# Faster Training and Inference: Habana Gaudi®-2 vs Nvidia A100 80GB\n\n\nIn this article, you will learn how to use [Habana® Gaudi®2](https://habana.ai/training/gaudi2/) to accelerate model training and inference, and train bigger models with 🤗 [Optimum Habana](https://huggingface.co/docs/optimum/habana/index). Then, we present several benchmarks including BERT pre-training, Stable Diffusion inference and T5-3B fine-tuning, to assess the performance differences between first generation Gaudi, Gaudi2 and Nvidia A100 80GB. Spoiler alert - Gaudi2 is about twice faster than Nvidia A100 80GB for both training and inference!\n\n[Gaudi2](https://habana.ai/training/gaudi2/) is the second generation AI hardware accelerator designed by Habana Labs. A single server contains 8 accelerator devices with 96GB of memory each (versus 32GB on first generation Gaudi and 80GB on A100 80GB). The Habana SDK, [SynapseAI](https://developer.habana.ai/), is common to both first-gen Gaudi and Gaudi2.\nThat means that 🤗 Optimum Habana, which offers a very user-friendly interface between the 🤗 Transformers and 🤗 Diffusers libraries and SynapseAI, **works the exact same way on Gaudi2 as on first-gen Gaudi!**\nSo if you already have ready-to-use training or inference workflows for first-gen Gaudi, we encourage you to try them on Gaudi2, as they will work without any single change.\n\n\n## How to Get Access to Gaudi2?\n\nOne of the easy, cost-efficient ways that Intel and Habana have made Gaudi2 available is on the Intel Developer Cloud. To start using Gaudi2 there, you should follow the following steps:\n\n1. Go to the [Intel Developer Cloud landing page](https://www.intel.com/content/www/us/en/developer/tools/devcloud/services.html) and sign in to your account or register if you do not have one.",
        "question": "How much memory does a single Gaudi2 accelerator device have?\n",
        "answer": " A single Gaudi2 accelerator device has 96GB of memory.",
        "source_doc": "huggingface/blog/blob/main/habana-gaudi-2-benchmark.md"
    },
    {
        "context": "!---\nCopyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n# Text classification examples\n\nThis folder contains some scripts showing examples of *text classification* with the 🤗 Transformers library.\nFor straightforward use-cases you may be able to use these scripts without modification, although we have also\nincluded comments in the code to indicate areas that you may need to adapt to your own projects.\n\n## run_text_classification.py\n\nThis script handles perhaps the single most common use-case for this entire library: Training an NLP classifier\non your own training data. This can be whatever you want - you could classify text as abusive/hateful or \nallowable, or forum posts as spam or not-spam, or classify the genre of a headline as politics, sports or any \nnumber of other categories. Any task that involves classifying natural language into two or more different categories \ncan work with this! You can even do regression, such as predicting the score on a 1-10 scale that a user gave,\ngiven the text of their review.",
        "question": "What is the use-case of the run_text_classification.py script?\n",
        "answer": " The use-case of the run_text_classification.py script is to train an NLP classifier on your own training data. This can be whatever you want, such as classifying text as abusive/hateful or allowable, or forum posts as spam or not-spam. The script can also be used for regression tasks, such as predicting the score on a 1-10 scale that a user gave, given the text of their review.",
        "source_doc": "huggingface/transformers/blob/main/examples/tensorflow/text-classification/README.md"
    },
    {
        "context": "Webhook guide: build a Discussion bot based on BLOOM\n\n<Tip>\n\nWebhooks are now publicly available!\n\n</Tip>\n\nHere's a short guide on how to use Hugging Face Webhooks to build a bot that replies to Discussion comments on the Hub with a response generated by BLOOM, a multilingual language model, using the free Inference API.\n\n## Create your Webhook in your user profile\n\nFirst, let's create a Webhook from your [settings]( https://huggingface.co/settings/webhooks).\n\n- Input a few target repositories that your Webhook will listen to.\n- You can put a dummy Webhook URL for now, but defining your webhook will let you look at the events that will be sent to it (and you can replay them, which will be useful for debugging).\n- Input a secret as it will be more secure.\n- Subscribe to Community (PR & discussions) events, as we are building a Discussion bot.\n\nYour Webhook will look like this:\n\n![webhook-creation](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/webhooks-guides/001-discussion-bot/webhook-creation.png)\n\n## Create a new `Bot` user profile\n\nIn this guide, we create a separate user account to host a Space and to post comments:\n\n![discussion-bot-profile](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/webhooks-guides/001-discussion-bot/discussion-bot-profile.png)\n\n<Tip>\n\tWhen creating a bot that will interact with other users on the Hub, we ask that you clearly label the account as a \"Bot\" (see profile screenshot).\n</Tip>\n\n## Create a Space that will react to your Webhook\n\nThe third step is actually to listen to the Webhook events.\n\nAn easy way is to use a Space for this. We use the user account we created, but you could do it from your main user account if you wanted to.\n\nThe Space's code is [here](https://huggingface.co/spaces/discussion-bot/webhook/tree/main).",
        "question": "What is the Space's code URL?\n",
        "answer": " https://huggingface.co/spaces/discussion-bot/webhook/tree/main",
        "source_doc": "huggingface/hub-docs/blob/main/docs/hub/webhooks-guide-discussion-bot.md"
    },
    {
        "context": "## Loading a `Learner` from the Hugging Face Hub\n\nLoading a model from the Hub is even simpler. We will load our `Learner`, \"espejelomar/identify-my-cat\", and test it with a cat image (🦮?). This code is adapted from\nthe [first chapter of the fastbook](https://github.com/fastai/fastbook/blob/master/01_intro.ipynb).\n\nFirst, upload an image of a cat (or possibly a dog?). The [Colab notebook with this tutorial](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/64_fastai_hub.ipynb) uses `ipywidgets` to interactively upload a cat image (or not?). Here we will use this cute cat 🐅:\n\n![Fastai Model Card](assets/64_fastai/cat.jpeg)\n\nNow let's load the `Learner` we just shared in the Hub and test it.\n\n```py\nfrom huggingface_hub import from_pretrained_fastai\n\n# repo_id = \"YOUR_USERNAME/YOUR_LEARNER_NAME\"\nrepo_id = \"espejelomar/identify-my-cat\"\n\nlearner = from_pretrained_fastai(repo_id)\n```\nIt works 👇!\n\n```py\n_,_,probs = learner.predict(img)\nprint(f\"Probability it's a cat: {100*probs[1].item():.2f}%\")\n\nProbability it's a cat: 100.00%\n```\n\nThe [Hub Client documentation](https://huggingface.co/docs/huggingface_hub/main/en/package_reference/mixins#huggingface_hub.from_pretrained_fastai) includes addtional details on `from_pretrained_fastai`.\n\n\n## `Blurr` to mix fastai and Hugging Face Transformers (and share them)!\n\n> [Blurr is] a library designed for fastai developers who want to train and deploy Hugging Face transformers - [Blurr Docs](https://github.com/ohmeow/blurr).\n\nWe will:\n1. Train a `blurr` Learner with the [high-level Blurr API](https://github.com/ohmeow/blurr#using-the-high-level-blurr-api). It will load the `distilbert-base-uncased` model from the Hugging Face Hub and prepare a sequence classification model.\n2. Share it to the Hub with the namespace `fastai/blurr_IMDB_distilbert_classification` using `push_to_hub_fastai`.\n3. Load it with `from_pretrained_fastai` and try it with `learner_blurr.predict()`.",
        "question": "(your factoid question)\n",
        "answer": "::\n\nHow do you load a `Learner` from the Hugging Face Hub using `from_pretrained_fastai`?\n\nTo load a `Learner` from the Hugging Face Hub using `from_pretrained_fastai`, you can use the following code:\n\n```py\nfrom huggingface_hub import from_pretrained_fastai\n\nrepo_id = \"espejelomar/identify-my-cat\"\n\nlearner = from_pretrained_fastai(repo_id)\n```\n\nThis code will load the `Learner` with the given `repo_id` and make it available for use. In this example, the `Learner` is called \"identify-my-cat\" and was created by the user \"espejelomar\".",
        "source_doc": "huggingface/blog/blob/main/fastai.md"
    },
    {
        "context": "The abstract from the paper is the following:\n\n*Detection Transformer (DETR) directly transforms queries to unique objects by using one-to-one bipartite matching during training and enables end-to-end object detection. Recently, these models have surpassed traditional detectors on COCO with undeniable elegance. However, they differ from traditional detectors in multiple designs, including model architecture and training schedules, and thus the effectiveness of one-to-one matching is not fully understood. In this work, we conduct a strict comparison between the one-to-one Hungarian matching in DETRs and the one-to-many label assignments in traditional detectors with non-maximum supervision (NMS). Surprisingly, we observe one-to-many assignments with NMS consistently outperform standard one-to-one matching under the same setting, with a significant gain of up to 2.5 mAP. Our detector that trains Deformable-DETR with traditional IoU-based label assignment achieved 50.2 COCO mAP within 12 epochs (1x schedule) with ResNet50 backbone, outperforming all existing traditional or transformer-based detectors in this setting. On multiple datasets, schedules, and architectures, we consistently show bipartite matching is unnecessary for performant detection transformers. Furthermore, we attribute the success of detection transformers to their expressive transformer architecture.*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/deta_architecture.jpg\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> DETA overview. Taken from the <a href=\"https://arxiv.org/abs/2212.06137\">original paper</a>. </small>\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr).\nThe original code can be found [here](https://github.com/jozhang97/DETA).\n\n## Resources\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with DETA.",
        "question": "What is the name of the model?\n",
        "answer": " DETA",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/deta.md"
    },
    {
        "context": "# Major ticks every half year, minor ticks every month,\n    ax.xaxis.set_major_locator(mdates.MonthLocator(bymonth=(1, 7)))\n    ax.xaxis.set_minor_locator(mdates.MonthLocator())\n\n    ax.plot(\n        index[-2*prediction_length:], \n        test_dataset[ts_index][\"target\"][-2*prediction_length:],\n        label=\"actual\",\n    )\n\n    plt.plot(\n        index[-prediction_length:], \n        np.median(forecasts[ts_index], axis=0),\n        label=\"median\",\n    )\n    \n    plt.fill_between(\n        index[-prediction_length:],\n        forecasts[ts_index].mean(0) - forecasts[ts_index].std(axis=0), \n        forecasts[ts_index].mean(0) + forecasts[ts_index].std(axis=0), \n        alpha=0.3, \n        interpolate=True,\n        label=\"+/- 1-std\",\n    )\n    plt.legend()\n    plt.show()\n```\n\nFor example:\n\n```python\nplot(334)\n```\n\n![png](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/time-series-transformers/output_65_1.png)\n    \nHow do we compare against other models? The [Monash Time Series Repository](https://forecastingdata.org/#results) has a comparison table of test set MASE metrics which we can add to:\n\n|Dataset | \tSES| \tTheta | \tTBATS| \tETS\t| (DHR-)ARIMA| \tPR|\tCatBoost |\tFFNN\t| DeepAR | \tN-BEATS | \tWaveNet| \t**Transformer** (Our) |\n|:------------------:|:-----------------:|:--:|:--:|:--:|:--:|:--:|:--:|:---:|:---:|:--:|:--:|:--:|\n|Tourism Monthly | \t3.306 |\t1.649 |\t1.751 |\t1.526|\t1.589|\t1.678\t|1.699|\t1.582\t| 1.409\t| 1.574|\t1.482\t|  **1.256**|\n\nNote that, with our model, we are beating all other models reported (see also table 2 in the corresponding [paper](https://openreview.net/pdf?id=wEc1mgAjU-)), and we didn't do any hyperparameter tuning. We just trained the Transformer for 40 epochs.",
        "question": "What is the MASE metric of the Transformer model on the Tourism Monthly dataset?\n",
        "answer": " The MASE metric of the Transformer model on the Tourism Monthly dataset is 1.256.",
        "source_doc": "huggingface/blog/blob/main/time-series-transformers.md"
    },
    {
        "context": "The SegFormer model we're going to fine-tune later expects specific names for the features. For convenience, we'll match this format now. Thus, we'll rename the `image` feature to `pixel_values` and the `label.segmentation_bitmap` to `label` and discard the other features.\n\n\n```python\nsemantic_dataset = semantic_dataset.rename_column('image', 'pixel_values')\nsemantic_dataset = semantic_dataset.rename_column('label.segmentation_bitmap', 'label')\nsemantic_dataset = semantic_dataset.remove_columns(['name', 'uuid', 'status', 'label.annotations'])\n```\n\nWe can now push the transformed dataset to the Hugging Face Hub. That way, your team and the Hugging Face community can make use of it. In the next section, we'll see how you can load the dataset from the Hub.\n\n\n```python\nhf_dataset_identifier = f\"{hf_username}/{dataset_name}\"\n\nsemantic_dataset.push_to_hub(hf_dataset_identifier)\n```\n\n## Use a dataset from the Hub\n\nIf you don't want to create your own dataset, but found a suitable dataset for your use case on the Hugging Face Hub, you can define the identifier here. \n\nFor example, you can use the full labeled sidewalk dataset. Note that you can check out the examples [directly in your browser](https://huggingface.co/datasets/segments/sidewalk-semantic).\n\n\n```python\nhf_dataset_identifier = \"segments/sidewalk-semantic\"\n```\n\n# 2. Load and prepare the Hugging Face dataset for training\n\nNow that we've created a new dataset and pushed it to the Hugging Face Hub, we can load the dataset in a single line.\n\n\n```python\nfrom datasets import load_dataset\n\nds = load_dataset(hf_dataset_identifier)\n```\n\nLet's shuffle the dataset and split the dataset in a train and test set.\n\n\n```python\nds = ds.shuffle(seed=1)\nds = ds[\"train\"].train_test_split(test_size=0.2)\ntrain_ds = ds[\"train\"]\ntest_ds = ds[\"test\"]\n```\n\nWe'll extract the number of labels and the human-readable ids, so we can configure the segmentation model correctly later on.",
        "question": "(your factoid question)\n",
        "answer": " (your answer to the factoid question)\n\nNow here is the context.\n\nContext: The SegFormer model we're going to fine-tune later expects specific names for the features. For convenience, we'll match this format now. Thus, we'll rename the `image` feature to `pixel_values` and the `label.segmentation_bitmap` to `label` and discard the other features.\n\n\n```python\nsemantic_dataset = semantic_dataset.rename_column('image', 'pixel_values')\nsemantic_dataset = semantic_dataset.rename_column('label.segmentation_bitmap', 'label')\nsemantic_dataset = semantic_dataset.remove_columns(['name', 'uuid', 'status', 'label.annotations'])\n```\n\nWe can now push the transformed dataset to the Hugging Face Hub. That way, your team and the Hugging Face community can make use of it. In the next section, we'll see how you can load the dataset from the Hub.\n\n\n```python\nhf_dataset_identifier = f\"{hf_username}/{dataset_name}\"\n\nsemantic_dataset.push_to_hub(hf_dataset_identifier)\n```\n\n## Use a dataset from the Hub\n\nIf you don't want to create your own dataset, but found a suitable dataset for your use case on the Hugging Face Hub, you can define the identifier here. \n\nFor example, you can use the full labeled sidewalk dataset. Note that you can check out the examples [directly in your browser](https://huggingface.co/datasets/segments/sidewalk-semantic).\n\n\n```python\nhf_dataset_identifier = \"segments/sidewalk-semantic\"\n```\n\n# 2. Load and prepare the Hugging Face dataset for training\n\nNow that we've created a new dataset and pushed it to the Hugging Face Hub, we can load the dataset in a single line.\n\n\n```python\nfrom datasets import load_dataset\n\nds = load_dataset(hf_dataset_identifier)\n```\n\nLet's shuffle the dataset and split the dataset in a train and test set.\n\n\n```python\nds = ds.shuffle(seed=1)\nds = ds[\"train\"].train_test_split(test_size=0.2)\ntrain_ds = ds[\"train\"]\ntest_ds = ds[\"test\"]\n```\n\nWe'll extract the number of labels and the human-readable ids, so we can configure the segmentation model correctly later on.\n\nOutput:::\n\nAnswer the question: How many labels are in the dataset?\n\nThere are 11 labels in the dataset.\n\nAnswer the question: What are the human-readable ids of the dataset?\n\nThe human-readable ids of the dataset are:\n\n* 1: background\n* 2: person\n* 3: road\n* 4: sidewalk\n* 5: building\n* 6: wall\n* 7: fence\n* 8: pole\n* 9: traffic light\n* 10: traffic sign\n* 11: vegetation",
        "source_doc": "huggingface/blog/blob/main/fine-tune-segformer.md"
    },
    {
        "context": "--\ntitle: \"Rocket Money x Hugging Face: Scaling Volatile ML Models in Production​\"\nthumbnail: /blog/assets/78_ml_director_insights/rocketmoney.png\nauthors:\n- user: nicokuzak\n  guest: true\n- user: ccpoirier\n  guest: true\n---\n\n# Rocket Money x Hugging Face: Scaling Volatile ML Models in Production\n\n\n#### \"We discovered that they were not just service providers, but partners who were invested in our goals and outcomes” _- Nicolas Kuzak, Senior ML Engineer at Rocket Money._\n\n## Scaling and Maintaining ML Models in Production Without an MLOps Team\n\nWe created [Rocket Money](https://www.rocketmoney.com/) (a personal finance app formerly known as Truebill) to help users improve their financial wellbeing. Users link their bank accounts to the app which then classifies and categorizes their transactions, identifying recurring patterns to provide a consolidated, comprehensive view of their personal financial life. A critical stage of transaction processing is detecting known merchants and services, some of which Rocket Money can cancel and negotiate the cost of for members. This detection starts with the transformation of short, often truncated and cryptically formatted transaction strings into classes we can use to enrich our product experience.\n\n## The Journey Toward a New System",
        "question": "What is the name of the personal finance app formerly known as Truebill?\n",
        "answer": " Rocket Money",
        "source_doc": "huggingface/blog/blob/main/rocketmoney-case-study.md"
    },
    {
        "context": "A trick is to make the distribution \\\\(P(w|w_{1:t-1})\\\\) sharper\n(increasing the likelihood of high probability words and decreasing the\nlikelihood of low probability words) by lowering the so-called\n`temperature` of the\n[softmax](https://en.wikipedia.org/wiki/Softmax_function#Smooth_arg_max).\n\nAn illustration of applying temperature to our example from above could\nlook as follows.\n\n<img src=\"/blog/assets/02_how-to-generate/sampling_search_with_temp.png\" alt=\"sampling temp search\" style=\"margin: auto; display: block;\">\n\nThe conditional next word distribution of step \\\\(t=1\\\\) becomes much\nsharper leaving almost no chance for word \\\\((\\text{\"car\"})\\\\) to be\nselected.\n\nLet's see how we can cool down the distribution in the library by\nsetting `temperature=0.6`:\n\n\n\n``` python\n# set seed to reproduce results. Feel free to change the seed though to get different results\nset_seed(42)\n\n# use temperature to decrease the sensitivity to low probability candidates\nsample_output = model.generate(\n    **model_inputs,\n    max_new_tokens=40,\n    do_sample=True,\n    top_k=0,\n    temperature=0.6,\n)\n\nprint(\"Output:\\n\" + 100 * '-')\nprint(tokenizer.decode(sample_output[0], skip_special_tokens=True))\n```\n\n```\nOutput:\n----------------------------------------------------------------------------------------------------\nI enjoy walking with my cute dog, but I don't like to chew on it. I like to eat it and not chew on it. I like to be able to walk with my dog.\"\n\nSo how did you decide\n```\n\n\n\nOK. There are less weird n-grams and the output is a bit more coherent\nnow\\! While applying temperature can make a distribution less random, in\nits limit, when setting `temperature` \\\\(\\to 0\\\\), temperature scaled\nsampling becomes equal to greedy decoding and will suffer from the same\nproblems as before.\n\n\n\n### Top-K Sampling",
        "question": "What is Top-K sampling in the context of language model generation?\n",
        "answer": " Top-K sampling is a method used in language model generation where the model only considers the top K most likely next words at each step, instead of considering all possible next words. This can help to reduce the randomness of the generated text and make the model's output more deterministic.",
        "source_doc": "huggingface/blog/blob/main/how-to-generate.md"
    },
    {
        "context": "Specify where to push your model and tokenizer in the [`~transformers.PushToHubCallback`]:\n\n```py\n>>> from transformers.keras_callbacks import PushToHubCallback\n\n>>> push_to_hub_callback = PushToHubCallback(\n...     output_dir=\"my_awesome_opus_books_model\",\n...     tokenizer=tokenizer,\n... )\n```\n\nThen bundle your callbacks together:\n\n```py\n>>> callbacks = [metric_callback, push_to_hub_callback]\n```\n\nFinally, you're ready to start training your model! Call [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) with your training and validation datasets, the number of epochs, and your callbacks to finetune the model:\n\n```py\n>>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=callbacks)\n```\n\nOnce training is completed, your model is automatically uploaded to the Hub so everyone can use it!\n</tf>\n</frameworkcontent>\n\n<Tip>\n\nFor a more in-depth example of how to finetune a model for translation, take a look at the corresponding\n[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation.ipynb)\nor [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation-tf.ipynb).\n\n</Tip>\n\n## Inference\n\nGreat, now that you've finetuned a model, you can use it for inference!\n\nCome up with some text you'd like to translate to another language. For T5, you need to prefix your input depending on the task you're working on. For translation from English to French, you should prefix your input as shown below:\n\n```py\n>>> text = \"translate English to French: Legumes share resources with nitrogen-fixing bacteria.\"\n```\n\nThe simplest way to try out your finetuned model for inference is to use it in a [`pipeline`]. Instantiate a `pipeline` for translation with your model, and pass your text to it:\n\n```py\n>>> from transformers import pipeline",
        "question": "(your factoid question)\n",
        "answer": " (your answer to the factoid question)\n\nNow here is the context.\n\nContext: Specify where to push your model and tokenizer in the [`~transformers.PushToHubCallback`]:\n\n```py\n>>> from transformers.keras_callbacks import PushToHubCallback\n\n>>> push_to_hub_callback = PushToHubCallback(\n...     output_dir=\"my_awesome_opus_books_model\",\n...     tokenizer=tokenizer,\n... )\n```\n\nThen bundle your callbacks together:\n\n```py\n>>> callbacks = [metric_callback, push_to_hub_callback]\n```\n\nFinally, you're ready to start training your model! Call [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) with your training and validation datasets, the number of epochs, and your callbacks to finetune the model:\n\n```py\n>>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=callbacks)\n```\n\nOnce training is completed, your model is automatically uploaded to the Hub so everyone can use it!\n</tf>\n</frameworkcontent>\n\n<Tip>\n\nFor a more in-depth example of how to finetune a model for translation, take a look at the corresponding\n[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation.ipynb)\nor [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation-tf.ipynb).\n\n</Tip>\n\n## Inference\n\nGreat, now that you've finetuned a model, you can use it for inference!\n\nCome up with some text you'd like to translate to another language. For T5, you need to prefix your input depending on the task you're working on. For translation from English to French, you should prefix your input as shown below:\n\n```py\n>>> text = \"translate English to French: Legumes share resources with nitrogen-fixing bacteria.\"\n```\n\nThe simplest way to try out your finetuned model for inference is to use it in a [`pipeline`]. Instantiate a `pipeline` for translation with your model, and pass your text to it:\n\n```py\n>>> from transformers import pipeline\n\nOutput:::\n>>> translate_pipeline = pipeline(\"translation_en_to_fr\", model=model, tokenizer=tokenizer)\n\n>>> translate_pipeline(text)\n{'input_text': 'translate English to French: Legumes share resources with nitrogen-fixing bacteria.',\n 'translation_text': 'faire partager des ressources avec des bactéries fixatrices d\\'azote: Les légumes.'}\n```\n\nYou can also use the model and tokenizer directly for inference. First, encode your text with the tokenizer:\n\n```py\n>>> inputs = tokenizer(text, return_tensors=\"tf\")\n```\n\nThen, pass your encoded inputs to the model to get the model's output:\n\n```py\n>>> outputs = model.generate(inputs[\"input_ids\"], max_length=40, num_beams=4, early_stopping=True)\n```\n\nFinally, decode the model's output to get the translated text:\n\n```py\n>>> tokenizer.decode(outputs[0])\n'faire partager des ressources avec des bactéries fixatrices d\\'azote: Les légumes.'\n```\n\n## Conclusion\n\nIn this tutorial, you've learned how to finetune a pretrained model for translation using the Hugging Face Transformers library. You've also learned how to use the finetuned model for inference.\n\nYou can use the same process to finetune a model for other tasks, such as summarization, question answering, or text classification.\n\nFor more information on how to use the Hugging Face Transformers library, take a look at the [official documentation](https://huggingface.co/transformers/index.html).\n\n",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/tasks/translation.md"
    },
    {
        "context": "| [Fine-tune T5 for Sentiment Span Extraction](https://github.com/enzoampil/t5-intro/blob/master/t5_qa_training_pytorch_span_extraction.ipynb)  | How to fine-tune T5 for sentiment span extraction using a text-to-text format with PyTorch Lightning |  [Lorenzo Ampil](https://github.com/enzoampil) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/enzoampil/t5-intro/blob/master/t5_qa_training_pytorch_span_extraction.ipynb) |\n| [Fine-tune DistilBert for Multiclass Classification](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb) | How to fine-tune DistilBert for multiclass classification with PyTorch | [Abhishek Kumar Mishra](https://github.com/abhimishra91) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb)|\n|[Fine-tune BERT for Multi-label Classification](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb)|How to fine-tune BERT for multi-label classification using PyTorch|[Abhishek Kumar Mishra](https://github.com/abhimishra91) |[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb)|\n|[Fine-tune T5 for Summarization](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb)|How to fine-tune T5 for summarization in PyTorch and track experiments with WandB|[Abhishek Kumar Mishra](https://github.com/abhimishra91) |[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb)|",
        "question": "How to fine-tune T5 for sentiment span extraction using a text-to-text format with PyTorch Lightning?\n",
        "answer": " The factoid question asks for information on how to fine-tune the T5 model for sentiment span extraction using a text-to-text format with PyTorch Lightning. The answer can be found in the first link provided in the context, which is a Jupyter notebook titled \"t5\\_qa\\_training\\_pytorch\\_span\\_extraction.ipynb\" by Lorenzo Ampil. The notebook provides a step-by-step guide on how to fine-tune the T5 model for sentiment span extraction using a text-to-text format with PyTorch Lightning.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/community.md"
    },
    {
        "context": "There are three main methods in [`DatasetBuilder`]:\n\n1. [`DatasetBuilder._info`] is in charge of defining the dataset attributes. When you call `dataset.info`, 🤗 Datasets returns the information stored here. Likewise, the [`Features`] are also specified here. Remember, the [`Features`] are like the skeleton of the dataset. It provides the names and types of each column.\n\n2. [`DatasetBuilder._split_generator`] downloads or retrieves the requested data files, organizes them into splits, and defines specific arguments for the generation process. This method has a [`DownloadManager`] that downloads files or fetches them from your local filesystem. Within the [`DownloadManager`], there is a [`DownloadManager.download_and_extract`] method that accepts a dictionary of URLs to the original data files, and downloads the requested files. Accepted inputs include: a single URL or path, or a list/dictionary of URLs or paths. Any compressed file types like TAR, GZIP and ZIP archives will be automatically extracted.\n\n   Once the files are downloaded, [`SplitGenerator`] organizes them into splits. The [`SplitGenerator`] contains the name of the split, and any keyword arguments that are provided to the [`DatasetBuilder._generate_examples`] method. The keyword arguments can be specific to each split, and typically comprise at least the local path to the data files for each split.\n\n3. [`DatasetBuilder._generate_examples`] reads and parses the data files for a split. Then it yields dataset examples according to the format specified in the `features` from [`DatasetBuilder._info`]. The input of [`DatasetBuilder._generate_examples`] is actually the `filepath` provided in the keyword arguments of the last method.",
        "question": "What method in [`DatasetBuilder`] is in charge of defining the dataset attributes?\n",
        "answer": " The method [`DatasetBuilder._info`] is in charge of defining the dataset attributes.",
        "source_doc": "huggingface/datasets/blob/main/docs/source/about_dataset_load.mdx"
    },
    {
        "context": ">>> image = cppe5[\"train\"][0][\"image\"]\n>>> annotations = cppe5[\"train\"][0][\"objects\"]\n>>> draw = ImageDraw.Draw(image)\n\n>>> categories = cppe5[\"train\"].features[\"objects\"].feature[\"category\"].names\n\n>>> id2label = {index: x for index, x in enumerate(categories, start=0)}\n>>> label2id = {v: k for k, v in id2label.items()}\n\n>>> for i in range(len(annotations[\"id\"])):\n...     box = annotations[\"bbox\"][i]\n...     class_idx = annotations[\"category\"][i]\n...     x, y, w, h = tuple(box)\n...     draw.rectangle((x, y, x + w, y + h), outline=\"red\", width=1)\n...     draw.text((x, y), id2label[class_idx], fill=\"white\")\n\n>>> image\n```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://i.imgur.com/TdaqPJO.png\" alt=\"CPPE-5 Image Example\"/>\n</div>\n\nTo visualize the bounding boxes with associated labels, you can get the labels from the dataset's metadata, specifically\nthe `category` field.\nYou'll also want to create dictionaries that map a label id to a label class (`id2label`) and the other way around (`label2id`).\nYou can use them later when setting up the model. Including these maps will make your model reusable by others if you share\nit on the Hugging Face Hub.\n\nAs a final step of getting familiar with the data, explore it for potential issues. One common problem with datasets for\nobject detection is bounding boxes that \"stretch\" beyond the edge of the image. Such \"runaway\" bounding boxes can raise\nerrors during training and should be addressed at this stage. There are a few examples with this issue in this dataset.\nTo keep things simple in this guide, we remove these images from the data.\n\n```py\n>>> remove_idx = [590, 821, 822, 875, 876, 878, 879]\n>>> keep = [i for i in range(len(cppe5[\"train\"])) if i not in remove_idx]\n>>> cppe5[\"train\"] = cppe5[\"train\"].select(keep)\n```\n\n## Preprocess the data",
        "question": "(your factoid question)\n",
        "answer": " (your answer to the factoid question)\n\nNow here is the context.\n\nContext: >>> image = cppe5[\"train\"][0][\"image\"]\n>>> annotations = cppe5[\"train\"][0][\"objects\"]\n>>> draw = ImageDraw.Draw(image)\n\n>>> categories = cppe5[\"train\"].features[\"objects\"].feature[\"category\"].names\n\n>>> id2label = {index: x for index, x in enumerate(categories, start=0)}\n>>> label2id = {v: k for k, v in id2label.items()}\n\n>>> for i in range(len(annotations[\"id\"])):\n...     box = annotations[\"bbox\"][i]\n...     class_idx = annotations[\"category\"][i]\n...     x, y, w, h = tuple(box)\n...     draw.rectangle((x, y, x + w, y + h), outline=\"red\", width=1)\n...     draw.text((x, y), id2label[class_idx], fill=\"white\")\n\n>>> image\n```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://i.imgur.com/TdaqPJO.png\" alt=\"CPPE-5 Image Example\"/>\n</div>\n\nTo visualize the bounding boxes with associated labels, you can get the labels from the dataset's metadata, specifically\nthe `category` field.\nYou'll also want to create dictionaries that map a label id to a label class (`id2label`) and the other way around (`label2id`).\nYou can use them later when setting up the model. Including these maps will make your model reusable by others if you share\nit on the Hugging Face Hub.\n\nAs a final step of getting familiar with the data, explore it for potential issues. One common problem with datasets for\nobject detection is bounding boxes that \"stretch\" beyond the edge of the image. Such \"runaway\" bounding boxes can raise\nerrors during training and should be addressed at this stage. There are a few examples with this issue in this dataset.\nTo keep things simple in this guide, we remove these images from the data.\n\n```py\n>>> remove_idx = [590, 821, 822, 875, 876, 878, 879]\n>>> keep = [i for i in range(len(cppe5[\"train\"])) if i not in remove_idx]\n>>> cppe5[\"train\"] = cppe5[\"train\"].select(keep)\n```\n\n## Preprocess the data\n\nOutput:::\n\nTo preprocess the data, you'll need to perform the following steps:\n\n1. **Resize the images**: You'll want to resize the images to a fixed size.\n   This is necessary to ensure that the input to the model is of a consistent shape.\n   You can use the `transforms` library from the `torchvision` package to perform this operation.\n\n2. **Normalize the images**: You'll want to normalize the images to ensure that the input to the model is in a consistent range.\n   You can use the mean and standard deviation of the ImageNet dataset to perform this operation.\n\n3. **Convert the bounding boxes**: You'll want to convert the bounding boxes to a format that's compatible with the model.\n   Specifically, you'll want to convert the bounding boxes to the format expected by the model's forward method.\n\n4. **Create a Dataset class**: You'll want to create a custom Dataset class that returns the preprocessed data and the corresponding labels.\n   This will allow you to use the data in a PyTorch DataLoader.\n\n```\n\nTo resize the images, you can use the `transforms` library from the `torchvision` package.\nYou can use the `Resize` transformation to resize the images to a fixed size.\n\n```py\nfrom torchvision.transforms import Resize\n\nresize = Resize((256, 256))\n```\n\nTo normalize the images, you can use the `transforms` library from the `torchvision` package.\nYou can use the `Normalize` transformation to normalize the images to the range of [0, 1].\n\n```py\nfrom torchvision.transforms import Normalize\n\nnormalize =",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/tasks/object_detection.md"
    },
    {
        "context": "- Weight Standardization\n    Tasks:\n    - Image Classification\n    Training Techniques:\n    - Mixup\n    - SGD with Momentum\n    - Weight Decay\n    Training Data:\n    - ImageNet\n    - JFT-300M\n    Training Resources: Cloud TPUv3-512\n    ID: resnetv2_101x3_bitm\n    LR: 0.03\n    Epochs: 90\n    Layers: 101\n    Crop Pct: '1.0'\n    Momentum: 0.9\n    Batch Size: 4096\n    Image Size: '480'\n    Weight Decay: 0.0001\n    Interpolation: bilinear\n  Code: https://github.com/rwightman/pytorch-image-models/blob/b9843f954b0457af2db4f9dea41a8538f51f5d78/timm/models/resnetv2.py#L451\n  Weights: https://storage.googleapis.com/bit_models/BiT-M-R101x3-ILSVRC2012.npz\n  Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 84.38%\n      Top 5 Accuracy: 97.37%\n- Name: resnetv2_152x2_bitm\n  In Collection: Big Transfer\n  Metadata:\n    FLOPs: 10659792\n    Parameters: 236340000\n    File Size: 945476668\n    Architecture:\n    - 1x1 Convolution\n    - Bottleneck Residual Block\n    - Convolution\n    - Global Average Pooling\n    - Group Normalization\n    - Max Pooling\n    - ReLU\n    - Residual Block\n    - Residual Connection\n    - Softmax\n    - Weight Standardization\n    Tasks:\n    - Image Classification\n    Training Techniques:\n    - Mixup\n    - SGD with Momentum\n    - Weight Decay\n    Training Data:\n    - ImageNet\n    - JFT-300M\n    ID: resnetv2_152x2_bitm\n    Crop Pct: '1.0'\n    Image Size: '480'\n    Interpolation: bilinear\n  Code: https://github.com/rwightman/pytorch-image-models/blob/b9843f954b0457af2db4f9dea41a8538f51f5d78/timm/models/resnetv2.py#L458\n  Weights: https://storage.googleapis.com/bit_models/BiT-M-R152x2-ILSVRC2012.npz\n  Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 84.4%\n      Top 5 Accuracy: 97.43%\n- Name: resnetv2_152x4_bitm\n  In Collection: Big Transfer\n  Metadata:\n    FLOPs: 21317584\n    Parameters: 936530000\n    File Size: 3746270104\n    Architecture:\n    - 1x1 Convolution",
        "question": "What is the FLOPs of resnetv2_152x2_bitm?\n",
        "answer": " The FLOPs of resnetv2_152x2_bitm is 10659792.",
        "source_doc": "huggingface/pytorch-image-models/blob/main/docs/models/big-transfer.md"
    },
    {
        "context": "The output of the metric depends on the IndicGLUE subset chosen, consisting of a dictionary that contains one or several of the following metrics:\n\n`accuracy`: the proportion of correct predictions among the total number of cases processed, with a range between 0 and 1 (see [accuracy](https://huggingface.co/metrics/accuracy) for more information). \n\n`f1`: the harmonic mean of the precision and recall (see [F1 score](https://huggingface.co/metrics/f1) for more information). Its range is 0-1 -- its lowest possible value is 0, if either the precision or the recall is 0, and its highest possible value is 1.0, which means perfect precision and recall.\n\n`precision@10`: the fraction of the true examples among the top 10 predicted examples, with a range between 0 and 1 (see [precision](https://huggingface.co/metrics/precision) for more information). \n\nThe `cvit-mkb-clsr` subset returns `precision@10`, the `wiki-ner` subset returns `accuracy` and `f1`, and all other subsets of Indic GLUE return only accuracy. \n\n### Values from popular papers\n\nThe [original IndicGlue paper](https://aclanthology.org/2020.findings-emnlp.445.pdf) reported an average accuracy of 0.766 on the dataset, which varies depending on the subset selected.\n\n## Examples \n\nMaximal values for the WNLI subset (which outputs `accuracy`):\n\n```python\nindic_glue_metric = evaluate.load('indic_glue', 'wnli') \nreferences = [0, 1]\npredictions = [0, 1]\nresults = indic_glue_metric.compute(predictions=predictions, references=references)\nprint(results)\n{'accuracy': 1.0}\n```\n\nMinimal values for the Wiki-NER subset (which outputs `accuracy` and `f1`):\n\n```python\n>>> indic_glue_metric = evaluate.load('indic_glue', 'wiki-ner')\n>>> references = [0, 1]\n>>> predictions = [1,0]\n>>> results = indic_glue_metric.compute(predictions=predictions, references=references)\n>>> print(results)\n{'accuracy': 1.0, 'f1': 1.0}\n```\n\nPartial match for the CVIT-Mann Ki Baat subset (which outputs `precision@10`)",
        "question": "What metric does the CVIT-Mann Ki Baat subset return?\n",
        "answer": " The CVIT-Mann Ki Baat subset returns precision@10.",
        "source_doc": "huggingface/evaluate/blob/main/metrics/indic_glue/README.md"
    },
    {
        "context": "# Step 3: Use ONNX Runtime for training!🤗\ntrainer.train()\n```\n\n## Looking Forward\n\nThe Hugging Face team is working on open sourcing more large models and lowering the barrier for users to benefit from them with acceleration tools on both training and inference. We are collaborating with the ONNX Runtime training team to bring more training optimizations to newer and larger model architectures, including Whisper and Stable Diffusion. Microsoft has also packaged its state-of-the-art training acceleration technologies in the [Azure Container for PyTorch](https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/enabling-deep-learning-with-azure-container-for-pytorch-in-azure/ba-p/3650489). This is a light-weight curated environment including DeepSpeed and ONNX Runtime to improve productivity for AI developers training with PyTorch. In addition to large model training, the ONNX Runtime training team is also building new solutions for learning on the edge – training on devices that are constrained on memory and power.\n\n## Getting Started\n\nWe invite you to check out the links below to learn more about, and get started with, Optimum ONNX Runtime Training for your Hugging Face models.",
        "question": "What is the name of the light-weight curated environment for AI developers training with PyTorch?\n",
        "answer": " The Azure Container for PyTorch",
        "source_doc": "huggingface/blog/blob/main/optimum-onnxruntime-training.md"
    },
    {
        "context": "| [fastai](https://github.com/fastai/fastai)                                  | Library to train fast and accurate models with state-of-the-art outputs.             | ✅ | ✅ | ✅ | ✅ |\n| [Keras](https://huggingface.co/docs/hub/keras)                              | Library that uses a consistent and simple API to build models leveraging TensorFlow and its ecosystem. | ❌ | ❌ | ✅ | ✅ |\n| [Flair](https://github.com/flairNLP/flair)                                  | Very simple framework for state-of-the-art NLP.                                      | ✅ | ✅ | ✅ | ✅ |\n| [MBRL-Lib](https://github.com/facebookresearch/mbrl-lib)                    | PyTorch implementations of MBRL Algorithms.                                          | ❌ | ❌ | ✅ | ✅ |\n| [MidiTok](https://github.com/Natooz/MidiTok)                                | Tokenizers for symbolic music / MIDI files.                                          | ❌ | ❌ | ✅ | ✅ |\n| [ML-Agents](https://github.com/huggingface/ml-agents)                       | Enables games and simulations made with Unity to serve as environments for training intelligent agents. | ❌ | ❌ | ✅ | ✅ |\n| [NeMo](https://github.com/NVIDIA/NeMo)                                      | Conversational AI toolkit built for researchers                                      | ✅ | ✅ | ✅ | ❌ |\n| [OpenCLIP](https://github.com/mlfoundations/open_clip)                      | Library for open-source implementation of OpenAI's CLIP                              | ❌ | ❌ | ✅ | ✅ |  \n| [PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)                      | Easy-to-use and powerful NLP library built on PaddlePaddle                           | ✅ | ✅ | ✅ | ✅ |\n| [PEFT](https://github.com/huggingface/peft)                      | Cutting-edge Parameter Efficient Fine-tuning Library                           | ✅ | ✅ | ✅ | ✅ |",
        "question": "Which library is a conversational AI toolkit built for researchers?\n",
        "answer": " NeMo",
        "source_doc": "huggingface/hub-docs/blob/main/docs/hub/models-libraries.md"
    },
    {
        "context": "All of the currently available GPT-Neo checkpoints are trained with the Pile dataset, a large text corpus that is extensively documented in ([Gao et al., 2021](https://arxiv.org/abs/2101.00027)). As such, it is expected to function better on the text that matches the distribution of its training text; we recommend keeping this in mind when designing your examples.\n\n---\n\n## 🤗 Accelerated Inference API\n\nThe [Accelerated Inference API](https://huggingface.co/inference-api) is our hosted service to run inference on any of the 10,000+ models publicly available on the 🤗 Model Hub, or your own private models, via simple API calls. The API includes acceleration on CPU and GPU with [up to 100x speedup](https://huggingface.co/blog/accelerated-inference) compared to out of the box deployment of Transformers.\n\nTo integrate Few-Shot Learning predictions with `GPT-Neo` in your own apps, you can use the 🤗 Accelerated Inference API with the code snippet below. You can find your API Token [here](https://huggingface.co/settings/token), if you don't have an account you can get started [here](https://huggingface.co/pricing).\n\n```python\nimport json\nimport requests\n\nAPI_TOKEN = \"\"\n\ndef query(payload='',parameters=None,options={'use_cache': False}):\n    API_URL = \"https://api-inference.huggingface.co/models/EleutherAI/gpt-neo-2.7B\"\n\t\theaders = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n    body = {\"inputs\":payload,'parameters':parameters,'options':options}\n    response = requests.request(\"POST\", API_URL, headers=headers, data= json.dumps(body))\n    try:\n      response.raise_for_status()\n    except requests.exceptions.HTTPError:\n        return \"Error:\"+\" \".join(response.json()['error'])\n    else:\n      return response.json()[0]['generated_text']\n\nparameters = {\n    'max_new_tokens':25,  # number of generated tokens\n    'temperature': 0.5,   # controlling the randomness of generations\n    'end_sequence': \"###\" # stopping sequence for generation\n}\n\nprompt=\"....\"             # few-shot prompt",
        "question": "What is the API URL for the Accelerated Inference API?\n",
        "answer": " The API URL for the Accelerated Inference API is \"<https://api-inference.huggingface.co>\".",
        "source_doc": "huggingface/blog/blob/main/few-shot-learning-gpt-neo-and-inference-api.md"
    },
    {
        "context": "- update logo by [@gary149](https://github.com/gary149) in [PR 1266](https://github.com/gradio-app/gradio/pull/1266)\n- fix slider by [@aliabid94](https://github.com/aliabid94) in [PR 1268](https://github.com/gradio-app/gradio/pull/1268)\n- maybe fix auth in iframes by [@pngwn](https://github.com/pngwn) in [PR 1261](https://github.com/gradio-app/gradio/pull/1261)\n- Improves \"Getting Started\" guide by [@abidlabs](https://github.com/abidlabs) in [PR 1269](https://github.com/gradio-app/gradio/pull/1269)\n- Add embedded demos to website by [@aliabid94](https://github.com/aliabid94) in [PR 1270](https://github.com/gradio-app/gradio/pull/1270)\n- Label hotfixes by [@abidlabs](https://github.com/abidlabs) in [PR 1281](https://github.com/gradio-app/gradio/pull/1281)\n- General tweaks by [@pngwn](https://github.com/pngwn) in [PR 1276](https://github.com/gradio-app/gradio/pull/1276)\n- only affect links within the document by [@pngwn](https://github.com/pngwn) in [PR 1282](https://github.com/gradio-app/gradio/pull/1282)\n- release 3.0b9 by [@abidlabs](https://github.com/abidlabs) in [PR 1283](https://github.com/gradio-app/gradio/pull/1283)\n- Dm by [@pngwn](https://github.com/pngwn) in [PR 1284](https://github.com/gradio-app/gradio/pull/1284)\n- Website fixes by [@aliabd](https://github.com/aliabd) in [PR 1286](https://github.com/gradio-app/gradio/pull/1286)\n- Create Streamables by [@aliabid94](https://github.com/aliabid94) in [PR 1279](https://github.com/gradio-app/gradio/pull/1279)\n- ensure table works on mobile by [@pngwn](https://github.com/pngwn) in [PR 1277](https://github.com/gradio-app/gradio/pull/1277)\n- changes by [@aliabid94](https://github.com/aliabid94) in [PR 1287](https://github.com/gradio-app/gradio/pull/1287)\n- demo alignment on landing page by [@aliabd](https://github.com/aliabd) in [PR 1288](https://github.com/gradio-app/gradio/pull/1288)\n- New meta img by [@aliabd](https://github.com/aliabd) in [PR 1289](https://github.com/gradio-app/gradio/pull/1289)",
        "question": "Who fixed the slider in the gradio app?\n",
        "answer": " The slider was fixed by @aliabid94 in PR 1268.",
        "source_doc": "gradio-app/gradio/blob/main/CHANGELOG.md"
    },
    {
        "context": "We have observed that distilled models are up to 100% faster than the original base models. The Benchmarking code can be found [here](https://github.com/segmind/distill-sd/blob/master/inference.py).\n\n<p align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/distill_sd/Picture4.jpeg\" width=500>\n</p>\n\n## Potential Limitations\n\nThe distilled models are in early phase and the outputs may not be at a production quality yet.\nThese models may not be the best general models. They are best used as fine-tuned or LoRA trained on specific concepts/styles.\nDistilled models are not very good at composibility or multiconcepts yet.\n\n## Fine-tuning SD-tiny model on portrait dataset\n\nWe have fine-tuned our sd-tiny model on portrait images generated with the Realistic Vision v4.0 model. Below are the fine tuning parameters used.\n\n- Steps: 131000\n- Learning rate: 1e-4\n- Batch size: 32\n- Gradient accumulation steps: 4\n- Image resolution: 768\n- Dataset size - 7k images\n- Mixed-precision: fp16\n\nWe were able to produce image quality close to the images produced by the original model, with almost 40% fewer parameters and the sample results below speak for themselves:\n\n<p align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/distill_sd/Picture5.png\" width=500>\n</p>\n\n\nThe code for fine-tuning the base models can be found [here](https://github.com/segmind/distill-sd/blob/master/checkpoint_training.py).\n\n## LoRA Training\n\nOne of the advantages of LoRA training on a distilled model is faster training. Below are some of the images of the first LoRA we trained on the distilled model on some abstract concepts. The code for the LoRA training can be found [here](https://github.com/segmind/distill-sd/blob/master/lora_training.py).",
        "question": "How much faster are distilled models than the original base models?\n",
        "answer": " Distilled models are up to 100% faster than the original base models.",
        "source_doc": "huggingface/blog/blob/main/sd_distillation.md"
    },
    {
        "context": "In our opinion, the library's code is not just a means to provide a product, *e.g.* the ability to use BERT for\ninference, but also as the very product that we want to improve. Hence, when adding a model, the user is not only the\nperson who will use your model, but also everybody who will read, try to understand, and possibly tweak your code.\n\nWith this in mind, let's go a bit deeper into the general library design.\n\n### Overview of models\n\nTo successfully add a model, it is important to understand the interaction between your model and its config,\n[`PreTrainedModel`], and [`PretrainedConfig`]. For exemplary purposes, we will\ncall the model to be added to 🤗 Transformers `BrandNewBert`.\n\nLet's take a look:\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers_overview.png\"/>",
        "question": "What is the name of the model to be added?\n",
        "answer": " The name of the model to be added is BrandNewBert.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/add_new_model.md"
    },
    {
        "context": "<br>\n<div style=\"background-color: #f0fcf0; padding: 8px 32px 1px; outline: 1px solid; border-radius: 10px;\">\n\nIn order to use 6-bit models, you need the development versions of iOS/iPadOS 17 or macOS 14 (Sonoma) because those are the ones that contain the latest Core ML framework. You can download them from the [Apple developer site](https://developer.apple.com) if you are a registered developer, or you can sign up for the public beta that will be released in a few weeks.\n\n</div>\n<br>\n\nNote that each variant is available in Core ML format and also as a `zip` archive. Zip files are ideal for native apps, such as our [open-source demo app](https://github.com/huggingface/swift-coreml-diffusers) and other [third party tools](https://github.com/godly-devotion/MochiDiffusion). If you just want to run the models on your own hardware, the easiest way is to use our demo app and select the quantized model you want to test. You need to compile the app using Xcode, but an update will be available for download in the App Store soon. For more details, check [our previous post](https://huggingface.co/blog/fast-mac-diffusers).\n\n<img style=\"border:none;\" alt=\"Running 6-bit stable-diffusion-2-1-base model in demo app\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/fast-diffusers-coreml/diffusers_mac_screenshot.png\" />\n<small>Running 6-bit stable-diffusion-2-1-base model in demo app</small>\n\nIf you want to download a particular Core ML package to integrate it in your own Xcode project, you can clone the repos or just download the version of interest using code like the following.\n\n```Python\nfrom huggingface_hub import snapshot_download\nfrom pathlib import Path\n\nrepo_id = \"apple/coreml-stable-diffusion-2-1-base-palettized\"\nvariant = \"original/packages\"",
        "question": "What is the repository ID for the 6-bit stable diffusion 2.1 base model?\n",
        "answer": " The repository ID for the 6-bit stable diffusion 2.1 base model is apple/coreml-stable-diffusion-2-1-base-palettized.",
        "source_doc": "huggingface/blog/blob/main/fast-diffusers-coreml.md"
    },
    {
        "context": "## Resources\n\nDemo notebooks regarding inference as well as fine-tuning ViT on custom data can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/VisionTransformer).\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with ViT. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n\n`ViTForImageClassification` is supported by:\n<PipelineTag pipeline=\"image-classification\"/>\n\n- A blog post on how to [Fine-Tune ViT for Image Classification with Hugging Face Transformers](https://huggingface.co/blog/fine-tune-vit)\n- A blog post on [Image Classification with Hugging Face Transformers and `Keras`](https://www.philschmid.de/image-classification-huggingface-transformers-keras)\n- A notebook on [Fine-tuning for Image Classification with Hugging Face Transformers](https://github.com/huggingface/notebooks/blob/main/examples/image_classification.ipynb)\n- A notebook on how to [Fine-tune the Vision Transformer on CIFAR-10 with the Hugging Face Trainer](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_the_%F0%9F%A4%97_Trainer.ipynb)\n- A notebook on how to [Fine-tune the Vision Transformer on CIFAR-10 with PyTorch Lightning](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_PyTorch_Lightning.ipynb)\n\n⚗️ Optimization\n\n- A blog post on how to [Accelerate Vision Transformer (ViT) with Quantization using Optimum](https://www.philschmid.de/optimizing-vision-transformer)\n\n⚡️ Inference",
        "question": "How to fine-tune the Vision Transformer on CIFAR-10 with the Hugging Face Trainer?\n",
        "answer": " A notebook on how to Fine-tune the Vision Transformer on CIFAR-10 with the Hugging Face Trainer can be found [here](https://github.com/N",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/vit.md"
    },
    {
        "context": "**Notes**:\n\n*  The `train_text_to_image_sdxl.py` script pre-computes text embeddings and the VAE encodings and keeps them in memory. While for smaller datasets like [`lambdalabs/pokemon-blip-captions`](https://hf.co/datasets/lambdalabs/pokemon-blip-captions), it might not be a problem, it can definitely lead to memory problems when the script is used on a larger dataset. For those purposes, you would want to serialize these pre-computed representations to disk separately and load them during the fine-tuning process. Refer to [this PR](https://github.com/huggingface/diffusers/pull/4505) for a more in-depth discussion.\n* The training script is compute-intensive and may not run on a consumer GPU like Tesla T4.\n* The training command shown above performs intermediate quality validation in between the training epochs and logs the results to Weights and Biases. `--report_to`, `--validation_prompt`, and `--validation_epochs` are the relevant CLI arguments here.\n* SDXL's VAE is known to suffer from numerical instability issues. This is why we also expose a CLI argument namely `--pretrained_vae_model_name_or_path` that lets you specify the location of a better VAE (such as [this one](https://huggingface.co/madebyollin/sdxl-vae-fp16-fix)).\n\n### Inference\n\n```python\nfrom diffusers import DiffusionPipeline\nimport torch\n\nmodel_path = \"you-model-id-goes-here\" # <-- change this\npipe = DiffusionPipeline.from_pretrained(model_path, torch_dtype=torch.float16)\npipe.to(\"cuda\")\n\nprompt = \"A pokemon with green eyes and red legs.\"\nimage = pipe(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\nimage.save(\"pokemon.png\")\n```\n\n### Inference in Pytorch XLA\n```python\nfrom diffusers import DiffusionPipeline\nimport torch\nimport torch_xla.core.xla_model as xm\n\nmodel_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\npipe = DiffusionPipeline.from_pretrained(model_id)\n\ndevice = xm.xla_device()\npipe.to(device)",
        "question": "What is the CLI argument that lets you specify the location of a better VAE?\n",
        "answer": " The CLI argument is `--pretrained_vae_model_name_or_path`.",
        "source_doc": "huggingface/diffusers/blob/main/examples/text_to_image/README_sdxl.md"
    },
    {
        "context": "--\ntitle: \"Introducing DOI: the Digital Object Identifier to Datasets and Models\"\nthumbnail: /blog/assets/107_launching_doi/thumbnail.jpeg\nauthors:\n- user: sasha\n- user: Sylvestre\n- user: cakiki\n  guest: true\n- user: aleroy\n  guest: true\n---\n\n  \n\n# Introducing DOI: the Digital Object Identifier to Datasets and Models\n\n\nOur mission at Hugging Face is to democratize good machine learning. That includes best practices that make ML models and datasets more reproducible, better documented, and easier to use and share.\n\nTo solve this challenge, **we're excited to announce that you can now generate a DOI for your model or dataset directly from the Hub**!\n\n![](assets/107_launching_doi/repo-settings.png)\n\nDOIs can be generated directly from your repo settings, and anyone will then be able to cite your work by clicking \"Cite this model/dataset\" on your model or dataset page 🔥.\n\n<kbd>\n  <img alt=\"Generating DOI\" src=\"assets/107_launching_doi/doi.gif\">\n</kbd>\n\n## DOIs in a nutshell and why do they matter?\n\nDOIs (Digital Object Identifiers) are strings uniquely identifying a digital object, anything from articles to figures, including datasets and models. DOIs are tied to object metadata, including the object's URL, version, creation date, description, etc. They are a commonly accepted reference to digital resources across research and academic communities; they are analogous to a book's ISBN.\n\nDOIs make finding information about a model or dataset easier and sharing them with the world via a permanent link that will never expire or change. As such, datasets/models with DOIs are intended to persist perpetually and may only be deleted upon filing a request with our support.\n\n## How are DOIs being assigned by Hugging Face? \n\nWe have partnered with [DataCite](https://datacite.org) to allow registered Hub users to request a DOI for their model or dataset. Once they’ve filled out the necessary metadata, they receive a shiny new DOI 🌟!",
        "question": "How does Hugging Face assign DOIs to models and datasets?\n",
        "answer": " Hugging Face assigns DOIs to models and datasets by partnering with DataCite to allow registered Hub users to request a DOI after filling out necessary metadata.",
        "source_doc": "huggingface/blog/blob/main/introducing-doi.md"
    },
    {
        "context": "## 5. Wrapping up\nSentiment analysis with Python has never been easier! Tools such as [🤗Transformers](https://github.com/huggingface/transformers) and the [🤗Hub](https://huggingface.co/models) makes sentiment analysis accessible to all developers. You can use open source, pre-trained models for sentiment analysis in just a few lines of code 🔥\n\nDo you want to train a custom model for sentiment analysis with your own data? Easy peasy! You can fine-tune a model using [Trainer API](https://huggingface.co/docs/transformers/v4.15.0/en/main_classes/trainer#transformers.Trainer) to build on top of large language models and get state-of-the-art results. If you want something even easier, you can use [AutoNLP](https://huggingface.co/autonlp) to train custom machine learning models by simply uploading data.\n\nIf you have questions, the Hugging Face community can help answer and/or benefit from, please ask them in the [Hugging Face forum](https://discuss.huggingface.co/). Also, join our [discord server](https://discord.gg/YRAq8fMnUG) to talk with us and with the Hugging Face community.",
        "question": "How can I train a custom model for sentiment analysis using AutoNLP?\n\n",
        "answer": " You can train a custom machine learning model for sentiment analysis using AutoNLP by simply uploading your data on the AutoNLP platform.",
        "source_doc": "huggingface/blog/blob/main/sentiment-analysis-python.md"
    },
    {
        "context": "```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n\t\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, variant=\"fp16\"\n).to(\"cuda\")\nimage = pipeline(\n\t\"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\", height=768, width=512\n).images[0]\nimage\n```\n\n<div class=\"flex justify-center\">\n\t<img class=\"rounded-xl\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/text2img-hw.png\"/>\n</div>\n\n<Tip warning={true}>\n\nOther models may have different default image sizes depending on the image sizes in the training dataset. For example, SDXL's default image size is 1024x1024 and using lower `height` and `width` values may result in lower quality images. Make sure you check the model's API reference first!\n\n</Tip>\n\n### Guidance scale\n\nThe `guidance_scale` parameter affects how much the prompt influences image generation. A lower value gives the model \"creativity\" to generate images that are more loosely related to the prompt. Higher `guidance_scale` values push the model to follow the prompt more closely, and if this value is too high, you may observe some artifacts in the generated image.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n\t\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16\n).to(\"cuda\")\nimage = pipeline(\n\t\"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\", guidance_scale=3.5\n).images[0]\nimage\n```",
        "question": "What is the effect of the guidance scale parameter on the generated image?\n",
        "answer": " The guidance scale parameter affects how much the prompt influences image generation. A lower value gives the model \"creativity\" to generate images that are more loosely related to the prompt. Higher guidance scale values push the model to follow the prompt more closely, and if this value is too high, you may observe some artifacts in the generated image.",
        "source_doc": "huggingface/diffusers/blob/main/docs/source/en/using-diffusers/conditional_image_generation.md"
    },
    {
        "context": "## Taking it up a Notch\n\nWe saw how we could use the `ModelSearchArguments` and `DatasetSearchArguments` to remove the guesswork from when we want to search the Hub, but what about if we have a very complex, messy query?\n\nSuch as:\nI want to search for all models trained for both `text-classification` and `zero-shot` classification, were trained on the Multi NLI and GLUE datasets, and are compatible with both PyTorch and TensorFlow (a more exact query to get the above model). \n\nTo setup this query, we'll make use of the `ModelFilter` class. It's designed to handle these types of situations, so we don't need to scratch our heads:\n\n```python\n>>> filt = ModelFilter(\n>>>     task = [\"text-classification\", \"zero-shot-classification\"],\n>>>     trained_dataset = [model_args.dataset.multi_nli, model_args.dataset.glue],\n>>>     library = ['pytorch', 'tensorflow']\n>>> )\n>>> api.list_models(filt)\n```\n```\n    [ModelInfo: {\n     \tmodelId: Jiva/xlm-roberta-large-it-mnli\n     \tsha: c6e64469ec4aa17fedbd1b2522256f90a90b5b86\n     \tlastModified: 2021-12-10T14:56:38.000Z\n     \ttags: ['pytorch', 'xlm-roberta', 'text-classification', 'it', 'dataset:multi_nli', 'dataset:glue', 'arxiv:1911.02116', 'transformers', 'tensorflow', 'license:mit', 'zero-shot-classification']\n     \tpipeline_tag: zero-shot-classification\n     \tsiblings: [ModelFile(rfilename='.gitattributes'), ModelFile(rfilename='README.md'), ModelFile(rfilename='config.json'), ModelFile(rfilename='pytorch_model.bin'), ModelFile(rfilename='sentencepiece.bpe.model'), ModelFile(rfilename='special_tokens_map.json'), ModelFile(rfilename='tokenizer.json'), ModelFile(rfilename='tokenizer_config.json')]\n     \tconfig: None\n     \tprivate: False\n     \tdownloads: 680\n     \tlibrary_name: transformers\n     \tlikes: 1\n     }]\n```\n\n\nVery quickly we see that it's a much more coordinated approach for searching through the API, with no added headache for you!\n\n## What is the magic?",
        "question": "What is the class used to handle complex search queries in the Hugging Face Model Hub API?\n",
        "answer": " The `ModelFilter` class",
        "source_doc": "huggingface/blog/blob/main/searching-the-hub.md"
    },
    {
        "context": "1. **[ImageGPT](https://huggingface.co/docs/transformers/model_doc/imagegpt)** (OpenAI 에서) Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, Ilya Sutskever 의 [Generative Pretraining from Pixels](https://openai.com/blog/image-gpt/) 논문과 함께 발표했습니다.\n1. **[Informer](https://huggingface.co/docs/transformers/model_doc/informer)** (from Beihang University, UC Berkeley, Rutgers University, SEDD Company) released with the paper [Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting](https://arxiv.org/abs/2012.07436) by Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.\n1. **[InstructBLIP](https://huggingface.co/docs/transformers/model_doc/instructblip)** (Salesforce 에서 제공)은 Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi.의 [InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://arxiv.org/abs/2305.06500)논문과 함께 발표했습니다.\n1. **[Jukebox](https://huggingface.co/docs/transformers/model_doc/jukebox)** (OpenAI 에서) Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, Ilya Sutskever 의 [Jukebox: A Generative Model for Music](https://arxiv.org/pdf/2005.00341.pdf) 논문과 함께 발표했습니다.\n1. **[KOSMOS-2](https://huggingface.co/docs/transformers/model_doc/kosmos-2)** (from Microsoft Research Asia) released with the paper [Kosmos-2: Grounding Multimodal Large Language Models to the World](https://arxiv.org/abs/2306.14824) by Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, Furu Wei.\n1. **[LayoutLM](https://huggingface.co/docs/transformers/model_doc/layoutlm)** (Microsoft Research Asia 에서) Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou 의 [LayoutLM: Pre-training of Text and Layout for Document Image Understanding](https://arxiv.org/abs/1912.13318) 논문과 함께 발표했습니다.",
        "question": "When was the LayoutLM model presented?\n",
        "answer": " The LayoutLM model was presented in December 2019.",
        "source_doc": "huggingface/transformers/blob/main/README_ko.md"
    },
    {
        "context": "### [Compressive Transformers for Long-Range Sequence Modelling](https://arxiv.org/abs/1911.05507)\n\nJack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Timothy P. Lillicrap\n\n[Transformer-XL (2019)](https://arxiv.org/abs/1901.02860) showed that caching previously computed layer activations in a memory can boost performance on language modeling tasks (such as *enwik8*). Instead of just attending the current \\\\(n\\\\) input tokens, the model can also attend to the past \\\\(n_m\\\\) tokens, with \\\\(n_m\\\\) being the memory size of the model. Transformer-XL has a memory complexity of \\\\(O(n^2+ n n_m)\\\\), which shows that memory cost can increase significantly for very large \\\\(n_m\\\\). Hence, Transformer-XL has to eventually discard past activations from the memory when the number of cached activations gets larger than \\\\(n_m\\\\). Compressive Transformer addresses this problem by adding an additional compressed memory to efficiently cache past activations that would have otherwise eventually been discarded. This way the model can learn better long-range sequence dependencies having access to significantly more past activations.\n\n\n<figure>\n  <img src=\"/blog/assets/14_long_range_transformers/CompressiveTransformer.png\" alt=\"Compressive Tranformer recurrence\"/>\n  <figcaption>Figure taken from Compressive Transfomer</figcaption>\n</figure>",
        "question": "What is the memory complexity of Transformer-XL?\n",
        "answer": " The memory complexity of Transformer-XL is O(n^2 + n\\_m), where n\\_m is the memory size of the model.",
        "source_doc": "huggingface/blog/blob/main/long-range-transformers.md"
    },
    {
        "context": "## 1.10.1\n\n### Patch Changes\n\n- Updated dependencies [[`92278729e`](https://github.com/gradio-app/gradio/commit/92278729ee008126af15ffe6be399236211b2f34), [`e8216be94`](https://github.com/gradio-app/gradio/commit/e8216be948f76ce064595183d11e9148badf9421)]:\n  - @gradio/gallery@0.4.3\n  - @gradio/dataframe@0.3.3\n\n## 1.10.0\n\n### Features\n\n- [#6261](https://github.com/gradio-app/gradio/pull/6261) [`8bbeca0e7`](https://github.com/gradio-app/gradio/commit/8bbeca0e772a5a2853d02a058b35abb2c15ffaf1) - Improve Embed and CDN handling and fix a couple of related bugs. Thanks [@pngwn](https://github.com/pngwn)!\n\n### Fixes\n\n- [#6266](https://github.com/gradio-app/gradio/pull/6266) [`e32bac894`](https://github.com/gradio-app/gradio/commit/e32bac8944c85e0ec4831963299889d6bbfa0351) - Fix updating interactive prop. Thanks [@abidlabs](https://github.com/abidlabs)!\n- [#6213](https://github.com/gradio-app/gradio/pull/6213) [`27194a987`](https://github.com/gradio-app/gradio/commit/27194a987fa7ba1234b5fc0ce8bf7fabef7033a9) - Ensure the statustracker for `gr.Image` displays in static mode. Thanks [@pngwn](https://github.com/pngwn)!\n- [#6234](https://github.com/gradio-app/gradio/pull/6234) [`aaa55ce85`](https://github.com/gradio-app/gradio/commit/aaa55ce85e12f95aba9299445e9c5e59824da18e) - Video/Audio fixes. Thanks [@freddyaboulton](https://github.com/freddyaboulton)!\n- [#6236](https://github.com/gradio-app/gradio/pull/6236) [`6bce259c5`](https://github.com/gradio-app/gradio/commit/6bce259c5db7b21b327c2067e74ea20417bc89ec) - Ensure `gr.CheckboxGroup` updates as expected. Thanks [@pngwn](https://github.com/pngwn)!\n\n## 1.9.2\n\n### Fixes\n\n- [#6191](https://github.com/gradio-app/gradio/pull/6191) [`b555bc09f`](https://github.com/gradio-app/gradio/commit/b555bc09ffe8e58b10da6227e2f11a0c084aa71d) - fix cdn build. Thanks [@pngwn](https://github.com/pngwn)!\n\n## 1.9.1\n\n### Features",
        "question": "(your factoid question)\n",
        "answer": " The version of the `@gradio/gallery` package in the `1.10.1` release is `0.4.3`.",
        "source_doc": "gradio-app/gradio/blob/main/js/app/CHANGELOG.md"
    },
    {
        "context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Trainer\n\nThe [`Trainer`] is a complete training and evaluation loop for PyTorch models implemented in the Transformers library. You only need to pass it the necessary pieces for training (model, tokenizer, dataset, evaluation function, training hyperparameters, etc.), and the [`Trainer`] class takes care of the rest. This makes it easier to start training faster without manually writing your own training loop. But at the same time, [`Trainer`] is very customizable and offers a ton of training options so you can tailor it to your exact training needs.\n\n<Tip>\n\nIn addition to the [`Trainer`] class, Transformers also provides a [`Seq2SeqTrainer`] class for sequence-to-sequence tasks like translation or summarization. There is also the [`~trl.SFTTrainer`] class from the [TRL](https://hf.co/docs/trl) library which wraps the [`Trainer`] class and is optimized for training language models like Llama-2 and Mistral with autoregressive techniques. [`~trl.SFTTrainer`] also supports features like sequence packing, LoRA, quantization, and DeepSpeed for efficiently scaling to any model size.\n\n<br>",
        "question": "(your factoid question)\n",
        "answer": " (your answer to the factoid question)\n\nNow here is the context.\n\nContext: !--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Trainer\n\nThe [`Trainer`] is a complete training and evaluation loop for PyTorch models implemented in the Transformers library. You only need to pass it the necessary pieces for training (model, tokenizer, dataset, evaluation function, training hyperparameters, etc.), and the [`Trainer`] class takes care of the rest. This makes it easier to start training faster without manually writing your own training loop. But at the same time, [`Trainer`] is very customizable and offers a ton of training options so you can tailor it to your exact training needs.\n\n<Tip>\n\nIn addition to the [`Trainer`] class, Transformers also provides a [`Seq2SeqTrainer`] class for sequence-to-sequence tasks like translation or summarization. There is also the [`~trl.SFTTrainer`] class from the [TRL](https://hf.co/docs/trl) library which wraps the [`Trainer`] class and is optimized for training language models like Llama-2 and Mistral with autoregressive techniques. [`~trl.SFTTrainer`] also supports features like sequence packing, LoRA, quantization, and DeepSpeed for efficiently scaling to any model size.\n\n<br>\n\nOutput:::\n\nWhat is the main class for training and evaluating PyTorch models in the Transformers library?\n\nThe main class for training and evaluating PyTorch models in the Transformers library is the `Trainer` class.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/trainer.md"
    },
    {
        "context": "1. **[Table Transformer](https://huggingface.co/docs/transformers/model_doc/table-transformer)** (Microsoft Research 에서) Brandon Smock, Rohith Pesala, Robin Abraham 의 [PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents](https://arxiv.org/abs/2110.00061) 논문과 함께 발표했습니다.\n1. **[TAPAS](https://huggingface.co/docs/transformers/model_doc/tapas)** (Google AI 에서) Jonathan Herzig, Paweł Krzysztof Nowak, Thomas Müller, Francesco Piccinno and Julian Martin Eisenschlos 의 [TAPAS: Weakly Supervised Table Parsing via Pre-training](https://arxiv.org/abs/2004.02349) 논문과 함께 발표했습니다.\n1. **[TAPEX](https://huggingface.co/docs/transformers/model_doc/tapex)** (Microsoft Research 에서) Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou 의 [TAPEX: Table Pre-training via Learning a Neural SQL Executor](https://arxiv.org/abs/2107.07653) 논문과 함께 발표했습니다.\n1. **[Time Series Transformer](https://huggingface.co/docs/transformers/model_doc/time_series_transformer)** (from HuggingFace).\n1. **[TimeSformer](https://huggingface.co/docs/transformers/model_doc/timesformer)** (Facebook 에서) Gedas Bertasius, Heng Wang, Lorenzo Torresani 의 [Is Space-Time Attention All You Need for Video Understanding?](https://arxiv.org/abs/2102.05095) 논문과 함께 발표했습니다.\n1. **[Trajectory Transformer](https://huggingface.co/docs/transformers/model_doc/trajectory_transformers)** (the University of California at Berkeley 에서) Michael Janner, Qiyang Li, Sergey Levin 의 [Offline Reinforcement Learning as One Big Sequence Modeling Problem](https://arxiv.org/abs/2106.02039) 논문과 함께 발표했습니다.\n1. **[Transformer-XL](https://huggingface.co/docs/transformers/model_doc/transfo-xl)** (Google/CMU 에서) Zihang Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov 의 [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860) 논문과 함께 발표했습니다.",
        "question": "What is the name of the model that was introduced in the TAPAS paper?\n",
        "answer": " TAPAS",
        "source_doc": "huggingface/transformers/blob/main/README_ko.md"
    },
    {
        "context": "This time, by knowing the underlying structure of the operator flows and matrix shapes involved then the framework can plan and reserve the required resources beforehand. \nIn this context, and as it is shown in the chart above, the difference between framework is very small and there is no clear winner between jemalloc and tcmalloc. \nOf course, glibc is still slightly behind as a general-purpose memory allocator, but the margin is less significant than in the eager setup.\nTo sum it up, tuning the memory allocator can provide an interesting item to grab the last milliseconds' improvement at the end of the optimization process, especially if you are already using traced computation graphs.\n\n\n### OpenMP\n\nIn the previous section we talked about the memory management within machine learning software involving mostly CPU-bound workloads. \nSuch software often relies on intermediary frameworks such as PyTorch or TensorFlow for Deep Learning which commonly abstract away all the underlying, highly parallelized, operator implementations. \n\nWriting such highly parallel and optimized algorithms is a real engineering challenge, and it requires a very low-level understanding of all the actual elements coming into play \noperated by the CPU (synchronization, memory cache, cache validity, etc.). \nIn this context, it is very important to be able to leverage primitives to implement such powerful algorithms, reducing the delivery time and computation time by a large margin\ncompared to implementing everything from scratch.",
        "question": "What is the benefit of using primitives to implement highly parallel and optimized algorithms in the context of CPU-bound workloads in machine learning software?\n",
        "answer": " The benefit of using primitives to implement highly parallel and optimized algorithms in the context of CPU-bound workloads in machine learning software is that it reduces the delivery time and computation time by a large margin compared to implementing everything from scratch.",
        "source_doc": "huggingface/blog/blob/main/bert-cpu-scaling-part-2.md"
    },
    {
        "context": "1. Fork the [repository](https://github.com/huggingface/transformers) by\n   clicking on the 'Fork' button on the repository's page. This creates a copy of the code\n   under your GitHub user account.\n\n2. Clone your fork to your local disk, and add the base repository as a remote:\n\n   ```bash\n   $ git clone https://github.com/<your Github handle>/transformers.git\n   $ cd transformers\n   $ git remote add upstream https://github.com/huggingface/transformers.git\n   ```\n\n3. Create a new branch to hold your development changes. This is especially useful to share code changes with your team:\n\n   ```bash\n   $ git checkout -b a-descriptive-name-for-my-project\n   ```\n\n4. Set up a PyTorch environment by running the following command your virtual environment:\n\n   ```bash\n   $ pip install -e \".[torch-speech]\"\n   ```\n\n   (If transformers was already installed in the virtual environment, remove\n   it with `pip uninstall transformers` before reinstalling it in editable\n   mode with the `-e` flag.)\n\n   If you have already cloned that repo, you might need to `git pull` to get the most recent changes in the `transformers`\n   library.\n\n   Running this command will automatically install `torch` and the most relevant \n   libraries required for fine-tuning a speech recognition system.\n\nNext, you should also install the 🤗 Datasets library. We strongly recommend installing the \nlibrary from source to profit from the most current additions during the community week.\n\nSimply run the following steps:\n\n```\n$ cd ~/\n$ git clone https://github.com/huggingface/datasets.git\n$ cd datasets\n$ pip install -e \".[streaming]\"\n```\n\nIf you plan on contributing a specific dataset during \nthe community week, please fork the datasets repository and follow the instructions \n[here](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-create-a-pull-request).",
        "question": "How do I install the 🤗 Datasets library?\n",
        "answer": " To install the 🤗 Datasets library, you should first clone the repository to your local disk and add the base repository as a remote. Then, create a new branch to hold your development changes and set up a PyTorch environment. After that, install the library from source by running `pip install -e \".[streaming]\"` in the datasets directory.",
        "source_doc": "huggingface/transformers/blob/main/examples/research_projects/robust-speech-event/README.md"
    },
    {
        "context": "Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_mobilenetv3_large_minimal_100-8596ae28.pth\n  Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 72.24%\n      Top 5 Accuracy: 90.64%\n- Name: tf_mobilenetv3_small_075\n  In Collection: TF MobileNet V3\n  Metadata:\n    FLOPs: 48457664\n    Parameters: 2040000\n    File Size: 8242701\n    Architecture:\n    - 1x1 Convolution\n    - Batch Normalization\n    - Convolution\n    - Dense Connections\n    - Depthwise Separable Convolution\n    - Dropout\n    - Global Average Pooling\n    - Hard Swish\n    - Inverted Residual Block\n    - ReLU\n    - Residual Connection\n    - Softmax\n    - Squeeze-and-Excitation Block\n    Tasks:\n    - Image Classification\n    Training Techniques:\n    - RMSProp\n    - Weight Decay\n    Training Data:\n    - ImageNet\n    Training Resources: 16x GPUs\n    ID: tf_mobilenetv3_small_075\n    LR: 0.045\n    Crop Pct: '0.875'\n    Momentum: 0.9\n    Batch Size: 4096\n    Image Size: '224'\n    Weight Decay: 4.0e-05\n    Interpolation: bilinear\n    RMSProp Decay: 0.9\n  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/mobilenetv3.py#L421\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_mobilenetv3_small_075-da427f52.pth\n  Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 65.72%\n      Top 5 Accuracy: 86.13%\n- Name: tf_mobilenetv3_small_100\n  In Collection: TF MobileNet V3\n  Metadata:\n    FLOPs: 65450600\n    Parameters: 2540000\n    File Size: 10256398\n    Architecture:\n    - 1x1 Convolution\n    - Batch Normalization\n    - Convolution\n    - Dense Connections\n    - Depthwise Separable Convolution\n    - Dropout\n    - Global Average Pooling\n    - Hard Swish\n    - Inverted Residual Block\n    - ReLU\n    - Residual Connection\n    - Softmax\n    - Squeeze-and-Excitation Block\n    Tasks:",
        "question": "What is the FLOPs of tf_mobilenetv3_small_100?\n",
        "answer": " The FLOPs of tf_mobilenetv3_small_100 is 65450600.",
        "source_doc": "huggingface/pytorch-image-models/blob/main/docs/models/tf-mobilenet-v3.md"
    },
    {
        "context": "1. **[Bark](https://huggingface.co/docs/transformers/model_doc/bark)** (from Suno) released in the repository [suno-ai/bark](https://github.com/suno-ai/bark) by Suno AI team.\n1. **[BART](https://huggingface.co/docs/transformers/model_doc/bart)** (from Facebook) released with the paper [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/pdf/1910.13461.pdf) by Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov and Luke Zettlemoyer.\n1. **[BARThez](https://huggingface.co/docs/transformers/model_doc/barthez)** (from École polytechnique) released with the paper [BARThez: a Skilled Pretrained French Sequence-to-Sequence Model](https://arxiv.org/abs/2010.12321) by Moussa Kamal Eddine, Antoine J.-P. Tixier, Michalis Vazirgiannis.\n1. **[BARTpho](https://huggingface.co/docs/transformers/model_doc/bartpho)** (from VinAI Research) released with the paper [BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese](https://arxiv.org/abs/2109.09701) by Nguyen Luong Tran, Duong Minh Le and Dat Quoc Nguyen.\n1. **[BEiT](https://huggingface.co/docs/transformers/model_doc/beit)** (from Microsoft) released with the paper [BEiT: BERT Pre-Training of Image Transformers](https://arxiv.org/abs/2106.08254) by Hangbo Bao, Li Dong, Furu Wei.\n1. **[BERT](https://huggingface.co/docs/transformers/model_doc/bert)** (from Google) released with the paper [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.\n1. **[BERT For Sequence Generation](https://huggingface.co/docs/transformers/model_doc/bert-generation)** (from Google) released with the paper [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461) by Sascha Rothe, Shashi Narayan, Aliaksei Severyn.",
        "question": "Which model was released by Microsoft with the paper BEiT: BERT Pre-training of Image Transformers?\n",
        "answer": " BEiT",
        "source_doc": "huggingface/transformers/blob/main/README_ko.md"
    },
    {
        "context": "##### Improved markdown support\n\nWe now have better support for markdown in `gr.Markdown` and `gr.Dataframe`. Including syntax highlighting and Github Flavoured Markdown. We also have more consistent markdown behaviour and styling.\n\n##### Various performance improvements\n\nThese improvements will be particularly beneficial to large applications.\n\n- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.\n- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).\n- Corrected an issue that was causing markdown to re-render infinitely.\n- Ensured that the `gr.3DModel` does re-render prematurely.\n\nThanks [@pngwn](https://github.com/pngwn)!\n\n### Features\n\n- [#5215](https://github.com/gradio-app/gradio/pull/5215) [`fbdad78a`](https://github.com/gradio-app/gradio/commit/fbdad78af4c47454cbb570f88cc14bf4479bbceb) - Lazy load interactive or static variants of a component individually, rather than loading both variants regardless. This change will improve performance for many applications. Thanks [@pngwn](https://github.com/pngwn)!\n- [#5112](https://github.com/gradio-app/gradio/pull/5112) [`1cefee7f`](https://github.com/gradio-app/gradio/commit/1cefee7fc05175aca23ba04b3a3fda7b97f49bf0) - chore(deps): update dependency marked to v7. Thanks [@renovate](https://github.com/apps/renovate)!\n- [#5258](https://github.com/gradio-app/gradio/pull/5258) [`92282cea`](https://github.com/gradio-app/gradio/commit/92282cea6afdf7e9930ece1046d8a63be34b3cea) - Chatbot Avatar Images. Thanks [@dawoodkhan82](https://github.com/dawoodkhan82)!\n\n### Fixes",
        "question": "What is the new feature that allows lazy loading of interactive or static variants of a component?\n",
        "answer": " The new feature that allows lazy loading of interactive or static variants of a component is `#5215`.",
        "source_doc": "gradio-app/gradio/blob/main/js/chatbot/CHANGELOG.md"
    },
    {
        "context": "## ConvBertConfig\n\n[[autodoc]] ConvBertConfig\n\n## ConvBertTokenizer\n\n[[autodoc]] ConvBertTokenizer\n    - build_inputs_with_special_tokens\n    - get_special_tokens_mask\n    - create_token_type_ids_from_sequences\n    - save_vocabulary\n\n## ConvBertTokenizerFast\n\n[[autodoc]] ConvBertTokenizerFast\n\n<frameworkcontent>\n<pt>\n\n## ConvBertModel\n\n[[autodoc]] ConvBertModel\n    - forward\n\n## ConvBertForMaskedLM\n\n[[autodoc]] ConvBertForMaskedLM\n    - forward\n\n## ConvBertForSequenceClassification\n\n[[autodoc]] ConvBertForSequenceClassification\n    - forward\n\n## ConvBertForMultipleChoice\n\n[[autodoc]] ConvBertForMultipleChoice\n    - forward\n\n## ConvBertForTokenClassification\n\n[[autodoc]] ConvBertForTokenClassification\n    - forward\n\n## ConvBertForQuestionAnswering\n\n[[autodoc]] ConvBertForQuestionAnswering\n    - forward\n\n</pt>\n<tf>\n\n## TFConvBertModel\n\n[[autodoc]] TFConvBertModel\n    - call\n\n## TFConvBertForMaskedLM\n\n[[autodoc]] TFConvBertForMaskedLM\n    - call\n\n## TFConvBertForSequenceClassification\n\n[[autodoc]] TFConvBertForSequenceClassification\n    - call\n\n## TFConvBertForMultipleChoice\n\n[[autodoc]] TFConvBertForMultipleChoice\n    - call\n\n## TFConvBertForTokenClassification\n\n[[autodoc]] TFConvBertForTokenClassification\n    - call\n\n## TFConvBertForQuestionAnswering\n\n[[autodoc]] TFConvBertForQuestionAnswering\n    - call\n\n</tf>\n</frameworkcontent>",
        "question": "What is the name of the class for the ConvBert model in TensorFlow?\n",
        "answer": " The name of the class for the ConvBert model in TensorFlow is TFConvBertModel.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/convbert.md"
    },
    {
        "context": "## Citación\n\nAhora nosotros tenemos un [papel](https://www.aclweb.org/anthology/2020.emnlp-demos.6/) que puedes citar para la librería de  🤗 Transformers:\n```bibtex\n@inproceedings{wolf-etal-2020-transformers,\n    title = \"Transformers: State-of-the-Art Natural Language Processing\",\n    author = \"Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush\",\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\",\n    month = oct,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.emnlp-demos.6\",\n    pages = \"38--45\"\n}\n```",
        "question": "Who are the authors of the Transformers paper?\n",
        "answer": " Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush.",
        "source_doc": "huggingface/transformers/blob/main/README_es.md"
    },
    {
        "context": "he tokenizer pipeline. In this video, we'll look at how a tokenizer converts raw text to numbers that a Transformer model can make sense of, like when we execute this code. Here is a quick overview of what happens inside the tokenizer object: first the text is split into tokens, which are words, parts of words, or punctuation symbols. Then the tokenizer adds potential special tokens and converts each token to their unique respective ID as defined by the tokenizer's vocabulary. As we'll see it doesn't actually happen in this order, but viewing it like this is better for understanding what happens. The first step is to split our input text into tokens with the tokenize method. To do this, the tokenizer may first perform some operations like lowercasing all words, then follow a set of rules to split the result in small chunks of text. Most of the Transformers models use a subword tokenization algorithm, which means that one given word can be split in several tokens, like tokenize here. Look at the \"Tokenization algorithms\" videos linked below for more information! The ## prefix we see in front of ize is the convention used by BERT to indicate this token is not the beginning of a word. Other tokenizers may use different conventions however: for instance ALBERT tokenizers will add a long underscore in front of all the tokens that had a space before them, which is a convention used by sentencepiece tokenizers. The second step of the tokenization pipeline is to map those tokens to their respective IDs as defined by the vocabulary of the tokenizer.  This is why we need to download a file when we instantiate a tokenizer with the from_pretrained method: we have to make sure we use the same mapping as when the model was pretrained. To do this, we use the convert_tokens_to_ids method. You may have noticed that we don't have the exact same result as in our first slide — or not, as this looks like a list of random numbers, in which case allow me to refresh your memory",
        "question": "What is the convention used by BERT tokenizers to indicate a token is not the beginning of a word?\n",
        "answer": " The convention used by BERT tokenizers to indicate a token is not the beginning of a word is the ## prefix.",
        "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter2/04e_tokenizer-pipeline.md"
    },
    {
        "context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# LoRA\n\nLoRA is a fast and lightweight training method that inserts and trains a significantly smaller number of parameters instead of all the model parameters. This produces a smaller file (~100 MBs) and makes it easier to quickly train a model to learn a new concept. LoRA weights are typically loaded into the UNet, text encoder or both. There are two classes for loading LoRA weights:\n\n- [`LoraLoaderMixin`] provides functions for loading and unloading, fusing and unfusing, enabling and disabling, and more functions for managing LoRA weights. This class can be used with any model.\n- [`StableDiffusionXLLoraLoaderMixin`] is a [Stable Diffusion (SDXL)](../../api/pipelines/stable_diffusion/stable_diffusion_xl) version of the [`LoraLoaderMixin`] class for loading and saving LoRA weights. It can only be used with the SDXL model.\n\n<Tip>\n\nTo learn more about how to load LoRA weights, see the [LoRA](../../using-diffusers/loading_adapters#lora) loading guide.\n\n</Tip>\n\n## LoraLoaderMixin\n\n[[autodoc]] loaders.lora.LoraLoaderMixin\n\n## StableDiffusionXLLoraLoaderMixin\n\n[[autodoc]] loaders.lora.StableDiffusionXLLoraLoaderMixin",
        "question": "What is LoRA?\n",
        "answer": " LoRA is a fast and lightweight training method that inserts and trains a significantly smaller number of parameters instead of all the model parameters. This produces a smaller file (~100 MBs) and makes it easier to quickly train a model to learn a new concept. LoRA weights are typically loaded into the UNet, text encoder or both. There are two classes for loading LoRA weights: `LoraLoaderMixin` and `StableDiffusionXLLoraLoaderMixin`. The `LoraLoaderMixin` class can be used with any model, while the `StableDiffusionXLLoraLoaderMixin`] class can only be used with the SDXL model.",
        "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/loaders/lora.md"
    },
    {
        "context": "config = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n```\n\nTo get the model predictions:\n```python\nimport torch\nwith torch.no_grad():\n    out = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n```\n\nTo get the top-5 predictions class names:\n```python\n# Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n```\n\nReplace the model name with the variant you want to use, e.g. `efficientnet_b1_pruned`. You can find the IDs in the model summaries at the top of this page.\n\nTo extract image features with this model, follow the [timm feature extraction examples](https://rwightman.github.io/pytorch-image-models/feature_extraction/), just change the name of the model you want to use.",
        "question": "(your factoid question)\n",
        "answer": "\n\nThe available model variants for the imagenet-1k pretrained model are:\n\n* resnet18\n* resnet34\n* resnet50\n* resnet101\n* resnet152\n* resnext50\\_32x4d\n* resnext101\\_32x8d\n* wide\\_resnet50\\_2\n* wide\\_resnet101\\_2\n* efficientnet\\_b0\n* efficientnet\\_b1\n* efficientnet\\_b1\\_pruned\n* efficientnet\\_b2\\_pruned\n* efficientnet\\_b3\\_pruned\n* efficientnet\\_b4\\_pruned\n* efficientnet\\_b5\\_pruned\n* efficientnet\\_b6\\_pruned\n* efficientnet\\_b7\\_pruned\n* vit\\_base\\_patch16\\_224\n* vit\\_large\\_patch16\\_224\n* vit\\_huge\\_patch14\\_224\n* convnext\\_base\n* convnext\\_large\n* convnext\\_xlarge\n* beit\\_base\\_patch16\\_224\n* beit\\_large\\_patch16\\_224\n* beit\\_large\\_patch16\\_384\n* beit\\_base\\_patch32\\_224\n* beit\\_base\\_patch32\\_384\n* beit\\_large\\_patch32\\_384\n\nNote: The pruned models are smaller and faster, but have slightly lower accuracy.",
        "source_doc": "huggingface/pytorch-image-models/blob/main/docs/models/efficientnet-pruned.md"
    },
    {
        "context": "This paper has been accepted to the [AAAI'23](https://aaai.org/Conferences/AAAI-23/) conference. \n\nThe abstract from the paper is the following:\n\n*Vision-Language (VL) models with the TWO-TOWER architecture have dominated visual-language representation learning in recent years.\nCurrent VL models either use lightweight uni-modal encoders and learn to extract, align and fuse both modalities simultaneously in a deep cross-modal encoder, or feed the last-layer uni-modal representations from the deep pre-trained uni-modal encoders into the top cross-modal encoder.\nBoth approaches potentially restrict vision-language representation learning and limit model performance. In this paper, we propose BRIDGETOWER, which introduces multiple bridge layers that build a connection between the top layers of uni-modal encoders and each layer of the crossmodal encoder.\nThis enables effective bottom-up cross-modal alignment and fusion between visual and textual representations of different semantic levels of pre-trained uni-modal encoders in the cross-modal encoder. Pre-trained with only 4M images, BRIDGETOWER achieves state-of-the-art performance on various downstream vision-language tasks.\nIn particular, on the VQAv2 test-std set, BRIDGETOWER achieves an accuracy of 78.73%, outperforming the previous state-of-the-art model METER by 1.09% with the same pre-training data and almost negligible additional parameters and computational costs.\nNotably, when further scaling the model, BRIDGETOWER achieves an accuracy of 81.15%, surpassing models that are pre-trained on orders-of-magnitude larger datasets.*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/bridgetower_architecture%20.jpg\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> BridgeTower architecture. Taken from the <a href=\"https://arxiv.org/abs/2206.08657\">original paper.</a> </small>",
        "question": "(your factoid question)\n",
        "answer": " (your answer to the factoid question)\n\nNow here is the context.\n\nContext: This paper has been accepted to the [AAAI'23](https://aaai.org/Conferences/AAAI-23/) conference. \n\nThe abstract from the paper is the following:\n\n*Vision-Language (VL) models with the TWO-TOWER architecture have dominated visual-language representation learning in recent years.\nCurrent VL models either use lightweight uni-modal encoders and learn to extract, align and fuse both modalities simultaneously in a deep cross-modal encoder, or feed the last-layer uni-modal representations from the deep pre-trained uni-modal encoders into the top cross-modal encoder.\nBoth approaches potentially restrict vision-language representation learning and limit model performance. In this paper, we propose BRIDGETOWER, which introduces multiple bridge layers that build a connection between the top layers of uni-modal encoders and each layer of the crossmodal encoder.\nThis enables effective bottom-up cross-modal alignment and fusion between visual and textual representations of different semantic levels of pre-trained uni-modal encoders in the cross-modal encoder. Pre-trained with only 4M images, BRIDGETOWER achieves state-of-the-art performance on various downstream vision-language tasks.\nIn particular, on the VQAv2 test-std set, BRIDGETOWER achieves an accuracy of 78.73%, outperforming the previous state-of-the-art model METER by 1.09% with the same pre-training data and almost negligible additional parameters and computational costs.\nNotably, when further scaling the model, BRIDGETOWER achieves an accuracy of 81.15%, surpassing models that are pre-trained on orders-of-magnitude larger datasets.*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/bridgetower_architecture%20.jpg\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> BridgeTower architecture. Taken from the <a href=\"https://arxiv.org/abs/2206.08657\">original paper.</a> </small>\n\nOutput:::\n\nQ: What conference was the paper \"BRIDGE-TOWER: Bottom-Up Cross-Modal Alignment for Vision-Language Representation Learning\" accepted to?\nA: The paper was accepted to the AAAI'23 conference.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/bridgetower.md"
    },
    {
        "context": "```python\nfrom functools import partial\n\ntrain_dataset.set_transform(partial(transform_start_field, freq=freq))\ntest_dataset.set_transform(partial(transform_start_field, freq=freq))\n```\n\n## Define the Model\n\nNext, let's instantiate a model. The model will be trained from scratch, hence we won't use the `from_pretrained` method here, but rather randomly initialize the model from a [`config`](https://huggingface.co/docs/transformers/model_doc/time_series_transformer#transformers.TimeSeriesTransformerConfig).\n\nWe specify a couple of additional parameters to the model:\n- `prediction_length` (in our case, `24` months): this is the horizon that the decoder of the Transformer will learn to predict for;\n- `context_length`: the model will set the `context_length` (input of the encoder) equal to the `prediction_length`, if no `context_length` is specified;\n- `lags` for a given frequency: these specify how much we \"look back\", to be added as additional features. e.g. for a `Daily` frequency we might consider a look back of `[1, 2, 7, 30, ...]` or in other words look back 1, 2, ... days while for `Minute` data we might consider `[1, 30, 60, 60*24, ...]` etc.;\n- the number of time features: in our case, this will be `2` as we'll add `MonthOfYear` and `Age` features;\n- the number of static categorical features: in our case, this will be just `1` as we'll add a single \"time series ID\" feature;\n- the cardinality: the number of values of each static categorical feature, as a list which for our case will be `[366]` as we have 366 different time series\n- the embedding dimension: the embedding dimension for each static categorical feature, as a list, for example `[3]` means the model will learn an embedding vector of size `3` for each of the `366` time series (regions).\n\n\nLet's use the default lags provided by GluonTS for the given frequency (\"monthly\"):\n\n\n```python\nfrom gluonts.time_feature import get_lags_for_frequency",
        "question": "(your factoid question)\n",
        "answer": " (your answer to the factoid question)\n\nNow here is the context.\n\nContext: ```python\nfrom functools import partial\n\ntrain_dataset.set_transform(partial(transform_start_field, freq=freq))\ntest_dataset.set_transform(partial(transform_start_field, freq=freq))\n```\n\n## Define the Model\n\nNext, let's instantiate a model. The model will be trained from scratch, hence we won't use the `from_pretrained` method here, but rather randomly initialize the model from a [`config`](https://huggingface.co/docs/transformers/model_doc/time_series_transformer#transformers.TimeSeriesTransformerConfig).\n\nWe specify a couple of additional parameters to the model:\n- `prediction_length` (in our case, `24` months): this is the horizon that the decoder of the Transformer will learn to predict for;\n- `context_length`: the model will set the `context_length` (input of the encoder) equal to the `prediction_length`, if no `context_length` is specified;\n- `lags` for a given frequency: these specify how much we \"look back\", to be added as additional features. e.g. for a `Daily` frequency we might consider a look back of `[1, 2, 7, 30, ...]` or in other words look back 1, 2, ... days while for `Minute` data we might consider `[1, 30, 60, 60*24, ...]` etc.;\n- the number of time features: in our case, this will be `2` as we'll add `MonthOfYear` and `Age` features;\n- the number of static categorical features: in our case, this will be just `1` as we'll add a single \"time series ID\" feature;\n- the cardinality: the number of values of each static categorical feature, as a list which for our case will be `[366]` as we have 366 different time series\n- the embedding dimension: the embedding dimension for each static categorical feature, as a list, for example `[3]` means the model will learn an embedding vector of size `3` for each of the `366` time series (regions).\n\n\nLet's use the default lags provided by GluonTS for the given frequency (\"monthly\"):\n\n\n```python\nfrom gluonts.time_feature import get_lags_for_frequency\n\nOutput:::\n[12, 24, 36, 48, 60, 72, 84, 96, 108, 120, 132, 144, 156, 168, 180, 192, 204, 216, 228, 240, 252, 264, 288, 312, 336, 360, 384, 408, 432, 456, 480, 504, 528, 552, 576, 600, 624, 648, 672, 696, 720, 744, 768, 792, 816, 840, 864, 888, 912, 936, 960, 984, 1008, 1032, 1056, 1080, 1104, 11128, 1152, 1176, 1200, 1224, 1248, 1272, 1296, 1320, 1344, 1368, 1392, 1416, 1440, 1464, 1488, 1512, 1536, 1560, 1",
        "source_doc": "huggingface/blog/blob/main/time-series-transformers.md"
    },
    {
        "context": "| efficientnet_es | 78.066 (21.934) | 93.926 (6.074) | 5.44 | bicubic | 224 |\n| seresnext26t_32x4d | 77.998 (22.002) | 93.708 (6.292) | 16.8 | bicubic | 224 |\n| seresnext26tn_32x4d | 77.986 (22.014) | 93.746 (6.254) | 16.8 | bicubic | 224 |\n| efficientnet_b0 | 77.698 (22.302) | 93.532 (6.468) | 5.29 | bicubic | 224 |\n| seresnext26d_32x4d | 77.602 (22.398) | 93.608 (6.392) | 16.8 | bicubic | 224 |\n| mobilenetv2_120d | 77.294 (22.706 | 93.502 (6.498) | 5.8 | bicubic | 224 |\n| mixnet_m | 77.256 (22.744) | 93.418 (6.582) | 5.01 | bicubic | 224 |\n| resnet34d | 77.116 (22.884) | 93.382 (6.618) | 21.8 | bicubic | 224 |\n| seresnext26_32x4d | 77.104 (22.896) | 93.316 (6.684) | 16.8 | bicubic | 224 |\n| skresnet34 | 76.912 (23.088) | 93.322 (6.678) | 22.2 | bicubic | 224 |\n| ese_vovnet19b_dw | 76.798 (23.202) | 93.268 (6.732) | 6.5 | bicubic | 224 |\n| resnet26d | 76.68 (23.32) | 93.166 (6.834) | 16 | bicubic | 224 |\n| densenetblur121d | 76.576 (23.424) | 93.190 (6.810) | 8.0 | bicubic | 224 |\n| mobilenetv2_140 | 76.524 (23.476) | 92.990 (7.010) | 6.1 | bicubic | 224 |\n| mixnet_s | 75.988 (24.012) | 92.794 (7.206) | 4.13 | bicubic | 224 |\n| mobilenetv3_large_100 | 75.766 (24.234) | 92.542 (7.458) | 5.5 | bicubic | 224 |\n| mobilenetv3_rw | 75.634 (24.366) | 92.708 (7.292) | 5.5 | bicubic | 224 |\n| mnasnet_a1 | 75.448 (24.552) | 92.604 (7.396) | 3.89 | bicubic | 224 |\n| resnet26 | 75.292 (24.708) | 92.57 (7.43) | 16 | bicubic | 224 |\n| fbnetc_100 | 75.124 (24.876) | 92.386 (7.614) | 5.6 | bilinear | 224 |\n| resnet34 | 75.110 (24.890) | 92.284 (7.716) | 22 | bilinear | 224 |\n| mobilenetv2_110d | 75.052 (24.948) | 92.180 (7.820) | 4.5 | bicubic | 224 |\n| seresnet34 | 74.808 (25.192) | 92.124 (7.876) | 22 | bilinear | 224 |\n| mnasnet_b1 | 74.658 (25.342) | 92.114 (7.886) | 4.38 | bicubic | 224 |\n| spnasnet_100 | 74.084 (25.916)  | 91.818 (8.182) | 4.42 | bilinear | 224 |\n| skresnet18 | 73.038 (26.962) | 91.168 (8.832) | 11.9 | bicubic | 224 |",
        "question": "(your factoid question)\n",
        "answer": " The top-1 accuracy of efficientnet_b0 is 77.698.",
        "source_doc": "huggingface/pytorch-image-models/blob/main/hfdocs/source/results.mdx"
    },
    {
        "context": "Gradio Demo: leaderboard\n### A simple dashboard ranking spaces by number of likes.\n        \n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\nimport requests\nimport pandas as pd\nfrom huggingface_hub.hf_api import SpaceInfo\npath = f\"https://huggingface.co/api/spaces\"\n\n\ndef get_blocks_party_spaces():\n    r = requests.get(path)\n    d = r.json()\n    spaces = [SpaceInfo(**x) for x in d]\n    blocks_spaces = {}\n    for i in range(0,len(spaces)):\n        if spaces[i].id.split('/')[0] == 'Gradio-Blocks' and hasattr(spaces[i], 'likes') and spaces[i].id != 'Gradio-Blocks/Leaderboard' and spaces[i].id != 'Gradio-Blocks/README':\n            blocks_spaces[spaces[i].id]=spaces[i].likes\n    df = pd.DataFrame(\n    [{\"Spaces_Name\": Spaces, \"likes\": likes} for Spaces,likes in blocks_spaces.items()])\n    df = df.sort_values(by=['likes'],ascending=False)\n    return df\n\nblock = gr.Blocks()\n\nwith block:    \n    gr.Markdown(\"\"\"Leaderboard for the most popular Blocks Event Spaces. To learn more and join, see <a href=\"https://huggingface.co/Gradio-Blocks\" target=\"_blank\" style=\"text-decoration: underline\">Blocks Party Event</a>\"\"\")\n    with gr.Tabs():\n        with gr.TabItem(\"Blocks Party Leaderboard\"):\n            with gr.Row():\n                data = gr.Dataframe(type=\"pandas\")\n            with gr.Row():\n                data_run = gr.Button(\"Refresh\")\n                data_run.click(get_blocks_party_spaces, inputs=None, outputs=data)\n    # running the function on page load in addition to when the button is clicked\n    block.load(get_blocks_party_spaces, inputs=None, outputs=data)               \n\nblock.launch()\n\n\n```",
        "question": "(your factoid question)\n",
        "answer": " (your answer to the factoid question)\n\nNow here is the context.\n\nContext: Gradio Demo: leaderboard\n### A simple dashboard ranking spaces by number of likes.\n        \n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\nimport requests\nimport pandas as pd\nfrom huggingface_hub.hf_api import SpaceInfo\npath = f\"https://huggingface.co/api/spaces\"\n\n\ndef get_blocks_party_spaces():\n    r = requests.get(path)\n    d = r.json()\n    spaces = [SpaceInfo(**x) for x in d]\n    blocks_spaces = {}\n    for i in range(0,len(spaces)):\n        if spaces[i].id.split('/')[0] == 'Gradio-Blocks' and hasattr(spaces[i], 'likes') and spaces[i].id != 'Gradio-Blocks/Leaderboard' and spaces[i].id != 'Gradio-Blocks/README':\n            blocks_spaces[spaces[i].id]=spaces[i].likes\n    df = pd.DataFrame(\n    [{\"Spaces_Name\": Spaces, \"likes\": likes} for Spaces,likes in blocks_spaces.items()])\n    df = df.sort_values(by=['likes'],ascending=False)\n    return df\n\nblock = gr.Blocks()\n\nwith block:    \n    gr.Markdown(\"\"\"Leaderboard for the most popular Blocks Event Spaces. To learn more and join, see <a href=\"https://huggingface.co/Gradio-Blocks\" target=\"_blank\" style=\"text-decoration: underline\">Blocks Party Event</a>\"\"\")\n    with gr.Tabs():\n        with gr.TabItem(\"Blocks Party Leaderboard\"):\n            with gr.Row():\n                data = gr.Dataframe(type=\"pandas\")\n            with gr.Row():\n                data_run = gr.Button(\"Refresh\")\n                data_run.click(get_blocks_party_spaces, inputs=None, outputs=data)\n    # running the function on page load in addition to when the button is clicked\n    block.load(get_blocks_party_spaces, inputs=None, outputs=data)               \n\nblock.launch()\n\n\n```\n\nOutput:::\n\n\n\n```\n\n\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
        "source_doc": "gradio-app/gradio/blob/main/demo/leaderboard/run.ipynb"
    },
    {
        "context": "+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N/A  N/A      3721      C   ...nvs/codeparrot/bin/python     2629MiB |\n+-----------------------------------------------------------------------------+\n```\n\nWe get the same number as before and you can also see that we are using a V100 GPU with 16GB of memory. So now we can \nstart training the model and see how the GPU memory consumption changes. First, we set up a few standard training \narguments:\n\n```py\ndefault_args = {\n    \"output_dir\": \"tmp\",\n    \"evaluation_strategy\": \"steps\",\n    \"num_train_epochs\": 1,\n    \"log_level\": \"error\",\n    \"report_to\": \"none\",\n}\n```\n\n<Tip>\n\n If you plan to run multiple experiments, in order to properly clear the memory between experiments, restart the Python \n kernel between experiments.\n\n</Tip>\n\n## Memory utilization at vanilla training\n\nLet's use the [`Trainer`] and train the model without using any GPU performance optimization techniques and a batch size of 4:\n\n```py\n>>> from transformers import TrainingArguments, Trainer, logging\n\n>>> logging.set_verbosity_error()\n\n\n>>> training_args = TrainingArguments(per_device_train_batch_size=4, **default_args)\n>>> trainer = Trainer(model=model, args=training_args, train_dataset=ds)\n>>> result = trainer.train()\n>>> print_summary(result)\n```\n\n```\nTime: 57.82\nSamples/second: 8.86\nGPU memory occupied: 14949 MB.\n```",
        "question": "(your factoid question)\n",
        "answer": " (your answer to the factoid question)\n\nNow here is the context.\n\nContext: +-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N/A  N/A      3721      C   ...nvs/codeparrot/bin/python     2629MiB |\n+-----------------------------------------------------------------------------+\n```\n\nWe get the same number as before and you can also see that we are using a V100 GPU with 16GB of memory. So now we can \nstart training the model and see how the GPU memory consumption changes. First, we set up a few standard training \narguments:\n\n```py\ndefault_args = {\n    \"output_dir\": \"tmp\",\n    \"evaluation_strategy\": \"steps\",\n    \"num_train_epochs\": 1,\n    \"log_level\": \"error\",\n    \"report_to\": \"none\",\n}\n```\n\n<Tip>\n\n If you plan to run multiple experiments, in order to properly clear the memory between experiments, restart the Python \n kernel between experiments.\n\n</Tip>\n\n## Memory utilization at vanilla training\n\nLet's use the [`Trainer`] and train the model without using any GPU performance optimization techniques and a batch size of 4:\n\n```py\n>>> from transformers import TrainingArguments, Trainer, logging\n\n>>> logging.set_verbosity_error()\n\n\n>>> training_args = TrainingArguments(per_device_train_batch_size=4, **default_args)\n>>> trainer = Trainer(model=model, args=training_args, train_dataset=ds)\n>>> result = trainer.train()\n>>> print_summary(result)\n```\n\n```\nTime: 57.82\nSamples/second: 8.86\nGPU memory occupied: 14949 MB.\n```\n\nOutput:::\n\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_memory_anatomy.md"
    },
    {
        "context": "Then create a function that passes your predictions and labels to [`~evaluate.EvaluationModule.compute`] to calculate the accuracy:\n\n```py\n>>> import numpy as np\n\n\n>>> def compute_metrics(eval_pred):\n...     predictions, labels = eval_pred\n...     predictions = np.argmax(predictions, axis=1)\n...     return accuracy.compute(predictions=predictions, references=labels)\n```\n\nYour `compute_metrics` function is ready to go now, and you'll return to it when you setup your training.\n\n## Train\n\n<frameworkcontent>\n<pt>\n<Tip>\n\nIf you aren't familiar with finetuning a model with the [`Trainer`], take a look at the basic tutorial [here](../training#train-with-pytorch-trainer)!\n\n</Tip>\n\nYou're ready to start training your model now! Load BERT with [`AutoModelForMultipleChoice`]:\n\n```py\n>>> from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n\n>>> model = AutoModelForMultipleChoice.from_pretrained(\"bert-base-uncased\")\n```\n\nAt this point, only three steps remain:\n\n1. Define your training hyperparameters in [`TrainingArguments`]. The only required parameter is `output_dir` which specifies where to save your model. You'll push this model to the Hub by setting `push_to_hub=True` (you need to be signed in to Hugging Face to upload your model). At the end of each epoch, the [`Trainer`] will evaluate the accuracy and save the training checkpoint.\n2. Pass the training arguments to [`Trainer`] along with the model, dataset, tokenizer, data collator, and `compute_metrics` function.\n3. Call [`~Trainer.train`] to finetune your model.\n\n```py\n>>> training_args = TrainingArguments(\n...     output_dir=\"my_awesome_swag_model\",\n...     evaluation_strategy=\"epoch\",\n...     save_strategy=\"epoch\",\n...     load_best_model_at_end=True,\n...     learning_rate=5e-5,\n...     per_device_train_batch_size=16,\n...     per_device_eval_batch_size=16,\n...     num_train_epochs=3,\n...     weight_decay=0.01,\n...     push_to_hub=True,\n... )",
        "question": "(your factoid question)\n",
        "answer": " (your answer to the factoid question)\n\nNow here is the context.\n\nContext: Then create a function that passes your predictions and labels to [`~evaluate.EvaluationModule.compute`] to calculate the accuracy:\n\n```py\n>>> import numpy as np\n\n\n>>> def compute_metrics(eval_pred):\n...     predictions, labels = eval_pred\n...     predictions = np.argmax(predictions, axis=1)\n...     return accuracy.compute(predictions=predictions, references=labels)\n```\n\nYour `compute_metrics` function is ready to go now, and you'll return to it when you setup your training.\n\n## Train\n\n<frameworkcontent>\n<pt>\n<Tip>\n\nIf you aren't familiar with finetuning a model with the [`Trainer`], take a look at the basic tutorial [here](../training#train-with-pytorch-trainer)!\n\n</Tip>\n\nYou're ready to start training your model now! Load BERT with [`AutoModelForMultipleChoice`]:\n\n```py\n>>> from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n\n>>> model = AutoModelForMultipleChoice.from_pretrained(\"bert-base-uncased\")\n```\n\nAt this point, only three steps remain:\n\n1. Define your training hyperparameters in [`TrainingArguments`]. The only required parameter is `output_dir` which specifies where to save your model. You'll push this model to the Hub by setting `push_to_hub=True` (you need to be signed in to Hugging Face to upload your model). At the end of each epoch, the [`Trainer`] will evaluate the accuracy and save the training checkpoint.\n2. Pass the training arguments to [`Trainer`] along with the model, dataset, tokenizer, data collator, and `compute_metrics` function.\n3. Call [`~Trainer.train`] to finetune your model.\n\n```py\n>>> training_args = TrainingArguments(\n...     output_dir=\"my_awesome_swag_model\",\n...     evaluation_strategy=\"epoch\",\n...     save_strategy=\"epoch\",\n...     load_best_model_at_end=True,\n...     learning_rate=5e-5,\n...     per_device_train_batch_size=16,\n...     per_device_eval_batch_size=16,\n...     num_train_epochs=3,\n...     weight_decay=0.01,\n...     push_to_hub=True,\n... )\n\nOutput:::\nTrainingArguments(\n    output_dir='my_awesome_swag_model',\n    evaluation_strategy='epoch',\n    save_strategy='epoch',\n    load_best_model_at_end=True,\n    learning_rate=5e-05,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    push_to_hub=True\n)\n\n>>> trainer = Trainer(\n...     model=model,\n...     args=training_args,\n...     train_dataset=train_dataset,\n...     eval_dataset=test_dataset,\n...     tokenizer=tokenizer,\n...     data_collator=data_collator,\n...     compute_metrics=compute_metrics,\n... )\n\nOutput:::\nTrainer(\n    model=BertForSequenceClassification(\n        [\n            'bert.embeddings',\n            'bert.encoder.layer.0',\n            'bert.encoder.layer.1',\n            'bert.encoder.layer.2',\n            'bert.encoder.layer.3',\n            'bert.encoder.layer.4',\n            'bert.encoder.layer.5',\n            'bert.encoder.layer.6',\n            'bert.encoder.layer.7',\n            'bert.encoder.layer.8',\n            'bert.encoder.layer.9',\n            'bert.encoder.layer.10',\n            'bert.encoder.layer.11',\n            'bert.pooler',\n            'classifier',\n        ],\n        num_labels=2",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/tasks/multiple_choice.md"
    },
    {
        "context": "To get the top-5 predictions class names:\n\n```py\n>>> # Get imagenet class mappings\n>>> url, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\n>>> urllib.request.urlretrieve(url, filename) \n>>> with open(\"imagenet_classes.txt\", \"r\") as f:\n...     categories = [s.strip() for s in f.readlines()]\n\n>>> # Print top categories per image\n>>> top5_prob, top5_catid = torch.topk(probabilities, 5)\n>>> for i in range(top5_prob.size(0)):\n...     print(categories[top5_catid[i]], top5_prob[i].item())\n>>> # prints class names and probabilities like:\n>>> # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n```\n\nReplace the model name with the variant you want to use, e.g. `dla102`. You can find the IDs in the model summaries at the top of this page.\n\nTo extract image features with this model, follow the [timm feature extraction examples](../feature_extraction), just change the name of the model you want to use.\n\n## How do I finetune this model?\n\nYou can finetune any of the pre-trained models just by changing the classifier (the last layer).\n\n```py\n>>> model = timm.create_model('dla102', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset.\n\n## How do I train this model?\n\nYou can follow the [timm recipe scripts](../scripts) for training a new model afresh.\n\n## Citation\n\n```BibTeX\n@misc{yu2019deep,\n      title={Deep Layer Aggregation}, \n      author={Fisher Yu and Dequan Wang and Evan Shelhamer and Trevor Darrell},\n      year={2019},\n      eprint={1707.06484},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```",
        "question": "(your factoid question)\n",
        "answer": " (your answer to the factoid question)\n\nNow here is the context.\n\nContext: To get the top-5 predictions class names:\n\n```py\n>>> # Get imagenet class mappings\n>>> url, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\n>>> urllib.request.urlretrieve(url, filename) \n>>> with open(\"imagenet_classes.txt\", \"r\") as f:\n...     categories = [s.strip() for s in f.readlines()]\n\n>>> # Print top categories per image\n>>> top5_prob, top5_catid = torch.topk(probabilities, 5)\n>>> for i in range(top5_prob.size(0)):\n...     print(categories[top5_catid[i]], top5_prob[i].item())\n>>> # prints class names and probabilities like:\n>>> # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n```\n\nReplace the model name with the variant you want to use, e.g. `dla102`. You can find the IDs in the model summaries at the top of this page.\n\nTo extract image features with this model, follow the [timm feature extraction examples](../feature_extraction), just change the name of the model you want to use.\n\n## How do I finetune this model?\n\nYou can finetune any of the pre-trained models just by changing the classifier (the last layer).\n\n```py\n>>> model = timm.create_model('dla102', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset.\n\n## How do I train this model?\n\nYou can follow the [timm recipe scripts](../scripts) for training a new model afresh.\n\n## Citation\n\n```BibTeX\n@misc{yu2019deep,\n      title={Deep Layer Aggregation}, \n      author={Fisher Yu and Dequan Wang and Evan Shelhamer and Trevor Darrell},\n      year={2019},\n      eprint={1707.06484},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\n\nOutput:::\nHow do I get the top-5 predictions class names for an image?\nTo get the top-5 predictions class names for an image, you can use the following code snippet:\n\n```py\n# Get imagenet class mappings\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\nurllib.request.urlretrieve(url, filename) \nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Print top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())\n```\n\nThis will print the top-5 class names and their corresponding probabilities for a given image.",
        "source_doc": "huggingface/pytorch-image-models/blob/main/hfdocs/source/models/dla.mdx"
    },
    {
        "context": "--\ntitle: \"Leveraging Pre-trained Language Model Checkpoints for Encoder-Decoder Models\"\nthumbnail: /blog/assets/08_warm_starting_encoder_decoder/thumbnail.png\nauthors:\n- user: patrickvonplaten\n---\n\n# Leveraging Pre-trained Language Model Checkpoints for Encoder-Decoder Models\n\n\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Leveraging_Pre_trained_Checkpoints_for_Encoder_Decoder_Models.ipynb\">\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n\n\nTransformer-based encoder-decoder models were proposed in [Vaswani et\nal. (2017)](https://arxiv.org/pdf/1706.03762.pdf) and have recently\nexperienced a surge of interest, *e.g.* [Lewis et al.\n(2019)](https://arxiv.org/abs/1910.13461), [Raffel et al.\n(2019)](https://arxiv.org/abs/1910.10683), [Zhang et al.\n(2020)](https://arxiv.org/abs/1912.08777), [Zaheer et al.\n(2020)](https://arxiv.org/abs/2007.14062), [Yan et al.\n(2020)](https://arxiv.org/pdf/2001.04063.pdf).\n\nSimilar to BERT and GPT2, massive pre-trained encoder-decoder models\nhave shown to significantly boost performance on a variety of\n*sequence-to-sequence* tasks [Lewis et al.\n(2019)](https://arxiv.org/abs/1910.13461), [Raffel et al.\n(2019)](https://arxiv.org/abs/1910.10683). However, due to the enormous\ncomputational cost attached to pre-training encoder-decoder models, the\ndevelopment of such models is mainly limited to large companies and\ninstitutes.",
        "question": "(your factoid question)\n",
        "answer": " Transformer-based encoder-decoder models were proposed in Vaswani et al. (2017).",
        "source_doc": "huggingface/blog/blob/main/warm-starting-encoder-decoder.md"
    },
    {
        "context": "1. That you have this tag in your model: ML-Agents-SoccerTwos. This is the tag we use to select models to be added to the challenge pool. To do that go to your model and check the tags\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/verify1.png\" alt=\"Verify\"/>\n\n\nIf it’s not the case you just need to modify the readme and add it\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/verify2.png\" alt=\"Verify\"/>\n\n2. That you have a `SoccerTwos.onnx` file\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/verify3.png\" alt=\"Verify\"/>\n\nWe strongly suggest that you create a new model when you push to the Hub if you want to train it again or train a new version.\n\n## Step 7: Visualize some match in our demo\n\nNow that your model is part of AI vs AI Challenge, **you can visualize how good it is compared to others**: https://huggingface.co/spaces/unity/ML-Agents-SoccerTwos\n\nIn order to do that, you just need to go to this demo:\n\n- Select your model as team blue (or team purple if you prefer) and another model to compete against. The best opponents to compare your model to are either whoever is on top of the leaderboard or the [baseline model](https://huggingface.co/unity/MLAgents-SoccerTwos)\n\nThe matches you see live are not used in the calculation of your result **but they are a good way to visualize how good your agent is**.\n\nAnd don't hesitate to share the best score your agent gets on discord in the #rl-i-made-this channel 🔥",
        "question": "What file should a model have to be added to the AI vs AI Challenge pool?\n",
        "answer": " The model should have a `SoccerTwos.onnx` file to be added to the AI vs AI Challenge pool.",
        "source_doc": "huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx"
    },
    {
        "context": "1. **[BridgeTower](https://huggingface.co/docs/transformers/model_doc/bridgetower)** (हरबिन इंस्टिट्यूट ऑफ़ टेक्नोलॉजी/माइक्रोसॉफ्ट रिसर्च एशिया/इंटेल लैब्स से) कागज के साथ [ब्रिजटॉवर: विजन-लैंग्वेज रिप्रेजेंटेशन लर्निंग में एनकोडर्स के बीच ब्रिज बनाना](<https://arxiv.org/abs/2206.08657>) by Xiao Xu, Chenfei Wu, Shachar Rosenman, Vasudev Lal, Wanxiang Che, Nan Duan.\n1. **[BROS](https://huggingface.co/docs/transformers/model_doc/bros)** (NAVER CLOVA से) Teakgyu Hong, Donghyun Kim, Mingi Ji, Wonseok Hwang, Daehyun Nam, Sungrae Park. द्वाराअनुसंधान पत्र [BROS: A Pre-trained Language Model Focusing on Text and Layout for Better Key Information Extraction from Documents](https://arxiv.org/abs/2108.04539) के साथ जारी किया गया\n1. **[ByT5](https://huggingface.co/docs/transformers/model_doc/byt5)** (Google अनुसंधान से) साथ में कागज [ByT5: पूर्व-प्रशिक्षित बाइट-टू-बाइट मॉडल के साथ एक टोकन-मुक्त भविष्य की ओर] (https://arxiv.org/abs/2105.13626) Linting Xue, Aditya Barua, Noah Constant, रामी अल-रफू, शरण नारंग, मिहिर काले, एडम रॉबर्ट्स, कॉलिन रैफेल द्वारा पोस्ट किया गया।\n1. **[CamemBERT](https://huggingface.co/docs/transformers/model_doc/camembert)** (इनरिया/फेसबुक/सोरबोन से) साथ में कागज [CamemBERT: एक टेस्टी फ्रेंच लैंग्वेज मॉडल](https:// arxiv.org/abs/1911.03894) लुई मार्टिन*, बेंजामिन मुलर*, पेड्रो जेवियर ऑर्टिज़ सुआरेज़*, योआन ड्यूपॉन्ट, लॉरेंट रोमरी, एरिक विलेमोन्टे डे ला क्लर्जरी, जैमे सेडाह और बेनोइट सगोट द्वारा।\n1. **[CANINE](https://huggingface.co/docs/transformers/model_doc/canine)** (Google रिसर्च से) साथ में दिया गया पेपर [कैनाइन: प्री-ट्रेनिंग ए एफिशिएंट टोकनाइजेशन-फ्री एनकोडर फॉर लैंग्वेज रिप्रेजेंटेशन]( https://arxiv.org/abs/2103.06874) जोनाथन एच क्लार्क, डैन गैरेट, यूलिया टर्क, जॉन विएटिंग द्वारा।",
        "question": "(your factoid question)\n",
        "answer": " (your answer to the factoid question)\n\nNow here is the context.\n\nContext: 1. **[BridgeTower](https://huggingface.co/docs/transformers/model_doc/bridgetower)** (हरबिन इंस्टिट्यूट ऑफ़ टेक्नोलॉजी/माइक्रोसॉफ्ट रिसर्च एशिया/इंटेल लैब्स से) कागज के साथ [ब्रिजटॉवर: विजन-लैंग्वेज रिप्रेजेंटेशन लर्निंग में एनकोडर्स के बीच ब्रिज बनाना](<https://arxiv.org/abs/2206.08657>) by Xiao Xu, Chenfei Wu, Shachar Rosenman, Vasudev Lal, Wanxiang Che, Nan Duan.\n1. **[BROS](https://huggingface.co/docs/transformers/model_doc/bros)** (NAVER CLOVA से) Teakgyu Hong, Donghyun Kim, Mingi Ji, Wonseok Hwang, Daehyun Nam, Sungrae Park. द्वाराअनुसंधान पत्र [BROS: A Pre-trained Language Model Focusing on Text and Layout for Better Key Information Extraction from Documents](https://arxiv.org/abs/2108.04539) के साथ जारी किया गया\n1. **[ByT5](https://huggingface.co/docs/transformers/model_doc/byt5)** (Google अनुसंधान से) साथ में कागज [ByT5: पूर्व-प्रशिक्षित बाइट-टू-बाइट मॉडल के साथ एक टोकन-मुक्त भविष्य की ओर] (https://arxiv.org/abs/2105.13626) Linting Xue, Aditya Barua, Noah Constant, रामी अल-रफू, शरण नारंग, मिहिर काले, एडम रॉबर्ट्स, कॉलिन रैफेल द्वारा पोस्ट किया गया।\n1. **[CamemBERT](https://huggingface.co/docs/transformers/model_doc/camembert)** (इनरिया/फेसबुक/सोरबोन से) साथ में कागज [CamemBERT: एक टेस्टी फ्रेंच लैंग्वेज मॉडल](https:// arxiv.org/abs/1911.03894) लुई मार्टिन*, बेंजामिन मुलर*, पेड्रो जेवियर ऑर्टिज़ सुआरेज़*, योआन ड्यूपॉन्ट, लॉरेंट रोमरी, एरिक विलेमोन्टे डे ला क्लर्जरी, जैमे सेडाह और बेनोइट सगोट द्वारा।\n1. **[CANINE](https://huggingface.co/docs/transformers/model_doc/canine)** (Google रिसर्च से) साथ में दिया गया पेपर [कैनाइन: प्री-ट्रेनिंग ए एफिशिएंट टोकनाइजेशन-फ्री एनकोडर फॉर लैंग्वेज रिप्रेजेंटेशन]( https://arxiv.org/abs/2103.06874) जोनाथन एच क्लार्क, डैन गैरेट, यूलिया टर्क, जॉन विएटिंग द्वारा।\n\nOutput:::\n\nQ: What is the name of the pre-trained language model for better key information extraction from documents developed by Xiao Xu, Chenfei Wu, Shachar Rosenman, Vasudev Lal, Wanxiang Che, and Nan Duan?\nA: BridgeTower",
        "source_doc": "huggingface/transformers/blob/main/README_hd.md"
    },
    {
        "context": "Run the following command to authenticate your token\n\n```bash\nhuggingface-cli login\n```\n\nIf you have already cloned the repo, then you won't need to go through these steps.\n\n<br>\n\n#### Hardware\nWith `gradient_checkpointing` and `mixed_precision` it should be possible to fine tune the model on a single 24GB GPU. For higher `batch_size` and faster training it's better to use GPUs with >30GB memory.\n\n**___Note: Change the `resolution` to 768 if you are using the [stable-diffusion-2](https://huggingface.co/stabilityai/stable-diffusion-2) 768x768 model.___**\n<!-- accelerate_snippet_start -->\n```bash\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport DATASET_NAME=\"lambdalabs/pokemon-blip-captions\"\n\naccelerate launch --mixed_precision=\"fp16\"  train_text_to_image.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --dataset_name=$DATASET_NAME \\\n  --use_ema \\\n  --resolution=512 --center_crop --random_flip \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --gradient_checkpointing \\\n  --max_train_steps=15000 \\\n  --learning_rate=1e-05 \\\n  --max_grad_norm=1 \\\n  --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n  --output_dir=\"sd-pokemon-model\"\n```\n<!-- accelerate_snippet_end -->\n\n\nTo run on your own training files prepare the dataset according to the format required by `datasets`, you can find the instructions for how to do that in this [document](https://huggingface.co/docs/datasets/v2.4.0/en/image_load#imagefolder-with-metadata).\nIf you wish to use custom loading logic, you should modify the script, we have left pointers for that in the training script.\n\n```bash\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport TRAIN_DIR=\"path_to_your_dataset\"",
        "question": "What is the recommended GPU memory for fine tuning the model with a higher batch size and faster training?\n\n",
        "answer": " The recommended GPU memory for fine tuning the model with a higher batch size and faster training is >30GB.",
        "source_doc": "huggingface/diffusers/blob/main/examples/text_to_image/README.md"
    },
    {
        "context": "At this stage, the vocabulary is `[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\", \"hug\"]` and our set of unique words\nis represented as\n\n```\n(\"hug\", 10), (\"p\" \"ug\", 5), (\"p\" \"un\", 12), (\"b\" \"un\", 4), (\"hug\" \"s\", 5)\n```\n\nAssuming, that the Byte-Pair Encoding training would stop at this point, the learned merge rules would then be applied\nto new words (as long as those new words do not include symbols that were not in the base vocabulary). For instance,\nthe word `\"bug\"` would be tokenized to `[\"b\", \"ug\"]` but `\"mug\"` would be tokenized as `[\"<unk>\", \"ug\"]` since\nthe symbol `\"m\"` is not in the base vocabulary. In general, single letters such as `\"m\"` are not replaced by the\n`\"<unk>\"` symbol because the training data usually includes at least one occurrence of each letter, but it is likely\nto happen for very special characters like emojis.\n\nAs mentioned earlier, the vocabulary size, *i.e.* the base vocabulary size + the number of merges, is a hyperparameter\nto choose. For instance [GPT](model_doc/gpt) has a vocabulary size of 40,478 since they have 478 base characters\nand chose to stop training after 40,000 merges.\n\n#### Byte-level BPE\n\nA base vocabulary that includes all possible base characters can be quite large if *e.g.* all unicode characters are\nconsidered as base characters. To have a better base vocabulary, [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) uses bytes\nas the base vocabulary, which is a clever trick to force the base vocabulary to be of size 256 while ensuring that\nevery base character is included in the vocabulary. With some additional rules to deal with punctuation, the GPT2's\ntokenizer can tokenize every text without the need for the <unk> symbol. [GPT-2](model_doc/gpt) has a vocabulary\nsize of 50,257, which corresponds to the 256 bytes base tokens, a special end-of-text token and the symbols learned\nwith 50,000 merges.\n\n<a id='wordpiece'></a>\n\n### WordPiece",
        "question": "(your factoid question)\n",
        "answer": "::\nThe base vocabulary of WordPiece is a set of subwords that are learned during the training process. It starts with a base vocabulary of individual characters and learns to merge them into subwords based on the frequency of their occurrence in the training data. The final vocabulary is the set of learned subwords.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/tokenizer_summary.md"
    },
    {
        "context": "### May 13, 2022\n* Official Swin-V2 models and weights added from (https://github.com/microsoft/Swin-Transformer). Cleaned up to support torchscript.\n* Some refactoring for existing `timm` Swin-V2-CR impl, will likely do a bit more to bring parts closer to official and decide whether to merge some aspects.\n* More Vision Transformer relative position / residual post-norm experiments (all trained on TPU thanks to TRC program)\n  * `vit_relpos_small_patch16_224` - 81.5 @ 224, 82.5 @ 320 -- rel pos, layer scale, no class token, avg pool\n  * `vit_relpos_medium_patch16_rpn_224` - 82.3 @ 224, 83.1 @ 320 -- rel pos + res-post-norm, no class token, avg pool\n  * `vit_relpos_medium_patch16_224` - 82.5 @ 224, 83.3 @ 320 -- rel pos, layer scale, no class token, avg pool\n  * `vit_relpos_base_patch16_gapcls_224` - 82.8 @ 224, 83.9 @ 320 -- rel pos, layer scale, class token, avg pool (by mistake)\n* Bring 512 dim, 8-head 'medium' ViT model variant back to life (after using in a pre DeiT 'small' model for first ViT impl back in 2020)\n* Add ViT relative position support for switching btw existing impl and some additions in official Swin-V2 impl for future trials\n* Sequencer2D impl (https://arxiv.org/abs/2205.01972), added via PR from author (https://github.com/okojoalg)",
        "question": "What is the accuracy of the `vit_relpos_small_patch16_224` model on ImageNet at 224 resolution?\n",
        "answer": " The accuracy of the `vit_relpos_small_patch16_224` model on ImageNet at 224 resolution is 81.5.",
        "source_doc": "huggingface/pytorch-image-models/blob/main/docs/changes.md"
    },
    {
        "context": "On the other hand, by using our Japanese tokenizer, the prompt is split into interpretable tokens and the number of tokens is reduced. For example, \"サラリーマン 油絵\" can be tokenized as `['▁', 'サラリーマン', '▁', '油', '絵']`, which is correctly tokenized in Japanese.\n```python\nfrom transformers import T5Tokenizer\ntokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-stable-diffusion\", subfolder=\"tokenizer\", use_auth_token=True)\ntokenizer.do_lower_case = True\ntokens = tokenizer(text, add_special_tokens=False)['input_ids']\nprint(\"tokens:\", tokenizer.convert_ids_to_tokens(tokens))\n# tokens: ['▁', 'サラリーマン', '▁', '油', '絵']\nprint(\"decoded text:\", tokenizer.decode(tokens))\n# decoded text: サラリーマン 油絵\n```\n\nThis stage enables the model to understand Japanese prompts but does not still output Japanese-style images because the latent diffusion model has not been changed at all. In other words, the Japanese word \"salary man\" can be interpreted as the English word \"businessman,\" but the generated result is a businessman with a Western face, as shown below.\n\n<p align=\"center\">\n<img src=\"assets/106_japanese_stable_diffusion/jsd-stage1.jpeg\" alt=\"salary man of japanese stable diffusion at stage 1\" title=\"salary man of japanese stable diffusion at stage 1\">\n</p>\n\n*\"サラリーマン 油絵\", which means exactly \"salary man, oil painting\", from the 1st-stage Japanese Stable Diffusion*\n\n\nTherefore, in the 2nd stage, we train to output more Japanese-style images.\n\n#### 2nd stage: Fine-tune the text encoder and the latent diffusion model jointly\nIn the 2nd stage, we will train both the text encoder and the latent diffusion model to generate Japanese-style images. This stage is essential to make the model become a more language-specific model. After this, the model can finally generate a businessman with a Japanese face, as shown in the image below.",
        "question": "What is the input prompt for the Japanese stable diffusion model at stage 1?\n",
        "answer": " The input prompt for the Japanese stable diffusion model at stage 1 is \"サラリーマン 油絵\".",
        "source_doc": "huggingface/blog/blob/main/japanese-stable-diffusion.md"
    },
    {
        "context": "With `gr.Interface()`, we can define all of that with a single function call:\n\n```python\nimport gradio as gr\n\ngr.Interface(\n    predict,\n    inputs=[\n        gr.Slider(0, 1000, label='Seed', default=42),\n    ],\n    outputs=\"image\",\n).launch()\n```\n\n\n## Step 4 — Even more punks!\n\nGenerating 4 punks at a time is a good start, but maybe we'd like to control how many we want to make each time. Adding more inputs to our Gradio interface is as simple as adding another item to the `inputs` list that we pass to `gr.Interface`:\n\n```python\ngr.Interface(\n    predict,\n    inputs=[\n        gr.Slider(0, 1000, label='Seed', default=42),\n        gr.Slider(4, 64, label='Number of Punks', step=1, default=10), # Adding another slider!\n    ],\n    outputs=\"image\",\n).launch()\n```\n\nThe new input will be passed to our `predict()` function, so we have to make some changes to that function to accept a new parameter:\n\n```python\ndef predict(seed, num_punks):\n    torch.manual_seed(seed)\n    z = torch.randn(num_punks, 100, 1, 1)\n    punks = model(z)\n    save_image(punks, \"punks.png\", normalize=True)\n    return 'punks.png'\n```\n\nWhen you relaunch your interface, you should see a second slider that'll let you control the number of punks!\n\n## Step 5 - Polishing it up\n\nYour Gradio app is pretty much good to go, but you can add a few extra things to really make it ready for the spotlight ✨\n\nWe can add some examples that users can easily try out by adding this to the `gr.Interface`:\n\n```python\ngr.Interface(\n    # ...\n    # keep everything as it is, and then add\n    examples=[[123, 15], [42, 29], [456, 8], [1337, 35]],\n).launch(cache_examples=True) # cache_examples is optional\n```\n\nThe `examples` parameter takes a list of lists, where each item in the sublists is ordered in the same order that we've listed the `inputs`. So in our case, `[seed, num_punks]`. Give it a try!",
        "question": "(your factoid question)\n",
        "answer": "** We can add examples to our `gr.Interface` by passing a list of lists to the `examples` parameter. Each sublist should be ordered in the same way as the `inputs` parameter.",
        "source_doc": "gradio-app/gradio/blob/main/guides/09_other-tutorials/create-your-own-friends-with-a-gan.md"
    },
    {
        "context": "```bibtex\n@article{scikit-learn,\ntitle={Scikit-learn: Machine Learning in {P}ython},\nauthor={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},\njournal={Journal of Machine Learning Research},\nvolume={12},\npages={2825--2830},\nyear={2011}\n}\n```\n\n## Further References\nThis implementation is a wrapper around the [Scikit-learn implementation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html). Much of the documentation here was adapted from their existing documentation, as well.\n\nThe [Guide to ROC and AUC](https://youtu.be/iCZJfO-7C5Q) video from the channel Data Science Bits is also very informative.",
        "question": "What is the Scikit-learn function to calculate the area under the ROC curve?\n\n",
        "answer": " The Scikit-learn function to calculate the area under the ROC curve is `roc_auc_score`.\n```",
        "source_doc": "huggingface/evaluate/blob/main/metrics/roc_auc/README.md"
    },
    {
        "context": "We use both in the `transformers` library. We leverage and adapt `logging`'s `captureWarning` method to allow\nmanagement of these warning messages by the verbosity setters above.\n\nWhat does that mean for developers of the library? We should respect the following heuristic:\n- `warnings` should be favored for developers of the library and libraries dependent on `transformers`\n- `logging` should be used for end-users of the library using it in every-day projects\n\nSee reference of the `captureWarnings` method below.\n\n[[autodoc]] logging.captureWarnings\n\n## Base setters\n\n[[autodoc]] logging.set_verbosity_error\n\n[[autodoc]] logging.set_verbosity_warning\n\n[[autodoc]] logging.set_verbosity_info\n\n[[autodoc]] logging.set_verbosity_debug\n\n## Other functions\n\n[[autodoc]] logging.get_verbosity\n\n[[autodoc]] logging.set_verbosity\n\n[[autodoc]] logging.get_logger\n\n[[autodoc]] logging.enable_default_handler\n\n[[autodoc]] logging.disable_default_handler\n\n[[autodoc]] logging.enable_explicit_format\n\n[[autodoc]] logging.reset_format\n\n[[autodoc]] logging.enable_progress_bar\n\n[[autodoc]] logging.disable_progress_bar",
        "question": "(your factoid question)\n",
        "answer": " (your answer to the factoid question)\n\nNow here is the context.\n\nContext: We use both in the `transformers` library. We leverage and adapt `logging`'s `captureWarning` method to allow\nmanagement of these warning messages by the verbosity setters above.\n\nWhat does that mean for developers of the library? We should respect the following heuristic:\n- `warnings` should be favored for developers of the library and libraries dependent on `transformers`\n- `logging` should be used for end-users of the library using it in every-day projects\n\nSee reference of the `captureWarnings` method below.\n\n[[autodoc]] logging.captureWarnings\n\n## Base setters\n\n[[autodoc]] logging.set_verbosity_error\n\n[[autodoc]] logging.set_verbosity_warning\n\n[[autodoc]] logging.set_verbosity_info\n\n[[autodoc]] logging.set_verbosity_debug\n\n## Other functions\n\n[[autodoc]] logging.get_verbosity\n\n[[autodoc]] logging.set_verbosity\n\n[[autodoc]] logging.get_logger\n\n[[autodoc]] logging.enable_default_handler\n\n[[autodoc]] logging.disable_default_handler\n\n[[autodoc]] logging.enable_explicit_format\n\n[[autodoc]] logging.reset_format\n\n[[autodoc]] logging.enable_progress_bar\n\n[[autodoc]] logging.disable_progress_bar\n\nOutput:::\n\nWhat is the heuristic for developers of the library when using the `transformers` library?\n\nThe heuristic for developers of the library when using the `transformers` library is to favor `warnings` over `logging` for development purposes, and use `logging` for end-users of the library using it in every-day projects. This is because the `transformers` library leverages and adapts `logging`'s `captureWarning` method to allow management of these warning messages by the verbosity setters above.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/main_classes/logging.md"
    },
    {
        "context": "--\ntitle: \"Accelerating PyTorch Transformers with Intel Sapphire Rapids - part 2\"\nthumbnail: /blog/assets/129_intel_sapphire_rapids_inference/01.png\nauthors:\n- user: juliensimon\n---\n\n# Accelerating PyTorch Transformers with Intel Sapphire Rapids, part 2\n\n\nIn a [recent post](https://huggingface.co/blog/intel-sapphire-rapids), we introduced you to the fourth generation of Intel Xeon CPUs, code-named [Sapphire Rapids](https://en.wikipedia.org/wiki/Sapphire_Rapids), and its new Advanced Matrix Extensions ([AMX](https://en.wikipedia.org/wiki/Advanced_Matrix_Extensions)) instruction set. Combining a cluster of Sapphire Rapids servers running on Amazon EC2 and Intel libraries like the [Intel Extension for PyTorch](https://github.com/intel/intel-extension-for-pytorch), we showed you how to efficiently run distributed training at scale, achieving an 8-fold speedup compared to the previous Xeon generation (Ice Lake) with near-linear scaling.\n\nIn this post, we're going to focus on inference. Working with popular HuggingFace transformers implemented with PyTorch, we'll first measure their performance on an Ice Lake server for short and long NLP token sequences. Then, we'll do the same with a Sapphire Rapids server and the latest version of Hugging Face [Optimum Intel](https://github.com/huggingface/optimum-intel), an open-source library dedicated to hardware acceleration for Intel platforms.\n\nLet's get started!\n\n\n## Why You Should Consider CPU-based Inference\n\nThere are several factors to consider when deciding whether to run deep learning inference on a CPU or GPU. The most important one is certainly the size of the model. In general, larger models may benefit more from the additional computational power provided by a GPU, while smaller models can run efficiently on a CPU.",
        "question": "What is one factor to consider when deciding whether to run deep learning inference on a CPU or GPU?\n",
        "answer": " The size of the model is one factor to consider. Larger models may benefit more from the additional computational power provided by a GPU, while smaller models can run efficiently on a CPU.",
        "source_doc": "huggingface/blog/blob/main/intel-sapphire-rapids-inference.md"
    },
    {
        "context": "Let's now run the model just like before *without Flash Attention* and measure the peak GPU memory requirement and inference time.\n\n```python\nimport time\n\nstart_time = time.time()\nresult = pipe(long_prompt, max_new_tokens=60)[0][\"generated_text\"][len(long_prompt):]\n\nprint(f\"Generated in {time.time() - start_time} seconds.\")\nresult\n```\n\n**Output**:\n```\nGenerated in 10.96854019165039 seconds.\nSure. Here is a function that does that.\\n\\ndef bytes_to_giga(bytes):\\n   return bytes / 1024 / 1024 / 1024\\n\\nAnswer: Sure. Here is a function that does that.\\n\\ndef\n````\n\nWe're getting the same output as before, however this time, the model repeats the answer multiple times until it's 60 tokens cut-off. This is not surprising as we've repeated the system prompt ten times for demonstration purposes and thus cued the model to repeat itself.\n\n**Note** that the system prompt should not be repeated ten times in real-world applications - one time is enough!\n\nLet's measure the peak GPU memory requirement.\n\n```python\nbytes_to_giga_bytes(torch.cuda.max_memory_allocated())\n```\n\n**Output**:\n```bash\n37.668193340301514\n```\n\nAs we can see the peak GPU memory requirement is now significantly higher than in the beginning, which is largely due to the longer input sequence. Also the generation takes a little over a minute now.\n\nWe call `flush()` to free GPU memory for our next experiment.\n\n```python\nflush()\n```\n\nFor comparison, let's run the same function, but enable Flash Attention instead.\nTo do so, we convert the model to [BetterTransformer](https://huggingface.co/docs/optimum/bettertransformer/overview) and by doing so enabling PyTorch's [SDPA self-attention](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention) which in turn is able to use Flash Attention.\n\n```python\nmodel.to_bettertransformer()\n```\n\nNow we run the exact same code snippet as before and under the hood Transformers will make use of Flash Attention.",
        "question": "What is the peak GPU memory requirement when running the code snippet with Flash Attention enabled?\n",
        "answer": " The peak GPU memory requirement when running the code snippet with Flash Attention enabled is 37.68896484375 GB.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/llm_tutorial_optimization.md"
    },
    {
        "context": "<!--\nType: model-index\nCollections:\n- Name: HRNet\n  Paper:\n    Title: Deep High-Resolution Representation Learning for Visual Recognition\n    URL: https://paperswithcode.com/paper/190807919\nModels:\n- Name: hrnet_w18\n  In Collection: HRNet\n  Metadata:\n    FLOPs: 5547205500\n    Parameters: 21300000\n    File Size: 85718883\n    Architecture:\n    - Batch Normalization\n    - Convolution\n    - ReLU\n    - Residual Connection\n    Tasks:\n    - Image Classification\n    Training Techniques:\n    - Nesterov Accelerated Gradient\n    - Weight Decay\n    Training Data:\n    - ImageNet\n    Training Resources: 4x NVIDIA V100 GPUs\n    ID: hrnet_w18\n    Epochs: 100\n    Layers: 18\n    Crop Pct: '0.875'\n    Momentum: 0.9\n    Batch Size: 256\n    Image Size: '224'\n    Weight Decay: 0.001\n    Interpolation: bilinear\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/hrnet.py#L800\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-hrnet/hrnetv2_w18-8cb57bb9.pth\n  Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 76.76%\n      Top 5 Accuracy: 93.44%\n- Name: hrnet_w18_small\n  In Collection: HRNet\n  Metadata:\n    FLOPs: 2071651488\n    Parameters: 13190000\n    File Size: 52934302\n    Architecture:\n    - Batch Normalization\n    - Convolution\n    - ReLU\n    - Residual Connection\n    Tasks:\n    - Image Classification\n    Training Techniques:\n    - Nesterov Accelerated Gradient\n    - Weight Decay\n    Training Data:\n    - ImageNet\n    Training Resources: 4x NVIDIA V100 GPUs\n    ID: hrnet_w18_small\n    Epochs: 100\n    Layers: 18\n    Crop Pct: '0.875'\n    Momentum: 0.9\n    Batch Size: 256\n    Image Size: '224'\n    Weight Decay: 0.001\n    Interpolation: bilinear\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/hrnet.py#L790",
        "question": "(your factoid question)\n",
        "answer": " The number of parameters for the hrnet_w18 model is 21,300,000.",
        "source_doc": "huggingface/pytorch-image-models/blob/main/docs/models/hrnet.md"
    },
    {
        "context": "# checkpoints to leverage\nencoder_id = \"facebook/wav2vec2-base\"\ndecoder_id = \"facebook/bart-base\"\n\n# load and save speech-encoder-decoder model\n# set some hyper-parameters for training and evaluation\nmodel = SpeechEncoderDecoderModel.from_encoder_decoder_pretrained(encoder_id, decoder_id, encoder_add_adapter=True, encoder_feat_proj_dropout=0.0, encoder_layerdrop=0.0, max_length=200, num_beams=5)\nmodel.config.decoder_start_token_id = model.decoder.config.bos_token_id\nmodel.config.pad_token_id = model.decoder.config.pad_token_id\nmodel.config.eos_token_id = model.decoder.config.eos_token_id\nmodel.save_pretrained(\"./\")\n\n# load and save processor\nfeature_extractor = AutoFeatureExtractor.from_pretrained(encoder_id)\ntokenizer = AutoTokenizer.from_pretrained(decoder_id)\nprocessor = Wav2Vec2Processor(feature_extractor, tokenizer)\nprocessor.save_pretrained(\"./\")\n```\n\nFinally, we can upload all files:\n```bash\ngit lfs install\ngit add . && git commit -m \"upload model files\" && git push\n```\n\nand link the official `run_speech_recognition_seq2seq.py` script to the folder:\n\n```bash\nln -s $(realpath <path/to/transformers>/examples/pytorch/speech-recognition/run_speech_recognition_seq2seq.py) ./\n```",
        "question": "What is the id of the decoder's eos token?\n",
        "answer": " The id of the decoder's eos token is 50257.",
        "source_doc": "huggingface/transformers/blob/main/examples/pytorch/speech-recognition/README.md"
    },
    {
        "context": "## Running inference in batch mode\n\nAll of the earlier sections illustrated IDEFICS for a single example. In a very similar fashion, you can run inference \nfor a batch of examples by passing a list of prompts:\n\n```py\n>>> prompts = [\n...     [   \"https://images.unsplash.com/photo-1543349689-9a4d426bee8e?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3501&q=80\",\n...         \"This is an image of \",\n...     ],\n...     [   \"https://images.unsplash.com/photo-1623944889288-cd147dbb517c?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3540&q=80\",\n...         \"This is an image of \",\n...     ],\n...     [   \"https://images.unsplash.com/photo-1471193945509-9ad0617afabf?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3540&q=80\",\n...         \"This is an image of \",\n...     ],\n... ]\n\n>>> inputs = processor(prompts, return_tensors=\"pt\").to(\"cuda\")\n>>> bad_words_ids = processor.tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids\n\n>>> generated_ids = model.generate(**inputs, max_new_tokens=10, bad_words_ids=bad_words_ids)\n>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n>>> for i,t in enumerate(generated_text):\n...     print(f\"{i}:\\n{t}\\n\") \n0:\nThis is an image of the Eiffel Tower in Paris, France.\n\n1:\nThis is an image of a couple on a picnic blanket.\n\n2:\nThis is an image of a vegetable stand.\n```\n\n## IDEFICS instruct for conversational use\n\nFor conversational use cases, you can find fine-tuned instructed versions of the model on the 🤗 Hub: \n`HuggingFaceM4/idefics-80b-instruct` and `HuggingFaceM4/idefics-9b-instruct`.\n\nThese checkpoints are the result of fine-tuning the respective base models on a mixture of supervised and instruction \nfine-tuning datasets, which boosts the downstream performance while making the models more usable in conversational settings.",
        "question": "What is the name of the model used in the example?\n",
        "answer": " The name of the model used in the example is IDEFICS.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/tasks/idefics.md"
    },
    {
        "context": "- We masked the start and end logits corresponding to tokens outside of the context.\n- We then converted the start and end logits into probabilities using a softmax.\n- We attributed a score to each `(start_token, end_token)` pair by taking the product of the corresponding two probabilities.\n- We looked for the pair with the maximum score that yielded a valid answer (e.g., a `start_token` lower than `end_token`).\n\nHere we will change this process slightly because we don't need to compute actual scores (just the predicted answer). This means we can skip the softmax step. To go faster, we also won't score all the possible `(start_token, end_token)` pairs, but only the ones corresponding to the highest `n_best` logits (with `n_best=20`). Since we will skip the softmax, those scores will be logit scores, and will be obtained by taking the sum of the start and end logits (instead of the product, because of the rule \\\\(\\log(ab) = \\log(a) + \\log(b)\\\\)).\n\nTo demonstrate all of this, we will need some kind of predictions. Since we have not trained our model yet, we are going to use the default model for the QA pipeline to generate some predictions on a small part of the validation set. We can use the same processing function as before; because it relies on the global constant `tokenizer`, we just have to change that object to the tokenizer of the model we want to use temporarily:\n\n```python\nsmall_eval_set = raw_datasets[\"validation\"].select(range(100))\ntrained_checkpoint = \"distilbert-base-cased-distilled-squad\"\n\ntokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)\neval_set = small_eval_set.map(\n    preprocess_validation_examples,\n    batched=True,\n    remove_columns=raw_datasets[\"validation\"].column_names,\n)\n```\n\nNow that the preprocessing is done, we change the tokenizer back to the one we originally picked:\n\n```python\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n```",
        "question": "What is the maximum score pair of start and end token logits?\n",
        "answer": " The maximum score pair of start and end token logits is the pair with the highest `n_best` logits (with `n_best=20`). Since we will skip the softmax, those scores will be logit scores, and will be obtained by taking the sum of the start and end logits (instead of the product, because of the rule \\\\(\\log(ab) = \\log(a) + \\log(b)\\\\)).",
        "source_doc": "huggingface/course/blob/main/chapters/en/chapter7/7.mdx"
    },
    {
        "context": "| [How to quantize a model with ONNX Runtime for text classification](https://github.com/huggingface/notebooks/blob/main/examples/text_classification_quantization_ort.ipynb) | Show how to apply static and dynamic quantization on a model using [ONNX Runtime](https://github.com/microsoft/onnxruntime) for any GLUE task. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_ort.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_ort.ipynb) |\n| [How to fine-tune a model for text classification with ONNX Runtime](https://github.com/huggingface/notebooks/blob/main/examples/text_classification_ort.ipynb)             | Show how to DistilBERT model on GLUE tasks using [ONNX Runtime](https://github.com/microsoft/onnxruntime).                                     | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_ort.ipynb)          | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification_ort.ipynb) |",
        "question": "(your factoid question)\n",
        "answer": " You can use the \"How to fine-tune a model for text classification with ONNX Runtime\" notebook provided by Hugging Face on their GitHub repository. The notebook demonstrates how to fine-tune a DistilBERT model on GLUE tasks using ONNX Runtime.",
        "source_doc": "huggingface/optimum/blob/main/docs/source/notebooks.md"
    },
    {
        "context": "![t2i_speedup](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/pt2_benchmarks/t2i_speedup.png)\n\nTo give you an even better idea of how this speed-up holds for the other pipelines, consider the following\ngraph for an A100 with PyTorch 2.0 and `torch.compile`:\n\n![a100_numbers](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/pt2_benchmarks/a100_numbers.png)\n\nIn the following tables, we report our findings in terms of the *number of iterations/second*.\n\n### A100 (batch size: 1)\n\n| **Pipeline** | **torch 2.0 - <br>no compile** | **torch nightly - <br>no compile** | **torch 2.0 - <br>compile** | **torch nightly - <br>compile** |\n|:---:|:---:|:---:|:---:|:---:|\n| SD - txt2img | 21.66 | 23.13 | 44.03 | 49.74 |\n| SD - img2img | 21.81 | 22.40 | 43.92 | 46.32 |\n| SD - inpaint | 22.24 | 23.23 | 43.76 | 49.25 |\n| SD - controlnet | 15.02 | 15.82 | 32.13 | 36.08 |\n| IF | 20.21 / <br>13.84 / <br>24.00 | 20.12 / <br>13.70 / <br>24.03 | ❌ | 97.34 / <br>27.23 / <br>111.66 |\n| SDXL - txt2img | 8.64 | 9.9 | - | - |\n\n### A100 (batch size: 4)\n\n| **Pipeline** | **torch 2.0 - <br>no compile** | **torch nightly - <br>no compile** | **torch 2.0 - <br>compile** | **torch nightly - <br>compile** |\n|:---:|:---:|:---:|:---:|:---:|\n| SD - txt2img | 11.6 | 13.12 | 14.62 | 17.27 |\n| SD - img2img | 11.47 | 13.06 | 14.66 | 17.25 |\n| SD - inpaint | 11.67 | 13.31 | 14.88 | 17.48 |\n| SD - controlnet | 8.28 | 9.38 | 10.51 | 12.41 |\n| IF | 25.02 | 18.04 | ❌ | 48.47 |\n| SDXL - txt2img | 2.44 | 2.74 | - | - |\n\n### A100 (batch size: 16)\n\n| **Pipeline** | **torch 2.0 - <br>no compile** | **torch nightly - <br>no compile** | **torch 2.0 - <br>compile** | **torch nightly - <br>compile** |\n|:---:|:---:|:---:|:---:|:---:|\n| SD - txt2img | 3.04 | 3.6 | 3.83 | 4.68 |\n| SD - img2img | 2.98 | 3.58 | 3.83 | 4.67 |\n| SD - inpaint | 3.04 | 3.66 | 3.9 | 4.76 |\n| SD - controlnet | 2.15 | 2.58 | 2.74 | 3.35 |\n| IF | 8.78 | 9.82 | ❌ | 16.77 |\n| SDXL - txt2img | 0.64 | 0.72 | - | - |",
        "question": "What is the number of iterations per second for the SD pipeline with txt2img on an A100 with a batch size of 16 and torch 2.0 with compile?\n\n",
        "answer": " 3.83",
        "source_doc": "huggingface/diffusers/blob/main/docs/source/en/optimization/torch2.0.md"
    },
    {
        "context": "### Device\n\nIf you use `device=n`, the pipeline automatically puts the model on the specified device.\nThis will work regardless of whether you are using PyTorch or Tensorflow.\n\n```py\ntranscriber = pipeline(model=\"openai/whisper-large-v2\", device=0)\n```\n\nIf the model is too large for a single GPU and you are using PyTorch, you can set `device_map=\"auto\"` to automatically \ndetermine how to load and store the model weights. Using the `device_map` argument requires the 🤗 [Accelerate](https://huggingface.co/docs/accelerate)\npackage:\n\n```bash\npip install --upgrade accelerate\n```\n\nThe following code automatically loads and stores model weights across devices:\n\n```py\ntranscriber = pipeline(model=\"openai/whisper-large-v2\", device_map=\"auto\")\n```\n\nNote that if  `device_map=\"auto\"` is passed, there is no need to add the argument `device=device` when instantiating your `pipeline` as you may encounter some unexpected behavior!\n\n### Batch size\n\nBy default, pipelines will not batch inference for reasons explained in detail [here](https://huggingface.co/docs/transformers/main_classes/pipelines#pipeline-batching). The reason is that batching is not necessarily faster, and can actually be quite slower in some cases.\n\nBut if it works in your use case, you can use:\n\n```py\ntranscriber = pipeline(model=\"openai/whisper-large-v2\", device=0, batch_size=2)\naudio_filenames = [f\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/{i}.flac\" for i in range(1, 5)]\ntexts = transcriber(audio_filenames)\n```\n\nThis runs the pipeline on the 4 provided audio files, but it will pass them in batches of 2\nto the model (which is on a GPU, where batching is more likely to help) without requiring any further code from you. \nThe output should always match what you would have received without batching. It is only meant as a way to help you get more speed out of a pipeline.",
        "question": "How do I specify the device for the model in a pipeline?\n",
        "answer": " You can specify the device for the model in a pipeline by using the `device` argument when instantiating the pipeline. For example, `transcriber = pipeline(model=\"openai/whisper-large-v2\", device=0)` will put the model on the first GPU.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/pipeline_tutorial.md"
    },
    {
        "context": "This model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found\n[here](https://github.com/clovaai/donut).\n\n## Usage tips\n\n- The quickest way to get started with Donut is by checking the [tutorial\n  notebooks](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/Donut), which show how to use the model\n  at inference time as well as fine-tuning on custom data.\n- Donut is always used within the [VisionEncoderDecoder](vision-encoder-decoder) framework.\n\n## Inference examples\n\nDonut's [`VisionEncoderDecoder`] model accepts images as input and makes use of\n[`~generation.GenerationMixin.generate`] to autoregressively generate text given the input image.\n\nThe [`DonutImageProcessor`] class is responsible for preprocessing the input image and\n[`XLMRobertaTokenizer`/`XLMRobertaTokenizerFast`] decodes the generated target tokens to the target string. The\n[`DonutProcessor`] wraps [`DonutImageProcessor`] and [`XLMRobertaTokenizer`/`XLMRobertaTokenizerFast`]\ninto a single instance to both extract the input features and decode the predicted token ids.\n\n- Step-by-step Document Image Classification\n\n```py\n>>> import re\n\n>>> from transformers import DonutProcessor, VisionEncoderDecoderModel\n>>> from datasets import load_dataset\n>>> import torch\n\n>>> processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-rvlcdip\")\n>>> model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-rvlcdip\")\n\n>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n>>> model.to(device)  # doctest: +IGNORE_RESULT\n\n>>> # load document image\n>>> dataset = load_dataset(\"hf-internal-testing/example-documents\", split=\"test\")\n>>> image = dataset[1][\"image\"]\n\n>>> # prepare decoder inputs\n>>> task_prompt = \"<s_rvlcdip>\"\n>>> decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n\n>>> pixel_values = processor(image, return_tensors=\"pt\").pixel_values",
        "question": "(your factoid question)\n",
        "answer": " (your answer to the factoid question)\n\nNow here is the context.\n\nContext: This model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found\n[here](https://github.com/clovaai/donut).\n\n## Usage tips\n\n- The quickest way to get started with Donut is by checking the [tutorial\n  notebooks](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/Donut), which show how to use the model\n  at inference time as well as fine-tuning on custom data.\n- Donut is always used within the [VisionEncoderDecoder](vision-encoder-decoder) framework.\n\n## Inference examples\n\nDonut's [`VisionEncoderDecoder`] model accepts images as input and makes use of\n[`~generation.GenerationMixin.generate`] to autoregressively generate text given the input image.\n\nThe [`DonutImageProcessor`] class is responsible for preprocessing the input image and\n[`XLMRobertaTokenizer`/`XLMRobertaTokenizerFast`] decodes the generated target tokens to the target string. The\n[`DonutProcessor`] wraps [`DonutImageProcessor`] and [`XLMRobertaTokenizer`/`XLMRobertaTokenizerFast`]\ninto a single instance to both extract the input features and decode the predicted token ids.\n\n- Step-by-step Document Image Classification\n\n```py\n>>> import re\n\n>>> from transformers import DonutProcessor, VisionEncoderDecoderModel\n>>> from datasets import load_dataset\n>>> import torch\n\n>>> processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-rvlcdip\")\n>>> model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-rvlcdip\")\n\n>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n>>> model.to(device)  # doctest: +IGNORE_RESULT\n\n>>> # load document image\n>>> dataset = load_dataset(\"hf-internal-testing/example-documents\", split=\"test\")\n>>> image = dataset[1][\"image\"]\n\n>>> # prepare decoder inputs\n>>> task_prompt = \"<s_rvlcdip>\"\n>>> decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n\n>>> pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n\nOutput:::\n    pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n\n>>> # generate text\n>>> outputs = model.generate(\n>>>     pixel_values.to(device),\n>>>     decoder_input_ids=decoder_input_ids.to(device),\n>>>     max_length=128,\n>>>     num_beams=4,\n>>>     early_stopping=True,\n>>> )\n\nOutput:::\n    outputs = model.generate(\n        pixel_values.to(device),\n        decoder_input_ids=decoder_input_ids.to(device),\n        max_length=128,\n        num_beams=4,\n        early_stopping=True,\n    )\n\n>>> # decode text\n>>> generated_text = processor.tokenizer.decode(outputs[0], skip_special_tokens=True)\n>>> generated_text = re.sub(r\"<.*?>\", \"\", generated_text)\n>>> print(generated_text)\n\nOutput:::\n    generated_text = processor.tokenizer.decode(outputs[0], skip_special_tokens=True)\n    generated_text = re.sub(r\"<.*?>\", \"\", generated_text)\n    print(generated_text)\n\n```\n\n## Fine-tuning\n\n- Fine-tuning Donut on a custom dataset\n\n```py\n>>> import re\n\n>>> from transformers import DonutProcessor, VisionEncoderDecoderModel\n>>> from datasets import load_dataset\n>>> import torch\n\n>>> processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-rvlcdip\")\n>>> model = VisionEncoderDecoderModel.from",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/donut.md"
    },
    {
        "context": "Creating a URL for a whole test session log:\n\n```bash\npytest --pastebin=all tests/utils/test_logging.py\n```\n\n## Writing tests\n\n🤗 transformers tests are based on `unittest`, but run by `pytest`, so most of the time features from both systems\ncan be used.\n\nYou can read [here](https://docs.pytest.org/en/stable/unittest.html) which features are supported, but the important\nthing to remember is that most `pytest` fixtures don't work. Neither parametrization, but we use the module\n`parameterized` that works in a similar way.\n\n\n### Parametrization\n\nOften, there is a need to run the same test multiple times, but with different arguments. It could be done from within\nthe test, but then there is no way of running that test for just one set of arguments.\n\n```python\n# test_this1.py\nimport unittest\nfrom parameterized import parameterized\n\n\nclass TestMathUnitTest(unittest.TestCase):\n    @parameterized.expand(\n        [\n            (\"negative\", -1.5, -2.0),\n            (\"integer\", 1, 1.0),\n            (\"large fraction\", 1.6, 1),\n        ]\n    )\n    def test_floor(self, name, input, expected):\n        assert_equal(math.floor(input), expected)\n```\n\nNow, by default this test will be run 3 times, each time with the last 3 arguments of `test_floor` being assigned the\ncorresponding arguments in the parameter list.\n\nand you could run just the `negative` and `integer` sets of params with:\n\n```bash\npytest -k \"negative and integer\" tests/test_mytest.py\n```\n\nor all but `negative` sub-tests, with:\n\n```bash\npytest -k \"not negative\" tests/test_mytest.py\n```\n\nBesides using the `-k` filter that was just mentioned, you can find out the exact name of each sub-test and run any\nor all of them using their exact names.\n\n```bash\npytest test_this1.py --collect-only -q\n```\n\nand it will list:\n\n```bash\ntest_this1.py::TestMathUnitTest::test_floor_0_negative\ntest_this1.py::TestMathUnitTest::test_floor_1_integer\ntest_this1.py::TestMathUnitTest::test_floor_2_large_fraction\n```",
        "question": "How to run a specific sub-test in a parametrized test?\n\nAnswer:\n\nYou can run a specific sub-test in a parametrized test by using the exact name of the sub-test. To find the exact name of each sub-test, you can use the `--collect-only` option with `pytest` and filter the results with `-q` for a quiet output.\n\nExample:\n\n```bash\npytest test_this1.py --collect-only -q\n```\n\nOutput:\n\n```bash\ntest_this1.py::TestMathUnitTest::test_floor_0_negative\ntest_this1.py::TestMathUnitTest::test_floor_1_integer\ntest_this1.py::TestMathUnitTest::test_floor_2_large_fraction\n```\n\nThen, you can run a specific sub-test by using its exact name.\n\nExample:\n\n```bash\npytest test_this1.py::TestMathUnitTest::test_floor_1_integer\n```",
        "answer": "\n\nYou can run a specific sub-test in a parametrized test by using the exact name of the sub-test. To find the exact name of each sub-test, you can use the `--collect-only` option with `pytest` and filter the results with `-q` for a quiet output.\n\nExample:\n\n```bash\npytest test_this1.py --collect-only -q\n```\n\nOutput:\n\n```bash\ntest_this1.py::TestMathUnitTest::test_floor_0_negative\ntest_this1.py::TestMathUnitTest::test_floor_1_integer\ntest_this1.py::TestMathUnitTest::test_floor_2_large_fraction\n```\n\nThen, you can run a specific sub-test by using its exact name.\n\nExample:\n\n```bash\npytest test_this1.py::TestMathUnitTest::test_floor_1_integer\n```",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/testing.md"
    },
    {
        "context": "![qualitative_comparison_1](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/personal_copilot/qlora_vs_finetune_1.png)\n*Qualitative Example 1*\n\nIn the example above, the completion from GitHub Copilot is along the correct lines but doesn't help much. On the other hand, completions from QLoRA and full fine-tuned models are correctly infilling the entire function call with the necessary parameters. However, they are also adding a lot more noise afterwards. This could be controlled with a post-processing step to limit completions to closing brackets or new lines. Note that both QLoRA and full fine-tuned models produce results with similar quality.\n\n![qualitative_comparison_2](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/personal_copilot/qlora_vs_finetune_2.png)\nQualitative Example 2\n\nIn the second example above, **GitHub Copilot didn't give any completion**. This can be due to the fact that 🤗 PEFT is a recent library and not yet part of Copilot's training data, which **is exactly the type of problem we are trying to address**. On the other hand, completions from QLoRA and full fine-tuned models are correctly infilling the entire function call with the necessary parameters. Again, note that both the QLoRA and the full fine-tuned models are giving generations of similar quality. Inference Code with various examples for full fine-tuned model and peft model are available at [Full_Finetuned_StarCoder_Inference.ipynb](https://github.com/pacman100/DHS-LLM-Workshop/blob/main/personal_copilot/inference/Full_Finetuned_StarCoder_Inference.ipynb) and [PEFT_StarCoder_Inference.ipynb](https://github.com/pacman100/DHS-LLM-Workshop/blob/main/personal_copilot/inference/PEFT_StarCoder_Inference.ipynb), respectively.\n\nTherefore, we can observe that the generations from both the variants are as per expectations. Awesome! 🚀\n\n## How do I use it in VS Code?",
        "question": "How do I use QLoRA in VS Code?\n",
        "answer": " To use QLoRA in VS Code, you need to install the QLoRA extension. Once installed, you can enable it by going to the Extensions view, finding the QLoRA extension, and clicking on the gear icon to open the settings. From there, you can enable the extension and configure any desired settings.",
        "source_doc": "huggingface/blog/blob/main/personal-copilot.md"
    },
    {
        "context": "Answer:  T.F. Riehl\nstart_index 17\nend_index 18\n```",
        "question": "(your factoid question)\n",
        "answer": "  D.E. Knuth",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/tasks/document_question_answering.md"
    },
    {
        "context": "Now, back to the graph mode where we benchmark framework having an omniscient representation of the overall computation graph.\n\n<br>\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"TorchScript memory allocator and cores scaling latencies\" src=\"assets/35_bert_cpu_scaling_part_2/allocators/allocator_and_cores_torchscript_latency.svg\"></medium-zoom>\n  <figcaption>Figure 17. TorchScript memory allocator and cores scaling latencies</figcaption>\n</figure>\n<br>\n<br>\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Google's TensorFlow memory allocator and cores scaling latencies\" src=\"assets/35_bert_cpu_scaling_part_2/allocators/allocator_and_cores_tensorflow_graph_latency.svg\"></medium-zoom>\n  <figcaption>Figure 18. Google's TensorFlow memory allocator and cores scaling latencies</figcaption>\n</figure>\n<br>\n<br>\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Google's TensorFlow with oneDNN enabled memory allocator and cores scaling latencies\" src=\"assets/35_bert_cpu_scaling_part_2/allocators/allocator_and_cores_tensorflow_onednn_graph_latency.svg\"></medium-zoom>\n  <figcaption>Figure 19. Google's TensorFlow with oneDNN enabled memory allocator and cores scaling latencies</figcaption>\n</figure>\n<br>\n<br>\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Intel TensorFlow memory allocator and cores scaling latencies\" src=\"assets/35_bert_cpu_scaling_part_2/allocators/allocator_and_cores_intel_tensorflow_graph_latency.svg\"></medium-zoom>\n  <figcaption>Figure 20. Intel TensorFlow memory allocator and cores scaling latencies</figcaption>\n</figure>\n<br>",
        "question": "(your factoid question)\n",
        "answer": " The memory allocator used by TensorFlow in the graph mode is not explicitly mentioned in the context.",
        "source_doc": "huggingface/blog/blob/main/bert-cpu-scaling-part-2.md"
    },
    {
        "context": "```python\ngenerated = model.generate(\n    **inputs, do_sample=True, seed=(42, 0), max_new_tokens=5\n)\nprint(\"Limiting to 5 new tokens:\", tokenizer.decode(generated[0]))\n# > Limiting to 5 new tokens: TensorFlow is a great learning platform for\ngenerated = model.generate(\n    **inputs, do_sample=True, seed=(42, 0), max_new_tokens=30\n)\nprint(\"Limiting to 30 new tokens:\", tokenizer.decode(generated[0]))\n# > Limiting to 30 new tokens: TensorFlow is a great learning platform for\n# learning about data structure and structure in data science................\n```\n\nSampling has a few knobs you can play with to control randomness. The most important is `temperature`, which sets the overall entropy\nof your output -- values below `1.0` will prioritize sampling tokens with a higher likelihood, whereas values above `1.0`\ndo the opposite. Setting it to `0.0` reduces the behavior to Greedy Decoding, whereas very large values approximate\nuniform sampling.\n\n```python\ngenerated = model.generate(\n    **inputs, do_sample=True, seed=(42, 0), temperature=0.7\n)\nprint(\"Temperature 0.7: \", tokenizer.decode(generated[0]))\n# > Temperature 0.7: TensorFlow is a great way to do things like this........\ngenerated = model.generate(\n    **inputs, do_sample=True, seed=(42, 0), temperature=1.5\n)\nprint(\"Temperature 1.5: \", tokenizer.decode(generated[0]))\n# > Temperature 1.5: TensorFlow is being developed for both Cython and Bamboo.\n# On Bamboo...\n```\n\nContrarily to Sampling, Greedy Decoding will always pick the most likely token at each iteration of generation.\nHowever, it often results in sub-optimal outputs. You can increase the quality of the results through the `num_beams`\nargument. When it is larger than `1`, it triggers Beam Search, which continuously explores high-probability sequences.\nThis exploration comes at the cost of additional resources and computational time.",
        "question": "What is the effect of increasing the `num_beams` argument in Greedy Decoding?\n",
        "answer": " Increasing the `num_beams` argument in Greedy Decoding triggers Beam Search, which continuously explores high-probability sequences. This exploration comes at the cost of additional resources and computational time.",
        "source_doc": "huggingface/blog/blob/main/tf-xla-generate.md"
    },
    {
        "context": "Glossary \n\nThis is a community-created glossary. Contributions are welcomed!\n\n- **Tabular Method:** Type of problem in which the state and action spaces are small enough to approximate value functions to be represented as arrays and tables. \n**Q-learning** is an example of tabular method since a table is used to represent the value for different state-action pairs.\n\n- **Deep Q-Learning:** Method that trains a neural network to approximate, given a state, the different **Q-values** for each possible action at that state.\nIt is used to solve problems when observational space is too big to apply a tabular Q-Learning approach. \n\n- **Temporal Limitation** is a difficulty presented when the environment state is represented by frames. A frame by itself does not provide temporal information. \nIn order to obtain temporal information, we need to **stack** a number of frames together.  \n\n- **Phases of Deep Q-Learning:**\n  - **Sampling:** Actions are performed, and observed experience tuples are stored in a **replay memory**.\n  - **Training:** Batches of tuples are selected randomly and the neural network updates its weights using gradient descent. \n  \n- **Solutions to stabilize Deep Q-Learning:**\n  - **Experience Replay:** A replay memory is created to save experiences samples that can be reused during training. \n  This allows the agent to learn from the same experiences multiple times. Also, it helps the agent avoid forgetting previous experiences as it gets new ones.\n  - **Random sampling** from replay buffer allows to remove correlation in the observation sequences and prevents action values from oscillating or diverging\n  catastrophically.",
        "question": "What is the method used to approximate value functions to be represented as arrays and tables?\n",
        "answer": " The Tabular Method is used to approximate value functions to be represented as arrays and tables.",
        "source_doc": "huggingface/deep-rl-class/blob/main/units/en/unit3/glossary.mdx"
    },
    {
        "context": "Hyperparameters are the same for all accelerators. We used a batch size of 48 samples for each device. You can check hyperparameters out [here](https://huggingface.co/regisss/bridgetower-newyorker-gaudi2-8x#training-hyperparameters) for Gaudi2 and [there](https://huggingface.co/regisss/bridgetower-newyorker-a100-8x#training-hyperparameters) for A100.\n\n**When dealing with datasets involving images, data loading is frequently a bottleneck** because many costly operations are computed on CPU (image decoding, image augmentations) and then full images are sent to the training devices. Ideally, *we would like to send only raw bytes to devices and then perform decoding and various image transformations on device*. But let's see first how to *easily* allocate more resources to data loading for accelerating your runs.\n\n\n### Making use of `dataloader_num_workers`\n\nWhen image loading is done on CPU, a quick way to speed it up would be to allocate more subprocesses for data loading. This is very easy to do with Transformers' `TrainingArguments` (or its Optimum Habana counterpart `GaudiTrainingArguments`): you can use the `dataloader_num_workers=N` argument to set the number of subprocesses (`N`) allocated on CPU for data loading.\n\nThe default is 0, which means that data is loaded in the main process. This may not be optimal as the main process has many things to manage. We can set it to 1 to have one fully dedicated subprocess for data loading. When several subprocesses are allocated, each one of them will be responsible for preparing a batch. This means that RAM consumption will increase with the number of workers. One recommendation would be to set it to the number of CPU cores, but those cores may not be fully free so you will have to try it out to find the best configuration.",
        "question": "How can one allocate more resources to data loading for accelerating runs in Transformers?\n\n",
        "answer": " One can use the `dataloader_num_workers` argument in `TrainingArguments` to set the number of subprocesses allocated on CPU for data loading. The default is 0, which means that data is loaded in the main process. Setting it to 1 will have one fully dedicated subprocess for data loading. When several subprocesses are allocated, each one of them will be responsible for preparing a batch, increasing RAM consumption. One recommendation would be to set it to the number of CPU cores, but those cores may not be fully free so you will have to try it out to find the best configuration.",
        "source_doc": "huggingface/blog/blob/main/bridgetower.md"
    },
    {
        "context": "```python\ninstance_segmentation = pipeline(\"image-segmentation\", \"facebook/mask2former-swin-large-cityscapes-instance\")\nresults = instance_segmentation(Image.open(image))\nresults\n```\n\nAs you can see below, there are multiple cars classified, and there's no classification for pixels other than pixels that belong to car and person instances.\n\n```bash\n[{'score': 0.999944,\n  'label': 'car',\n  'mask': <PIL.Image.Image image mode=L size=612x415>},\n {'score': 0.999945,\n  'label': 'car',\n  'mask': <PIL.Image.Image image mode=L size=612x415>},\n {'score': 0.999652,\n  'label': 'car',\n  'mask': <PIL.Image.Image image mode=L size=612x415>},\n {'score': 0.903529,\n  'label': 'person',\n  'mask': <PIL.Image.Image image mode=L size=612x415>}]\n```\nChecking out one of the car masks below.\n\n```python\nresults[2][\"mask\"]\n```\n<div class=\"flex justify-center\">\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/instance_segmentation_output.png\" alt=\"Semantic Segmentation Output\"/>\n</div>\n\nPanoptic segmentation combines semantic segmentation and instance segmentation, where every pixel is classified into a class and an instance of that class, and there are multiple masks for each instance of a class. We can use [facebook/mask2former-swin-large-cityscapes-panoptic](https://huggingface.co/facebook/mask2former-swin-large-cityscapes-panoptic) for this.\n\n```python\npanoptic_segmentation = pipeline(\"image-segmentation\", \"facebook/mask2former-swin-large-cityscapes-panoptic\")\nresults = panoptic_segmentation(Image.open(image))\nresults\n```\nAs you can see below, we have more classes. We will later illustrate to see that every pixel is classified into one of the classes.",
        "question": "(your factoid question)\n",
        "answer": " (your answer to the factoid question)\n\nNow here is the context.\n\nContext: ```python\ninstance_segmentation = pipeline(\"image-segmentation\", \"facebook/mask2former-swin-large-cityscapes-instance\")\nresults = instance_segmentation(Image.open(image))\nresults\n```\n\nAs you can see below, there are multiple cars classified, and there's no classification for pixels other than pixels that belong to car and person instances.\n\n```bash\n[{'score': 0.999944,\n  'label': 'car',\n  'mask': <PIL.Image.Image image mode=L size=612x415>},\n {'score': 0.999945,\n  'label': 'car',\n  'mask': <PIL.Image.Image image mode=L size=612x415>},\n {'score': 0.999652,\n  'label': 'car',\n  'mask': <PIL.Image.Image image mode=L size=612x415>},\n {'score': 0.903529,\n  'label': 'person',\n  'mask': <PIL.Image.Image image mode=L size=612x415>}]\n```\nChecking out one of the car masks below.\n\n```python\nresults[2][\"mask\"]\n```\n<div class=\"flex justify-center\">\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/instance_segmentation_output.png\" alt=\"Semantic Segmentation Output\"/>\n</div>\n\nPanoptic segmentation combines semantic segmentation and instance segmentation, where every pixel is classified into a class and an instance of that class, and there are multiple masks for each instance of a class. We can use [facebook/mask2former-swin-large-cityscapes-panoptic](https://huggingface.co/facebook/mask2former-swin-large-cityscapes-panoptic) for this.\n\n```python\npanoptic_segmentation = pipeline(\"image-segmentation\", \"facebook/mask2former-swin-large-cityscapes-panoptic\")\nresults = panoptic_segmentation(Image.open(image))\nresults\n```\nAs you can see below, we have more classes. We will later illustrate to see that every pixel is classified into one of the classes.\n\nOutput:::\n\n[{'id': 255,\n  'category_id': 0,\n  'iscrowd': False,\n  'area': 10512,\n  'bbox': [0, 0, 612, 415],\n  'segmentation': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  'score': 0.999944,\n  'label': 'car'},\n {'id': 255,\n  'category_id': 1,\n  'iscrowd': False,\n  'area': 10512,\n  'bbox': [0, 0, 612, 415],\n  'segmentation': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/tasks/semantic_segmentation.md"
    },
    {
        "context": "`tokenizers-android-arm-eabi`\n\nThis is the **armv7-linux-androideabi** binary for `tokenizers`",
        "question": "What is the architecture of the `tokenizers-android-arm` binary?\n",
        "answer": " The architecture of the `tokenizers-android-arm` binary is armv7-linux-androideabi.",
        "source_doc": "huggingface/tokenizers/blob/main/bindings/node/npm/android-arm-eabi/README.md"
    },
    {
        "context": "## Inference Endpoint Security level\n\nWe currently offer three types of endpoints, in order or increasing security level:\n\n- **Public**: A Public Endpoint is available from the internet, secured with TLS/SSL, and requires no authentication.\n- **Protected**: A Protected Endpoint is available from the internet, secured with TLS/SSL, and requires a valid Hugging Face token for authentication.\n- **Private** A Private Endpoint is only available through an intra-region secured AWS or Azure PrivateLink connection. Private Endpoints are not accessible from the internet.\n\nPublic and Protected Endpoints do not require any additional configuration. For Private Endpoints, you need to provide the AWS account ID of the account which also should have access to 🤗 Inference Endpoints.\n\n<img\n  src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/endpoint_types.png\"\n  alt=\"endpoint types\"\n/>\n\nHugging Face Privacy Policy - https://huggingface.co/privacy",
        "question": "What are the three types of endpoints offered by Hugging Face in order of increasing security level?\n\n",
        "answer": " The three types of endpoints offered by Hugging Face in order of increasing security level are Public, Protected, and Private.",
        "source_doc": "huggingface/hf-endpoints-documentation/blob/main/docs/source/security.mdx"
    },
    {
        "context": "</div>\n    </div>\n        </div>\n\n\n### Next Steps\n\nAre you seeing the possibilities of Document AI? Every day we work with enterprises to unlock valuable data using state-of-the-art vision and language models. We included links to various demos throughout this post, so use them as a starting point. The last section of the post contains resources for starting to code up your own models, such as visual question answering. Once you are ready to start building your solutions, the [Hugging Face public hub](https://huggingface.co/models) is a great starting point. It hosts a vast array of Document AI models.\n\nIf you want to accelerate your Document AI efforts, Hugging Face can help. Through our [Enterprise Acceleration Program](https://huggingface.co/support) we partner with enterprises to provide guidance on AI use cases. For Document AI, this could involve helping build a pre-train model, improving accuracy on a fine-tuning task, or providing overall guidance on tackling your first Document AI use case.\n\nWe can also provide bundles of compute credits to use our training (AutoTrain) or inference (Spaces or Inference Endpoints) products at scale.\n\n### Resources\n\nNotebooks and tutorials for many Document AI models can be found at: \n- Niels' [Transformers-Tutorials](https://github.com/NielsRogge/Transformers-Tutorials)\n- Philipp's [Document AI with Hugging Face Transformers](https://github.com/philschmid/document-ai-transformers) \n\n<html itemscope itemtype=\"https://schema.org/FAQPage\">\n  <div itemscope itemprop=\"mainEntity\" itemtype=\"https://schema.org/Question\">\n    <a id=\"10-what-are-models\"><h3 itemprop=\"name\"> What are Popular Open-Source Models for Document AI?</h3></a>\n    <div itemscope itemprop=\"acceptedAnswer\" itemtype=\"https://schema.org/Answer\">\n      <div itemprop=\"text\">   \n\nA table of the currently available Transformers models achieving state-of-the-art performance on Document AI tasks. This was last updated in November 2022.",
        "question": "(your factoid question)\n",
        "answer": " (your answer to the factoid question)\n\nNow here is the context.\n\nContext: </div>\n    </div>\n        </div>\n\n\n### Next Steps\n\nAre you seeing the possibilities of Document AI? Every day we work with enterprises to unlock valuable data using state-of-the-art vision and language models. We included links to various demos throughout this post, so use them as a starting point. The last section of the post contains resources for starting to code up your own models, such as visual question answering. Once you are ready to start building your solutions, the [Hugging Face public hub](https://huggingface.co/models) is a great starting point. It hosts a vast array of Document AI models.\n\nIf you want to accelerate your Document AI efforts, Hugging Face can help. Through our [Enterprise Acceleration Program](https://huggingface.co/support) we partner with enterprises to provide guidance on AI use cases. For Document AI, this could involve helping build a pre-train model, improving accuracy on a fine-tuning task, or providing overall guidance on tackling your first Document AI use case.\n\nWe can also provide bundles of compute credits to use our training (AutoTrain) or inference (Spaces or Inference Endpoints) products at scale.\n\n### Resources\n\nNotebooks and tutorials for many Document AI models can be found at: \n- Niels' [Transformers-Tutorials](https://github.com/NielsRogge/Transformers-Tutorials)\n- Philipp's [Document AI with Hugging Face Transformers](https://github.com/philschmid/document-ai-transformers) \n\n<html itemscope itemtype=\"https://schema.org/FAQPage\">\n  <div itemscope itemprop=\"mainEntity\" itemtype=\"https://schema.org/Question\">\n    <a id=\"10-what-are-models\"><h3 itemprop=\"name\"> What are Popular Open-Source Models for Document AI?</h3></a>\n    <div itemscope itemprop=\"acceptedAnswer\" itemtype=\"https://schema.org/Answer\">\n      <div itemprop=\"text\">   \n\nA table of the currently available Transformers models achieving state-of-the-art performance on Document AI tasks. This was last updated in November 2022.\n\nOutput:::\n\n| Model | Task |\n| --- | --- |\n| [Longformer-base-4096](https://huggingface.co/allenai/longformer-base-4096) | Long-range sequence classification |\n| [Longformer-base-4096-finetuned-squadv2](https://huggingface.co/allenai/longformer-base-4096-finetuned-squadv2) | Question answering |\n| [Longformer-base-4096-finetuned-triviaqa](https://huggingface.co/allenai/longformer-base-4096-finetuned-triviaqa) | Question answering |\n| [Longformer-base-4096-finetuned-mnli](https://huggingface.co/allenai/longformer-base-4096-finetuned-mnli) | Natural language inference |\n| [Longformer-base-4096-finetuned-rte](https://huggingface.co/allenai/longformer-base-4096-finetuned-rte) | Natural language inference |\n| [Longformer-base-4096-finetuned-convqa](https://huggingface.co/allenai/longformer-base-4096-finetuned-convqa) | Question answering |\n| [Longformer-base-4096-finetuned-squadv2-distill](https://huggingface.co/allenai/longformer-base-4096-finetuned-squadv2-",
        "source_doc": "huggingface/blog/blob/main/document-ai.md"
    },
    {
        "context": "What if my dataset isn't on the Hub?[[what-if-my-dataset-isnt-on-the-hub]]\n\n<CourseFloatingBanner chapter={5}\n  classNames=\"absolute z-10 right-0 top-0\"\n  notebooks={[\n    {label: \"Google Colab\", value: \"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter5/section2.ipynb\"},\n    {label: \"Aws Studio\", value: \"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter5/section2.ipynb\"},\n]} />\n\nYou know how to use the [Hugging Face Hub](https://huggingface.co/datasets) to download datasets, but you'll often find yourself working with data that is stored either on your laptop or on a remote server. In this section we'll show you how 🤗 Datasets can be used to load datasets that aren't available on the Hugging Face Hub.\n\n<Youtube id=\"HyQgpJTkRdE\"/>\n\n## Working with local and remote datasets[[working-with-local-and-remote-datasets]]\n\n🤗 Datasets provides loading scripts to handle the loading of local and remote datasets. It supports several common data formats, such as:\n\n|    Data format     | Loading script |                         Example                         |\n| :----------------: | :------------: | :-----------------------------------------------------: |\n|     CSV & TSV      |     `csv`      |     `load_dataset(\"csv\", data_files=\"my_file.csv\")`     |\n|     Text files     |     `text`     |    `load_dataset(\"text\", data_files=\"my_file.txt\")`     |\n| JSON & JSON Lines  |     `json`     |   `load_dataset(\"json\", data_files=\"my_file.jsonl\")`    |\n| Pickled DataFrames |    `pandas`    | `load_dataset(\"pandas\", data_files=\"my_dataframe.pkl\")` |\n\nAs shown in the table, for each data format we just need to specify the type of loading script in the `load_dataset()` function, along with a `data_files` argument that specifies the path to one or more files. Let's start by loading a dataset from local files; later we'll see how to do the same with remote files.",
        "question": "(your factoid question)\n",
        "answer": " You can use the `load_dataset()` function from the `datasets` library to load datasets that aren't available on the Hugging Face Hub. The function supports several common data formats, such as CSV, TSV, text files, JSON, JSON Lines, and pickled DataFrames. You just need to specify the type of loading script and the path to the dataset files.",
        "source_doc": "huggingface/course/blob/main/chapters/en/chapter5/2.mdx"
    },
    {
        "context": "```python\nimport gspreadimport pandas as pd\n# 与 Google 进行身份验证并获取表格URL = 'https://docs.google.com/spreadsheets/d/1_91Vps76SKOdDQ8cFxZQdgjTJiz23375sAT7vPvaj4k/edit#gid=0'\ngc = gspread.service_account(\"path/to/key.json\")sh = gc.open_by_url(URL)worksheet = sh.sheet1\ndef get_data():\n    values = worksheet.get_all_values()\n    df = pd.DataFrame(values[1:], columns=values[0])\n    return df\n```\n\n4\\. 数据查询是一个函数，这意味着可以使用 `gr.DataFrame` 组件实时显示数据，或使用 `gr.LinePlot` 组件实时绘制数据（当然，根据数据的不同，可能需要使用不同的图表）。要实现这一点，只需将函数传递给相应的组件，并根据需要设置 `every` 参数来确定组件刷新的频率（以秒为单位）。以下是 Gradio 代码：\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# 📈 实时折线图\")\n    with gr.Row():\n        with gr.Column():\n            gr.DataFrame(get_data, every=5)\n        with gr.Column():\n            gr.LinePlot(get_data, every=5, x=\"日期\", y=\"销售额\", y_title=\"销售额（百万美元）\", overlay_point=True, width=500, height=500)\n\ndemo.queue().launch()  # 启动带有排队功能的演示\n```\n\n现在你有一个每 5 秒刷新一次的仪表盘，可以从你的 Google 表格中获取数据。\n\n## 结论\n\n就是这样！只需几行代码，你就可以使用 `gradio` 和其他库从公共或私有的 Google 表格中读取数据，然后在实时仪表盘中显示和绘制数据。",
        "question": "(your factoid question)\n",
        "answer": " (your answer to the factoid question)\n\nNow here is the context.\n\nContext: ```python\nimport gspreadimport pandas as pd\n# 与 Google 进行身份验证并获取表格URL = 'https://docs.google.com/spreadsheets/d/1_91Vps76SKOdDQ8cFxZQdgjTJiz23375sAT7vPvaj4k/edit#gid=0'\ngc = gspread.service_account(\"path/to/key.json\")sh = gc.open_by_url(URL)worksheet = sh.sheet1\ndef get_data():\n    values = worksheet.get_all_values()\n    df = pd.DataFrame(values[1:], columns=values[0])\n    return df\n```\n\n4\\. 数据查询是一个函数，这意味着可以使用 `gr.DataFrame` 组件实时显示数据，或使用 `gr.LinePlot` 组件实时绘制数据（当然，根据数据的不同，可能需要使用不同的图表）。要实现这一点，只需将函数传递给相应的组件，并根据需要设置 `every` 参数来确定组件刷新的频率（以秒为单位）。以下是 Gradio 代码：\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# 📈 实时折线图\")\n    with gr.Row():\n        with gr.Column():\n            gr.DataFrame(get_data, every=5)\n        with gr.Column():\n            gr.LinePlot(get_data, every=5, x=\"日期\", y=\"销售额\", y_title=\"销售额（百万美元）\", overlay_point=True, width=500, height=500)\n\ndemo.queue().launch()  # 启动带有排队功能的演示\n```\n\n现在你有一个每 5 秒刷新一次的仪表盘，可以从你的 Google 表格中获取数据。\n\n## 结论\n\n就是这样！只需几行代码，你就可以使用 `gradio` 和其他库从公共或私有的 Google 表格中读取数据，然后在实时仪表盘中显示和绘制数据。\n\nOutput:::\n\n**问题：** 在我的仪表盘中，我可以看到所有的数据，但我只想看到最近的一周的数据。\n\n**回答：** 在 `get_data` 函数中，您可以使用 `pd.date_range` 函数来获取最近一周的日期，然后使用 `df.loc` 函数仅获取这些日期的数据。以下是修改后的代码：\n\n```python\ndef get_data():\n    start_date = pd.date_range(end=pd.Timestamp.now(), periods=7, freq=\"D\")[0]\n    end_date = pd.date_range(end=pd.Timestamp.now(), periods=7, freq=\"D\")[-1]\n    values = worksheet.get_all_values()\n    df = pd.DataFrame(values[1:], columns=values[0])\n    df = df.loc[(df[\"日期\"] >= start_date) & (df[\"日期\"] <= end_date)]\n    return df\n```\n\n这将仅在仪表盘中显示最近一周的数据。",
        "source_doc": "gradio-app/gradio/blob/main/guides/cn/05_tabular-data-science-and-plots/creating-a-realtime-dashboard-from-google-sheets.md"
    },
    {
        "context": "## 3. Baselines\n\nAll the results below were run on [Amazon Web Services (AWS) c5.metal instance](https://aws.amazon.com/ec2/instance-types/c5) \nleveraging an Intel Xeon Platinum 8275 CPU (48 cores/96 threads).\nThe choice of this instance provides all the useful CPU features to speedup Deep Learning workloads such as: \n\n- AVX512 instructions set (_which might not be leveraged out-of-the-box by the various frameworks_)\n- Intel Deep Learning Boost (also known as Vector Neural Network Instruction - VNNI) which provides specialized \n  CPU instructions for running quantized networks (_using int8 data type_)\n  \nThe choice of using _metal_ instance is to avoid any virtualization issue which can arise when using cloud providers.\nThis gives us full control of the hardware, especially while targeting the NUMA (Non-Unified Memory Architecture) controller, which \nwe will cover later in this post.\n\n_The operating system was Ubuntu 20.04 (LTS) and all the experiments were conducted using Hugging Face transformers version 4.5.0, PyTorch 1.8.1 & Google TensorFlow 2.4.0_\n\n## 4. Out of the box results\n\n<br>\n<figure class=\"image\">\n  <img alt=\"pytorch versus tensorflow out of the box\" src=\"assets/21_bert_cpu_scaling_part_1/imgs/pytorch_vs_tf_oob.svg\" />\n  <figcaption>Figure 1. PyTorch (1.8.1) vs Google TensorFlow (2.4.1) out of the box</figcaption>\n</figure>\n<br>\n\n\n<br>\n<figure class=\"image\">\n  <img alt=\"pytorch versus tensorflow out of the box bigger batch sizes\" src=\"assets/21_bert_cpu_scaling_part_1/imgs/pytorch_vs_tf_oob_big_batch.svg\" />\n  <figcaption>Figure 2. PyTorch (1.8.1) vs Google TensorFlow (2.4.1) out of the box - (Bigger Batch Size)</figcaption>\n</figure>\n<br>\n\nStraigh to the point, out-of-the-box, PyTorch shows better inference results over TensorFlow for all the configurations tested here.  \nIt is important to note the results out-of-the-box might not reflect the \"optimal\" setup for both PyTorch and TensorFlow and thus it can look deceiving here.",
        "question": "What is the operating system used for the experiments?\n",
        "answer": " The operating system used for the experiments is Ubuntu 20.04 (LTS).",
        "source_doc": "huggingface/blog/blob/main/bert-cpu-scaling-part-1.md"
    },
    {
        "context": "c. output logs:\n  ```bash\n  GPU Memory before entering the train : 1916\n  GPU Memory consumed at the end of the train (end-begin): 66\n  GPU Peak Memory consumed during the train (max-begin): 7488\n  GPU Total Peak Memory consumed during the train (max): 9404\n  CPU Memory before entering the train : 19411\n  CPU Memory consumed at the end of the train (end-begin): 0\n  CPU Peak Memory consumed during the train (max-begin): 0\n  CPU Total Peak Memory consumed during the train (max): 19411\n  epoch=4: train_ppl=tensor(1.0705, device='cuda:0') train_epoch_loss=tensor(0.0681, device='cuda:0')\n  100%|████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:27<00:00,  3.92s/it]\n  GPU Memory before entering the eval : 1982\n  GPU Memory consumed at the end of the eval (end-begin): -66\n  GPU Peak Memory consumed during the eval (max-begin): 672\n  GPU Total Peak Memory consumed during the eval (max): 2654\n  CPU Memory before entering the eval : 19411\n  CPU Memory consumed at the end of the eval (end-begin): 0\n  CPU Peak Memory consumed during the eval (max-begin): 0\n  CPU Total Peak Memory consumed during the eval (max): 19411\n  accuracy=100.0\n  eval_preds[:10]=['no complaint', 'no complaint', 'complaint', 'complaint', 'no complaint', 'no complaint', 'no complaint', 'complaint', 'complaint', 'no complaint']\n  dataset['train'][label_column][:10]=['no complaint', 'no complaint', 'complaint', 'complaint', 'no complaint', 'no complaint', 'no complaint', 'complaint', 'complaint', 'no complaint']\n  ```\n\n### Example of PEFT model inference using 🤗 Accelerate's Big Model Inferencing capabilities\nAn example is provided in [this notebook](https://github.com/huggingface/peft/blob/main/examples/causal_language_modeling/peft_lora_clm_accelerate_big_model_inference.ipynb).\n\n\n## Models support matrix",
        "question": "(your factoid question)\n",
        "answer": " (your answer to the factoid question)\n\nNow here is the context.\n\nContext: c. output logs:\n  ```bash\n  GPU Memory before entering the train : 1916\n  GPU Memory consumed at the end of the train (end-begin): 66\n  GPU Peak Memory consumed during the train (max-begin): 7488\n  GPU Total Peak Memory consumed during the train (max): 9404\n  CPU Memory before entering the train : 19411\n  CPU Memory consumed at the end of the train (end-begin): 0\n  CPU Peak Memory consumed during the train (max-begin): 0\n  CPU Total Peak Memory consumed during the train (max): 19411\n  epoch=4: train_ppl=tensor(1.0705, device='cuda:0') train_epoch_loss=tensor(0.0681, device='cuda:0')\n  100%|████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:27<00:00,  3.92s/it]\n  GPU Memory before entering the eval : 1982\n  GPU Memory consumed at the end of the eval (end-begin): -66\n  GPU Peak Memory consumed during the eval (max-begin): 672\n  GPU Total Peak Memory consumed during the eval (max): 2654\n  CPU Memory before entering the eval : 19411\n  CPU Memory consumed at the end of the eval (end-begin): 0\n  CPU Peak Memory consumed during the eval (max-begin): 0\n  CPU Total Peak Memory consumed during the eval (max): 19411\n  accuracy=100.0\n  eval_preds[:10]=['no complaint', 'no complaint', 'complaint', 'complaint', 'no complaint', 'no complaint', 'no complaint', 'complaint', 'complaint', 'no complaint']\n  dataset['train'][label_column][:10]=['no complaint', 'no complaint', 'complaint', 'complaint', 'no complaint', 'no complaint', 'no complaint', 'complaint', 'complaint', 'no complaint']\n  ```\n\n### Example of PEFT model inference using 🤗 Accelerate's Big Model Inferencing capabilities\nAn example is provided in [this notebook](https://github.com/huggingface/peft/blob/main/examples/causal_language_modeling/peft_lora_clm_accelerate_big_model_inference.ipynb).\n\n\n## Models support matrix\n\nOutput:::\n\n| Model                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ",
        "source_doc": "huggingface/peft/blob/main/README.md"
    },
    {
        "context": "The abstract from the paper is:\n\n*We introduce Würstchen, a novel architecture for text-to-image synthesis that combines competitive performance with unprecedented cost-effectiveness for large-scale text-to-image diffusion models. A key contribution of our work is to develop a latent diffusion technique in which we learn a detailed but extremely compact semantic image representation used to guide the diffusion process. This highly compressed representation of an image provides much more detailed guidance compared to latent representations of language and this significantly reduces the computational requirements to achieve state-of-the-art results. Our approach also improves the quality of text-conditioned image generation based on our user preference study. The training requirements of our approach consists of 24,602 A100-GPU hours - compared to Stable Diffusion 2.1's 200,000 GPU hours. Our approach also requires less training data to achieve these results. Furthermore, our compact latent representations allows us to perform inference over twice as fast, slashing the usual costs and carbon footprint of a state-of-the-art (SOTA) diffusion model significantly, without compromising the end performance. In a broader comparison against SOTA models our approach is substantially more efficient and compares favorably in terms of image quality. We believe that this work motivates more emphasis on the prioritization of both performance and computational accessibility.*",
        "question": "How many GPU hours does the Würstchen architecture require for training?\n",
        "answer": " The Würstchen architecture requires 24,602 A100-GPU hours for training.",
        "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/wuerstchen.md"
    },
    {
        "context": "### Examples\n\nA minimal example of looks as follows:\n```Python\nqrel = {\n    \"query\": [0],\n    \"q0\": [\"q0\"],\n    \"docid\": [\"doc_1\"],\n    \"rel\": [2]\n}\nrun = {\n    \"query\": [0, 0],\n    \"q0\": [\"q0\", \"q0\"],\n    \"docid\": [\"doc_2\", \"doc_1\"],\n    \"rank\": [0, 1],\n    \"score\": [1.5, 1.2],\n    \"system\": [\"test\", \"test\"]\n}\n\ntrec_eval = evaluate.load(\"trec_eval\")\nresults = trec_eval.compute(references=[qrel], predictions=[run])\nresults[\"P@5\"]\n0.2\n```\n\nA more realistic use case with an examples from [`trectools`](https://github.com/joaopalotti/trectools):\n\n```python\nqrel = pd.read_csv(\"robust03_qrels.txt\", sep=\"\\s+\", names=[\"query\", \"q0\", \"docid\", \"rel\"])\nqrel[\"q0\"] = qrel[\"q0\"].astype(str)\nqrel = qrel.to_dict(orient=\"list\")\n\nrun = pd.read_csv(\"input.InexpC2\", sep=\"\\s+\", names=[\"query\", \"q0\", \"docid\", \"rank\", \"score\", \"system\"])\nrun = run.to_dict(orient=\"list\")\n\ntrec_eval = evaluate.load(\"trec_eval\")\nresult = trec_eval.compute(run=[run], qrel=[qrel])\n```\n\n```python\nresult\n\n{'runid': 'InexpC2',\n 'num_ret': 100000,\n 'num_rel': 6074,\n 'num_rel_ret': 3198,\n 'num_q': 100,\n 'map': 0.22485930431817494,\n 'gm_map': 0.10411523825735523,\n 'bpref': 0.217511695914079,\n 'Rprec': 0.2502547201167236,\n 'recip_rank': 0.6646545943335417,\n 'P@5': 0.44,\n 'P@10': 0.37,\n 'P@15': 0.34600000000000003,\n 'P@20': 0.30999999999999994,\n 'P@30': 0.2563333333333333,\n 'P@100': 0.1428,\n 'P@200': 0.09510000000000002,\n 'P@500': 0.05242,\n 'P@1000': 0.03198,\n 'NDCG@5': 0.4101480395089769,\n 'NDCG@10': 0.3806761417784469,\n 'NDCG@15': 0.37819463408955706,\n 'NDCG@20': 0.3686080836061317,\n 'NDCG@30': 0.352474353427451,\n 'NDCG@100': 0.3778329431025776,\n 'NDCG@200': 0.4119129817248979,\n 'NDCG@500': 0.4585354576461375,\n 'NDCG@1000': 0.49092149290805653}\n```\n\n## Limitations and Bias\nThe `trec_eval` metric requires the inputs to be in the TREC run and qrel formats for predictions and references.\n\n\n## Citation",
        "question": "What is the NDCG@10 score of the `InexpC2` run?\n",
        "answer": " 0.3806761417784469",
        "source_doc": "huggingface/evaluate/blob/main/metrics/trec_eval/README.md"
    },
    {
        "context": "@gradio/radio\n\n## 0.3.7\n\n### Patch Changes\n\n- Updated dependencies [[`828fb9e`](https://github.com/gradio-app/gradio/commit/828fb9e6ce15b6ea08318675a2361117596a1b5d), [`73268ee`](https://github.com/gradio-app/gradio/commit/73268ee2e39f23ebdd1e927cb49b8d79c4b9a144)]:\n  - @gradio/statustracker@0.4.3\n  - @gradio/atoms@0.4.1\n\n## 0.3.6\n\n### Patch Changes\n\n- Updated dependencies [[`4d1cbbc`](https://github.com/gradio-app/gradio/commit/4d1cbbcf30833ef1de2d2d2710c7492a379a9a00)]:\n  - @gradio/atoms@0.4.0\n  - @gradio/statustracker@0.4.2\n\n## 0.3.5\n\n### Patch Changes\n\n- Updated dependencies []:\n  - @gradio/atoms@0.3.1\n  - @gradio/statustracker@0.4.1\n\n## 0.3.4\n\n### Patch Changes\n\n- Updated dependencies [[`9caddc17b`](https://github.com/gradio-app/gradio/commit/9caddc17b1dea8da1af8ba724c6a5eab04ce0ed8)]:\n  - @gradio/atoms@0.3.0\n  - @gradio/statustracker@0.4.0\n\n## 0.3.3\n\n### Patch Changes\n\n- Updated dependencies [[`f816136a0`](https://github.com/gradio-app/gradio/commit/f816136a039fa6011be9c4fb14f573e4050a681a)]:\n  - @gradio/atoms@0.2.2\n  - @gradio/statustracker@0.3.2\n\n## 0.3.2\n\n### Patch Changes\n\n- Updated dependencies [[`3cdeabc68`](https://github.com/gradio-app/gradio/commit/3cdeabc6843000310e1a9e1d17190ecbf3bbc780), [`fad92c29d`](https://github.com/gradio-app/gradio/commit/fad92c29dc1f5cd84341aae417c495b33e01245f)]:\n  - @gradio/atoms@0.2.1\n  - @gradio/statustracker@0.3.1\n\n## 0.3.1\n\n### Fixes\n\n- [#6262](https://github.com/gradio-app/gradio/pull/6262) [`afb72bd19`](https://github.com/gradio-app/gradio/commit/afb72bd1970e6c43ddba0638fe9861330bdabb64) - Fix bug where radio.select passes the previous value to the function instead of the selected value. Thanks [@freddyaboulton](https://github.com/freddyaboulton)!\n- [#6249](https://github.com/gradio-app/gradio/pull/6249) [`2cffcf3c3`](https://github.com/gradio-app/gradio/commit/2cffcf3c39acd782f314f8a406100ae22e0809b7) - ensure radios have different names. Thanks [@pngwn](https://github.com/pngwn)!\n\n## 0.3.0\n\n### Features",
        "question": "(your factoid question)\n",
        "answer": "::\n\nThe latest version of @gradio/radio is 0.3.7.",
        "source_doc": "gradio-app/gradio/blob/main/js/radio/CHANGELOG.md"
    },
    {
        "context": "Jump to the last section to see how to upload files to your newly created repository!\n\n## Using the `huggingface_hub` Python library[[using-the-huggingfacehub-python-library]]\n\nThe `huggingface_hub` Python library is a package which offers a set of tools for the model and datasets hubs. It provides simple methods and classes for common tasks like \ngetting information about repositories on the hub and managing them. It provides simple APIs that work on top of git to manage those repositories' content and to integrate the Hub\nin your projects and libraries.\n\nSimilarly to using the `push_to_hub` API, this will require you to have your API token saved in your cache. In order to do this, you will need to use the `login` command from the CLI, as mentioned in the previous section (again, make sure to prepend these commands with the `!` character if running in Google Colab):\n\n```bash\nhuggingface-cli login\n```\n\nThe `huggingface_hub` package offers several methods and classes which are useful for our purpose. Firstly, there are a few methods to manage repository creation, deletion, and others:\n\n```python no-format\nfrom huggingface_hub import (\n    # User management\n    login,\n    logout,\n    whoami,\n\n    # Repository creation and management\n    create_repo,\n    delete_repo,\n    update_repo_visibility,\n\n    # And some methods to retrieve/change information about the content\n    list_models,\n    list_datasets,\n    list_metrics,\n    list_repo_files,\n    upload_file,\n    delete_file,\n)\n```\n\n\nAdditionally, it offers the very powerful `Repository` class to manage a local repository. We will explore these methods and that class in the next few section to understand how to leverage them.\n\nThe `create_repo` method can be used to create a new repository on the hub:\n\n```py\nfrom huggingface_hub import create_repo\n\ncreate_repo(\"dummy-model\")\n```",
        "question": "(your factoid question)\n",
        "answer": "\n\nHow do I create a new repository on the Hugging Face Model Hub using the `create_repo` method from the `huggingface_hub` Python library?\n\nTo create a new repository on the Hugging Face Model Hub using the `create_repo` method from the `huggingface_hub` Python library, you can use the following code snippet:\n```python\nfrom huggingface_hub import create_repo\n\ncreate_repo(\"my-new-repository\")\n```\nThis will create a new repository on the Hugging Face Model Hub with the name \"my-new-repository\". Make sure to replace \"my-new-repository\" with the desired name of your repository.",
        "source_doc": "huggingface/course/blob/main/chapters/en/chapter4/3.mdx"
    },
    {
        "context": "- Website Reload: README in demos docker by [@aliabd](https://github.com/aliabd) in [PR 1100](https://github.com/gradio-app/gradio/pull/1100)\n- Flagging fixes by [@abidlabs](https://github.com/abidlabs) in [PR 1081](https://github.com/gradio-app/gradio/pull/1081)\n- Backend for optional labels by [@abidlabs](https://github.com/abidlabs) in [PR 1080](https://github.com/gradio-app/gradio/pull/1080)\n- Optional labels fe by [@pngwn](https://github.com/pngwn) in [PR 1105](https://github.com/gradio-app/gradio/pull/1105)\n- clean-deprecated-parameters by [@FarukOzderim](https://github.com/FarukOzderim) in [PR 1090](https://github.com/gradio-app/gradio/pull/1090)\n- Blocks rendering fix by [@abidlabs](https://github.com/abidlabs) in [PR 1102](https://github.com/gradio-app/gradio/pull/1102)\n- Redos #1106 by [@abidlabs](https://github.com/abidlabs) in [PR 1112](https://github.com/gradio-app/gradio/pull/1112)\n- Interface types: handle input-only, output-only, and unified interfaces by [@abidlabs](https://github.com/abidlabs) in [PR 1108](https://github.com/gradio-app/gradio/pull/1108)\n- Hotfix + New pypi release 2.9b11 by [@abidlabs](https://github.com/abidlabs) in [PR 1118](https://github.com/gradio-app/gradio/pull/1118)\n- issue-checkbox by [@FarukOzderim](https://github.com/FarukOzderim) in [PR 1122](https://github.com/gradio-app/gradio/pull/1122)\n- issue-checkbox-hotfix by [@FarukOzderim](https://github.com/FarukOzderim) in [PR 1127](https://github.com/gradio-app/gradio/pull/1127)\n- Fix demos in website by [@aliabd](https://github.com/aliabd) in [PR 1130](https://github.com/gradio-app/gradio/pull/1130)\n- Guide for Gradio ONNX model zoo on Huggingface by [@AK391](https://github.com/AK391) in [PR 1073](https://github.com/gradio-app/gradio/pull/1073)\n- ONNX guide fixes by [@aliabd](https://github.com/aliabd) in [PR 1131](https://github.com/gradio-app/gradio/pull/1131)",
        "question": "(your factoid question)\n",
        "answer": " PR 1130",
        "source_doc": "gradio-app/gradio/blob/main/gradio/CHANGELOG.md"
    },
    {
        "context": ">>> model = AutoModelForTokenClassification.from_pretrained(\n...     \"distilbert-base-uncased\", num_labels=13, id2label=id2label, label2id=label2id\n... )\n```\n\nAt this point, only three steps remain:\n\n1. Define your training hyperparameters in [`TrainingArguments`]. The only required parameter is `output_dir` which specifies where to save your model. You'll push this model to the Hub by setting `push_to_hub=True` (you need to be signed in to Hugging Face to upload your model). At the end of each epoch, the [`Trainer`] will evaluate the seqeval scores and save the training checkpoint.\n2. Pass the training arguments to [`Trainer`] along with the model, dataset, tokenizer, data collator, and `compute_metrics` function.\n3. Call [`~Trainer.train`] to finetune your model.\n\n```py\n>>> training_args = TrainingArguments(\n...     output_dir=\"my_awesome_wnut_model\",\n...     learning_rate=2e-5,\n...     per_device_train_batch_size=16,\n...     per_device_eval_batch_size=16,\n...     num_train_epochs=2,\n...     weight_decay=0.01,\n...     evaluation_strategy=\"epoch\",\n...     save_strategy=\"epoch\",\n...     load_best_model_at_end=True,\n...     push_to_hub=True,\n... )\n\n>>> trainer = Trainer(\n...     model=model,\n...     args=training_args,\n...     train_dataset=tokenized_wnut[\"train\"],\n...     eval_dataset=tokenized_wnut[\"test\"],\n...     tokenizer=tokenizer,\n...     data_collator=data_collator,\n...     compute_metrics=compute_metrics,\n... )\n\n>>> trainer.train()\n```\n\nOnce training is completed, share your model to the Hub with the [`~transformers.Trainer.push_to_hub`] method so everyone can use your model:\n\n```py\n>>> trainer.push_to_hub()\n```\n</pt>\n<tf>\n<Tip>\n\nIf you aren't familiar with finetuning a model with Keras, take a look at the basic tutorial [here](../training#train-a-tensorflow-model-with-keras)!\n\n</Tip>\nTo finetune a model in TensorFlow, start by setting up an optimizer function, learning rate schedule, and some training hyperparameters:",
        "question": "(your factoid question)\n",
        "answer": " (your answer to the factoid question)\n\nNow here is the context.\n\nContext: >>> model = AutoModelForTokenClassification.from_pretrained(\n...     \"distilbert-base-uncased\", num_labels=13, id2label=id2label, label2id=label2id\n... )\n```\n\nAt this point, only three steps remain:\n\n1. Define your training hyperparameters in [`TrainingArguments`]. The only required parameter is `output_dir` which specifies where to save your model. You'll push this model to the Hub by setting `push_to_hub=True` (you need to be signed in to Hugging Face to upload your model). At the end of each epoch, the [`Trainer`] will evaluate the seqeval scores and save the training checkpoint.\n2. Pass the training arguments to [`Trainer`] along with the model, dataset, tokenizer, data collator, and `compute_metrics` function.\n3. Call [`~Trainer.train`] to finetune your model.\n\n```py\n>>> training_args = TrainingArguments(\n...     output_dir=\"my_awesome_wnut_model\",\n...     learning_rate=2e-5,\n...     per_device_train_batch_size=16,\n...     per_device_eval_batch_size=16,\n...     num_train_epochs=2,\n...     weight_decay=0.01,\n...     evaluation_strategy=\"epoch\",\n...     save_strategy=\"epoch\",\n...     load_best_model_at_end=True,\n...     push_to_hub=True,\n... )\n\n>>> trainer = Trainer(\n...     model=model,\n...     args=training_args,\n...     train_dataset=tokenized_wnut[\"train\"],\n...     eval_dataset=tokenized_wnut[\"test\"],\n...     tokenizer=tokenizer,\n...     data_collator=data_collator,\n...     compute_metrics=compute_metrics,\n... )\n\n>>> trainer.train()\n```\n\nOnce training is completed, share your model to the Hub with the [`~transformers.Trainer.push_to_hub`] method so everyone can use your model:\n\n```py\n>>> trainer.push_to_hub()\n```\n</pt>\n<tf>\n<Tip>\n\nIf you aren't familiar with finetuning a model with Keras, take a look at the basic tutorial [here](../training#train-a-tensorflow-model-with-keras)!\n\n</Tip>\nTo finetune a model in TensorFlow, start by setting up an optimizer function, learning rate schedule, and some training hyperparameters:\n\nOutput:::\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/tasks/token_classification.md"
    },
    {
        "context": "optional arguments:\n  -h, --help            show this help message and exit\n\nRequired arguments:\n  -m MODEL, --model MODEL\n                        Model ID on huggingface.co or path on disk to load model from.\n  output                Path indicating the directory where to store generated TFLite model.\n\nOptional arguments:\n  --task TASK           The task to export the model for. If not specified, the task will be auto-inferred based on the model. Available tasks depend on\n                        the model, but are among: ['default', 'fill-mask', 'text-generation', 'text2text-generation', 'text-classification', 'token-classification',\n                        'multiple-choice', 'object-detection', 'question-answering', 'image-classification', 'image-segmentation', 'masked-im', 'semantic-\n                        segmentation', 'automatic-speech-recognition', 'audio-classification', 'audio-frame-classification', 'automatic-speech-recognition', 'audio-xvector', 'vision2seq-\n                        lm', 'stable-diffusion', 'zero-shot-object-detection']. For decoder models, use `xxx-with-past` to export the model using past key\n                        values in the decoder.\n  --atol ATOL           If specified, the absolute difference tolerance when validating the model. Otherwise, the default atol for the model will be used.\n  --pad_token_id PAD_TOKEN_ID\n                        This is needed by some models, for some tasks. If not provided, will attempt to use the tokenizer to guess it.\n  --cache_dir CACHE_DIR\n                        Path indicating where to store cache.\n  --trust-remote-code   Allow to use custom code for the modeling hosted in the model repository. This option should only be set for repositories you trust\n                        and in which you have read the code, as it will execute on your local machine arbitrary code present in the model repository.",
        "question": "(your factoid question)\n",
        "answer": " (your answer to the factoid question)\n\nNow here is the context.\n\nContext: optional arguments:\n  -h, --help            show this help message and exit\n\nRequired arguments:\n  -m MODEL, --model MODEL\n                        Model ID on huggingface.co or path on disk to load model from.\n  output                Path indicating the directory where to store generated TFLite model.\n\nOptional arguments:\n  --task TASK           The task to export the model for. If not specified, the task will be auto-inferred based on the model. Available tasks depend on\n                        the model, but are among: ['default', 'fill-mask', 'text-generation', 'text2text-generation', 'text-classification', 'token-classification',\n                        'multiple-choice', 'object-detection', 'question-answering', 'image-classification', 'image-segmentation', 'masked-im', 'semantic-\n                        segmentation', 'automatic-speech-recognition', 'audio-classification', 'audio-frame-classification', 'automatic-speech-recognition', 'audio-xvector', 'vision2seq-\n                        lm', 'stable-diffusion', 'zero-shot-object-detection']. For decoder models, use `xxx-with-past` to export the model using past key\n                        values in the decoder.\n  --atol ATOL           If specified, the absolute difference tolerance when validating the model. Otherwise, the default atol for the model will be used.\n  --pad_token_id PAD_TOKEN_ID\n                        This is needed by some models, for some tasks. If not provided, will attempt to use the tokenizer to guess it.\n  --cache_dir CACHE_DIR\n                        Path indicating where to store cache.\n  --trust-remote-code   Allow to use custom code for the modeling hosted in the model repository. This option should only be set for repositories you trust\n                        and in which you have read the code, as it will execute on your local machine arbitrary code present in the model repository.\n\nOutput:::\nWhat is the task to export the model for?\nThe task to export the model for is auto-inferred based on the model.",
        "source_doc": "huggingface/optimum/blob/main/docs/source/exporters/tflite/usage_guides/export_a_model.mdx"
    },
    {
        "context": "In order to override the default you simply add a new configuration entry:\n\n```json\n{\n    \"communication_data_type\": \"fp32\"\n}\n```\nThe valid values as of this writing are \"fp16\", \"bfp16\", \"fp32\".\n\nnote: stage zero 3 had a bug with regards to bf16 comm dtype that was fixed in `deepspeed==0.8.1`\n\n\n\n### apex\n\nTo configure apex AMP-like mode set:\n\n```json\n\"amp\": {\n    \"enabled\": \"auto\",\n    \"opt_level\": \"auto\"\n}\n```\n\nand the [`Trainer`] will automatically configure it based on the values of `args.fp16_backend` and\n`args.fp16_opt_level`.\n\nThis mode gets enabled when `--fp16 --fp16_backend apex --fp16_opt_level 01` command line args are passed.\n\nYou can also configure this mode explicitly:\n\n```json\n{\n    \"amp\": {\n        \"enabled\": true,\n        \"opt_level\": \"O1\"\n    }\n}\n```\n\nBut then you're on your own synchronizing the [`Trainer`] command line arguments and the DeepSpeed\nconfiguration.\n\nHere is the [documentation](https://www.deepspeed.ai/docs/config-json/#automatic-mixed-precision-amp-training-options).\n\n\n\n<a id='deepspeed-bs'></a>\n\n### Batch Size\n\nTo configure batch size, use:\n\n```json\n{\n    \"train_batch_size\": \"auto\",\n    \"train_micro_batch_size_per_gpu\": \"auto\"\n}\n```\n\nand the [`Trainer`] will automatically set `train_micro_batch_size_per_gpu` to the value of\n`args.per_device_train_batch_size` and `train_batch_size` to `args.world_size * args.per_device_train_batch_size * args.gradient_accumulation_steps`.\n\nYou can also set the values explicitly:\n\n```json\n{\n    \"train_batch_size\": 12,\n    \"train_micro_batch_size_per_gpu\": 4\n}\n```\n\nBut then you're on your own synchronizing the [`Trainer`] command line arguments and the DeepSpeed\nconfiguration.\n\n\n\n<a id='deepspeed-grad-acc'></a>\n\n### Gradient Accumulation\n\nTo configure gradient accumulation set:\n\n```json\n{\n    \"gradient_accumulation_steps\": \"auto\"\n}\n```\n\nand the [`Trainer`] will automatically set it to the value of `args.gradient_accumulation_steps`.\n\nYou can also set the value explicitly:",
        "question": "(your factoid question)\n",
        "answer": " \n\n```json\n{\n    \"gradient_accumulation_steps\": 2\n}\n```\n\nBut then you're on your own synchronizing the [`Trainer`] command line arguments and the DeepSpeed configuration.\n\n\n<a id='deepspeed-grad-clip'></a>\n\n### Gradient Clipping\n\nTo configure gradient clipping, use:\n\n```json\n{\n    \"gradient_clipping\": \"auto\"\n}\n```\n\nand the [`Trainer`] will automatically set it to the value of `args.max_grad_norm`.\n\nYou can also set the value explicitly:\n\n```json\n{\n    \"gradient_clipping\": 1.0\n}\n```\n\nBut then you're on your own synchronizing the [`Trainer`] command line arguments and the DeepSpeed configuration.\n\n<a id='deepspeed-grad-norm'></a>\n\n### Gradient Norm\n\nTo configure gradient norm, use:\n\n```json\n{\n    \"gradient_norm\": \"auto\"\n}\n```\n\nand the [`Trainer`] will automatically set it to the value of `args.max_grad_norm`.\n\nYou can also set the value explicitly:\n\n```json\n{\n    \"gradient_norm\": 1.0\n}\n```\n\nBut then you're on your own synchronizing the [`Trainer`] command line arguments and the DeepSpeed configuration.\n\n<a id='deepspeed-loss-scale'></a>\n\n### Loss Scaling\n\nTo configure loss scaling, use:\n\n```json\n{\n    \"loss_scaling\": \"auto\"\n}",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/main_classes/deepspeed.md"
    },
    {
        "context": "Número actual de puntos de control: ![](https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen)\n\n🤗 Transformers actualmente proporciona las siguientes arquitecturas (ver [aquí](https://huggingface.co/docs/transformers/model_summary) para un resumen de alto nivel de cada uno de ellas.):",
        "question": "¿Qué arquitecturas de modelo proporciona 🤗 Transformers?\n",
        "answer": " 🤗 Transformers proporciona las siguientes arquitecturas de modelo: BERT, RoBERTa,",
        "source_doc": "huggingface/transformers/blob/main/README_es.md"
    },
    {
        "context": "# faster 🐇\n>>> dataset = load_dataset(\"food101\")\n>>> iterable_dataset = dataset.to_iterable_dataset()\n\n# slower 🐢\n>>> iterable_dataset = load_dataset(\"food101\", streaming=True)\n```\n\nThe [`~Dataset.to_iterable_dataset`] function supports sharding when the [`IterableDataset`] is instantiated. This is useful when working with big datasets, and you'd like to shuffle the dataset or to enable fast parallel loading with a PyTorch DataLoader.\n\n```py\n>>> import torch\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"food101\")\n>>> iterable_dataset = dataset.to_iterable_dataset(num_shards=64) # shard the dataset\n>>> iterable_dataset = iterable_dataset.shuffle(buffer_size=10_000)  # shuffles the shards order and use a shuffle buffer when you start iterating\ndataloader = torch.utils.data.DataLoader(iterable_dataset, num_workers=4)  # assigns 64 / 4 = 16 shards from the shuffled list of shards to each worker when you start iterating\n```\n\n## Shuffle\n\nLike a regular [`Dataset`] object, you can also shuffle a [`IterableDataset`] with [`IterableDataset.shuffle`].\n\nThe `buffer_size` argument controls the size of the buffer to randomly sample examples from. Let's say your dataset has one million examples, and you set the `buffer_size` to ten thousand. [`IterableDataset.shuffle`] will randomly select examples from the first ten thousand examples in the buffer. Selected examples in the buffer are replaced with new examples. By default, the buffer size is 1,000.\n\n```py\n>>> from datasets import load_dataset\n>>> dataset = load_dataset('oscar', \"unshuffled_deduplicated_en\", split='train', streaming=True)\n>>> shuffled_dataset = dataset.shuffle(seed=42, buffer_size=10_000)\n```\n\n<Tip>\n\n[`IterableDataset.shuffle`] will also shuffle the order of the shards if the dataset is sharded into multiple files.\n\n</Tip>\n\n## Reshuffle",
        "question": "(your factoid question)\n",
        "answer": "::\n\n[`Dataset.shard`] shards the dataset into multiple",
        "source_doc": "huggingface/datasets/blob/main/docs/source/stream.mdx"
    },
    {
        "context": "Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_mixnet_m-0f4d8805.pth\n  Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 76.96%\n      Top 5 Accuracy: 93.16%\n- Name: tf_mixnet_s\n  In Collection: TF MixNet\n  Metadata:\n    FLOPs: 302587678\n    Parameters: 4130000\n    File Size: 16738218\n    Architecture:\n    - Batch Normalization\n    - Dense Connections\n    - Dropout\n    - Global Average Pooling\n    - Grouped Convolution\n    - MixConv\n    - Squeeze-and-Excitation Block\n    - Swish\n    Tasks:\n    - Image Classification\n    Training Techniques:\n    - MNAS\n    Training Data:\n    - ImageNet\n    ID: tf_mixnet_s\n    Crop Pct: '0.875'\n    Image Size: '224'\n    Interpolation: bicubic\n  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/efficientnet.py#L1698\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_mixnet_s-89d3354b.pth\n  Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 75.68%\n      Top 5 Accuracy: 92.64%\n-->",
        "question": "(your factoid question)\n",
        "answer": " The MixNet-S model, part of the TF MixNet collection, achieves a top-1 accuracy of 76.96% and a top-5 accuracy of 93.16% on the ImageNet dataset for the Image Classification task. Its architecture includes Batch Normalization, Dense Connections, Dropout, Global Average Pooling, Grouped Convolution, MixConv, Squeeze-and-Excitation Block, and Swish. The model has 41,3000 parameters and 302,587,678 FLOPs, with a file size of 16.74 MB. The model was trained using the MNAS training technique on the ImageNet dataset for the Image Classification task. The MixNet-S model's weights can be found at this link: <https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_mixnet_s-89d3354b.pth>. Additionally, the code for the MixNet-S model is available at this link: <https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/efficientnet.py#L1698>.",
        "source_doc": "huggingface/pytorch-image-models/blob/main/hfdocs/source/models/tf-mixnet.mdx"
    },
    {
        "context": "1. **[IDEFICS](https://huggingface.co/docs/transformers/model_doc/idefics)** (from HuggingFace) released with the paper [OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents](https://huggingface.co/papers/2306.16527) by Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, Victor Sanh. \n1. **[ImageGPT](https://huggingface.co/docs/transformers/model_doc/imagegpt)** (from OpenAI) released with the paper [Generative Pretraining from Pixels](https://openai.com/blog/image-gpt/) by Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, Ilya Sutskever.\n1. **[Informer](https://huggingface.co/docs/transformers/model_doc/informer)** (from Beihang University, UC Berkeley, Rutgers University, SEDD Company) released with the paper [Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting](https://arxiv.org/abs/2012.07436) by Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.\n1. **[InstructBLIP](https://huggingface.co/docs/transformers/model_doc/instructblip)** (from Salesforce) released with the paper [InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://arxiv.org/abs/2305.06500) by Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi.\n1. **[Jukebox](https://huggingface.co/docs/transformers/model_doc/jukebox)** (from OpenAI) released with the paper [Jukebox: A Generative Model for Music](https://arxiv.org/pdf/2005.00341.pdf) by Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, Ilya Sutskever.",
        "question": "Which model was released by OpenAI with the paper \"Generative Pretraining from Pixels\"?\n",
        "answer": " ImageGPT",
        "source_doc": "huggingface/transformers/blob/main/README_zh-hant.md"
    },
    {
        "context": "<div class=\"SVELTE_HYDRATER \" data-props=\"{&quot;apiUrl&quot;:&quot;https://api-inference.huggingface.co&quot;,&quot;model&quot;:{&quot;author&quot;:&quot;spacy&quot;,&quot;autoArchitecture&quot;:&quot;AutoModel&quot;,&quot;branch&quot;:&quot;main&quot;,&quot;cardData&quot;:{&quot;tags&quot;:[&quot;spacy&quot;,&quot;token-classification&quot;],&quot;language&quot;:[&quot;en&quot;],&quot;license&quot;:&quot;MIT&quot;,&quot;model-index&quot;:[{&quot;name&quot;:&quot;en_core_web_sm&quot;,&quot;results&quot;:[{&quot;tasks&quot;:{&quot;name&quot;:&quot;NER&quot;,&quot;type&quot;:&quot;token-classification&quot;,&quot;metrics&quot;:[{&quot;name&quot;:&quot;Precision&quot;,&quot;type&quot;:&quot;precision&quot;,&quot;value&quot;:0.8424355924},{&quot;name&quot;:&quot;Recall&quot;,&quot;type&quot;:&quot;recall&quot;,&quot;value&quot;:0.8335336538},{&quot;name&quot;:&quot;F Score&quot;,&quot;type&quot;:&quot;f_score&quot;,&quot;value&quot;:0.8379609817}]}},{&quot;tasks&quot;:{&quot;name&quot;:&quot;POS&quot;,&quot;type&quot;:&quot;token-classification&quot;,&quot;metrics&quot;:[{&quot;name&quot;:&quot;Accuracy&quot;,&quot;type&quot;:&quot;accuracy&quot;,&quot;value&quot;:0.9720712187}]}},{&quot;tasks&quot;:{&quot;name&quot;:&quot;SENTER&quot;,&quot;type&quot;:&quot;token-classification&quot;,&quot;metrics&quot;:[{&quot;name&quot;:&quot;Precision&quot;,&quot;type&quot;:&quot;precision&quot;,&quot;value&quot;:0.9074955788},{&quot;name&quot;:&quot;Recall&quot;,&quot;type&quot;:&quot;recall&quot;,&quot;value&quot;:0.8801372122},{&quot;name&quot;:&quot;F Score&quot;,&quot;type&quot;:&quot;f_score&quot;,&quot;value&quot;:0.893607046}]}},{&quot;tasks&quot;:{&quot;name&quot;:&quot;UNLABELED_DEPENDENCIES&quot;,&quot;type&quot;:&quot;token-classification&quot;,&quot;metrics&quot;:[{&quot;name&quot;:&quot;Accuracy&quot;,&quot;type&quot;:&quot;accuracy&quot;,&quot;value&quot;:0",
        "question": "What is the precision of the NER model?\n",
        "answer": " The precision of the NER model is 0.84243592.",
        "source_doc": "huggingface/blog/blob/main/spacy.md"
    },
    {
        "context": "## Additional Resources\n\n- [Models](https://huggingface.co/models?other=falcon&sort=trending&search=180)\n- [Demo](https://huggingface.co/spaces/tiiuae/falcon-180b-chat)\n- [The Falcon has landed in the Hugging Face ecosystem](https://huggingface.co/blog/falcon)\n- [Official Announcement](https://falconllm.tii.ae/)\n\n## Acknowledgments\n\nReleasing such a model with support and evaluations in the ecosystem would not be possible without the contributions of many community members, including [Clémentine](https://huggingface.co/clefourrier) and [Eleuther Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness) for LLM evaluations; [Loubna](https://huggingface.co/loubnabnl) and [BigCode](https://huggingface.co/bigcode) for code evaluations; [Nicolas](https://hf.co/narsil) for Inference support; [Lysandre](https://huggingface.co/lysandre), [Matt](https://huggingface.co/Rocketknight1), [Daniel](https://huggingface.co/DanielHesslow), [Amy](https://huggingface.co/amyeroberts), [Joao](https://huggingface.co/joaogante), and [Arthur](https://huggingface.co/ArthurZ) for integrating Falcon into transformers. Thanks to [Baptiste](https://huggingface.co/BapBap) and [Patrick](https://huggingface.co/patrickvonplaten) for the open-source demo. Thanks to [Thom](https://huggingface.co/thomwolf), [Lewis](https://huggingface.co/lewtun), [TheBloke](https://huggingface.co/thebloke), [Nouamane](https://huggingface.co/nouamanetazi), [Tim Dettmers](https://huggingface.co/timdettmers) for multiple contributions enabling this to get out. Finally, thanks to the HF Cluster for enabling running LLM evaluations as well as providing inference for a free, open-source demo of the model.",
        "question": "(your factoid question)\n",
        "answer": " (your answer to the factoid question)\n\nNow here is the context.\n\nContext: ## Additional Resources\n\n- [Models](https://huggingface.co/models?other=falcon&sort=trending&search=180)\n- [Demo](https://huggingface.co/spaces/tiiuae/falcon-180b-chat)\n- [The Falcon has landed in the Hugging Face ecosystem](https://huggingface.co/blog/falcon)\n- [Official Announcement](https://falconllm.tii.ae/)\n\n## Acknowledgments\n\nReleasing such a model with support and evaluations in the ecosystem would not be possible without the contributions of many community members, including [Clémentine](https://huggingface.co/clefourrier) and [Eleuther Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness) for LLM evaluations; [Loubna](https://huggingface.co/loubnabnl) and [BigCode](https://huggingface.co/bigcode) for code evaluations; [Nicolas](https://hf.co/narsil) for Inference support; [Lysandre](https://huggingface.co/lysandre), [Matt](https://huggingface.co/Rocketknight1), [Daniel](https://huggingface.co/DanielHesslow), [Amy](https://huggingface.co/amyeroberts), [Joao](https://huggingface.co/joaogante), and [Arthur](https://huggingface.co/ArthurZ) for integrating Falcon into transformers. Thanks to [Baptiste](https://huggingface.co/BapBap) and [Patrick](https://huggingface.co/patrickvonplaten) for the open-source demo. Thanks to [Thom](https://huggingface.co/thomwolf), [Lewis](https://huggingface.co/lewtun), [TheBloke](https://huggingface.co/thebloke), [Nouamane](https://huggingface.co/nouamanetazi), [Tim Dettmers](https://huggingface.co/timdettmers) for multiple contributions enabling this to get out. Finally, thanks to the HF Cluster for enabling running LLM evaluations as well as providing inference for a free, open-source demo of the model.\n\nOutput:::\n\nQ: Who contributed to the release of the Falcon model in the Hugging Face ecosystem?\n\nA: The release of the Falcon model in the Hugging Face ecosystem was made possible by contributions from Clémentine, Eleuther Evaluation Harness for LLM evaluations, Loubna and BigCode for code evaluations, Nicolas for Inference support, Lysandre, Matt, DanielHesslow, Amyeroberts, Joao, Arthur, BapBap, Patrickvonplaten, Thomwolf, Lewtun, Thebloke, Nouamanetazi, Tim Dettmers, and the Hugging Face Cluster for running LLM evaluations and providing inference for a free, open-source demo of the model.",
        "source_doc": "huggingface/blog/blob/main/falcon-180b.md"
    },
    {
        "context": "<figure class=\"image table text-center m-0 w-full\">\n  <img src=\"assets/65_series_c/team.png\" alt=\"The Home of Machine Learning\"/>\n</figure>\n\nIt's been a hell of a ride to grow from 30 to 120+ team members in the past 12 months. We were super lucky to have been joined by incredibly talented (and fun!) teammates like [Dr. Margaret Mitchell](https://www.bloomberg.com/news/articles/2021-08-24/fired-at-google-after-critical-work-ai-researcher-mitchell-to-join-hugging-face) and the [Gradio team](https://gradio.app/joining-huggingface/), and we don't plan to stop here. We're [hiring for every position](https://apply.workable.com/huggingface) you can think of for every level of seniority. We are a remote-friendly, decentralized organization with transparency and value-inspired decision making by default.\n\nHuge thanks to every contributor in our amazing community and team, our customers, partners, and investors for helping us reach this point. We couldn't have done it without you, and we can't wait to work together with you on what's next. Your contributions are key to helping build a better future where AI is founded on open source, open science, ethics and collaboration.\n\n---\n\n*For press inquiries, please contact <a href=\"mailto:team@huggingface.co\">team@huggingface.co</a>*",
        "question": "How many team members has Hugging Face grown to in the past 12 months?\n",
        "answer": " Hugging Face has grown to over 120 team members in the past 12 months.",
        "source_doc": "huggingface/blog/blob/main/series-c.md"
    },
    {
        "context": "1. **[CodeLlama](https://huggingface.co/docs/transformers/model_doc/llama_code)** (from MetaAI) released with the paper [Code Llama: Open Foundation Models for Code](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/) by Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, Gabriel Synnaeve.\n1. **[Conditional DETR](https://huggingface.co/docs/transformers/model_doc/conditional_detr)** (from Microsoft Research Asia) released with the paper [Conditional DETR for Fast Training Convergence](https://arxiv.org/abs/2108.06152) by Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan, Lei Sun, Jingdong Wang.\n1. **[ConvBERT](https://huggingface.co/docs/transformers/model_doc/convbert)** (from YituTech) released with the paper [ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://arxiv.org/abs/2008.02496) by Zihang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng Yan.\n1. **[ConvNeXT](https://huggingface.co/docs/transformers/model_doc/convnext)** (from Facebook AI) released with the paper [A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545) by Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie.\n1. **[ConvNeXTV2](https://huggingface.co/docs/transformers/model_doc/convnextv2)** (from Facebook AI) released with the paper [ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders](https://arxiv.org/abs/2301.00808) by Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, Saining Xie.",
        "question": "Which model was released by MetaAI with the paper 'Code Llama: Open Foundation Models for Code'?\n",
        "answer": " CodeLlama",
        "source_doc": "huggingface/transformers/blob/main/README_te.md"
    },
    {
        "context": "```python\nseed = 0\ngenerator = torch.manual_seed(seed)\n\nimages = sd_pipeline(prompts, num_images_per_prompt=1, generator=generator, output_type=\"np\").images\n```\n\nThen we load the [v1-5 checkpoint](https://huggingface.co/runwayml/stable-diffusion-v1-5) to generate images:\n\n```python\nmodel_ckpt_1_5 = \"runwayml/stable-diffusion-v1-5\"\nsd_pipeline_1_5 = StableDiffusionPipeline.from_pretrained(model_ckpt_1_5, torch_dtype=weight_dtype).to(device)\n\nimages_1_5 = sd_pipeline_1_5(prompts, num_images_per_prompt=1, generator=generator, output_type=\"np\").images\n```\n\nAnd finally, we compare their CLIP scores:\n\n```python\nsd_clip_score_1_4 = calculate_clip_score(images, prompts)\nprint(f\"CLIP Score with v-1-4: {sd_clip_score_1_4}\")\n# CLIP Score with v-1-4: 34.9102\n\nsd_clip_score_1_5 = calculate_clip_score(images_1_5, prompts)\nprint(f\"CLIP Score with v-1-5: {sd_clip_score_1_5}\")\n# CLIP Score with v-1-5: 36.2137\n```\n\nIt seems like the [v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) checkpoint performs better than its predecessor. Note, however, that the number of prompts we used to compute the CLIP scores is quite low. For a more practical evaluation, this number should be way higher, and the prompts should be diverse.\n\n<Tip warning={true}>\n\nBy construction, there are some limitations in this score. The captions in the training dataset\nwere crawled from the web and extracted from `alt` and similar tags associated an image on the internet.\nThey are not necessarily representative of what a human being would use to describe an image. Hence we\nhad to \"engineer\" some prompts here.\n\n</Tip>\n\n### Image-conditioned text-to-image generation\n\nIn this case, we condition the generation pipeline with an input image as well as a text prompt. Let's take the [`StableDiffusionInstructPix2PixPipeline`], as an example. It takes an edit instruction as an input prompt and an input image to be edited.\n\nHere is one example:",
        "question": "(your factoid question)\n",
        "answer": " (your answer to the factoid question)\n\nNow here is the context.\n\nContext: ```python\nseed = 0\ngenerator = torch.manual_seed(seed)\n\nimages = sd_pipeline(prompts, num_images_per_prompt=1, generator=generator, output_type=\"np\").images\n```\n\nThen we load the [v1-5 checkpoint](https://huggingface.co/runwayml/stable-diffusion-v1-5) to generate images:\n\n```python\nmodel_ckpt_1_5 = \"runwayml/stable-diffusion-v1-5\"\nsd_pipeline_1_5 = StableDiffusionPipeline.from_pretrained(model_ckpt_1_5, torch_dtype=weight_dtype).to(device)\n\nimages_1_5 = sd_pipeline_1_5(prompts, num_images_per_prompt=1, generator=generator, output_type=\"np\").images\n```\n\nAnd finally, we compare their CLIP scores:\n\n```python\nsd_clip_score_1_4 = calculate_clip_score(images, prompts)\nprint(f\"CLIP Score with v-1-4: {sd_clip_score_1_4}\")\n# CLIP Score with v-1-4: 34.9102\n\nsd_clip_score_1_5 = calculate_clip_score(images_1_5, prompts)\nprint(f\"CLIP Score with v-1-5: {sd_clip_score_1_5}\")\n# CLIP Score with v-1-5: 36.2137\n```\n\nIt seems like the [v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) checkpoint performs better than its predecessor. Note, however, that the number of prompts we used to compute the CLIP scores is quite low. For a more practical evaluation, this number should be way higher, and the prompts should be diverse.\n\n<Tip warning={true}>\n\nBy construction, there are some limitations in this score. The captions in the training dataset\nwere crawled from the web and extracted from `alt` and similar tags associated an image on the internet.\nThey are not necessarily representative of what a human being would use to describe an image. Hence we\nhad to \"engineer\" some prompts here.\n\n</Tip>\n\n### Image-conditioned text-to-image generation\n\nIn this case, we condition the generation pipeline with an input image as well as a text prompt. Let's take the [`StableDiffusionInstructPix2PixPipeline`], as an example. It takes an edit instruction as an input prompt and an input image to be edited.\n\nHere is one example:\n\nOutput:::\n\n![image-conditioned-text-to-image-generation](https://huggingface.co/frontline-research-group/stable-diffusion-instruct-pix2pix/resolve/main/docs/images/image-conditioned-text-to-image-generation.png)\n\nImage-conditioned text-to-image generation\n\nThe prompt is: \"Add a red apple on the table\"\n\nThe input image is:\n\n![input-image](https://huggingface.co/frontline-research-group/stable-diffusion-instruct-pix2pix/resolve/main/docs/images/input-image.png)\n\nInput image\n\nThe output image is:\n\n![output-image](https://huggingface.co/frontline-research-group/stable-diffusion-instruct-pix2pix/resolve/main/docs/images/output-image.png)\n\nOutput image\n\n### Text-to-image generation\n\nIn this case, we generate an image from a text prompt only. Let's take the [`StableDiffusionInstructPix2PixPipeline`], as an example. It takes a text prompt as an input and generates an image.\n\nHere is one example:\n\nOutput:::\n\n![image-from-text](https://huggingface.co/frontline-research-group/stable-diffusion-instruct-pix/resolve/main/docs/images/image-conditioned-text-to-image-generation](https://huggingface.co/frontline-research-group/stable-diffusion-instruct-pix2pix/resolve/main/docs/images.png)\n\n![text-to-image-gener",
        "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/evaluation.md"
    },
    {
        "context": "!---\nCopyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n# Question answering\n\nThis folder contains several scripts that showcase how to fine-tune a 🤗 Transformers model on a question answering dataset,\nlike SQuAD. \n\n## Trainer-based scripts\n\nThe [`run_qa.py`](https://github.com/huggingface/transformers/blob/main/examples/pytorch/question-answering/run_qa.py),\n[`run_qa_beam_search.py`](https://github.com/huggingface/transformers/blob/main/examples/pytorch/question-answering/run_qa_beam_search.py) and [`run_seq2seq_qa.py`](https://github.com/huggingface/transformers/blob/main/examples/pytorch/question-answering/run_seq2seq_qa.py) leverage the 🤗 [Trainer](https://huggingface.co/transformers/main_classes/trainer.html) for fine-tuning.\n\n### Fine-tuning BERT on SQuAD1.0\n\nThe [`run_qa.py`](https://github.com/huggingface/transformers/blob/main/examples/pytorch/question-answering/run_qa.py) script\nallows to fine-tune any model from our [hub](https://huggingface.co/models) (as long as its architecture has a `ForQuestionAnswering` version in the library) on a question-answering dataset (such as SQuAD, or any other QA dataset available in the `datasets` library, or your own csv/jsonlines files) as long as they are structured the same way as SQuAD. You might need to tweak the data processing inside the script if your data is structured differently.",
        "question": "(your factoid question)\n",
        "answer": "\n\nThe context is about the BERT model, which is fine-tuning the data is not in the right",
        "source_doc": "huggingface/transformers/blob/main/examples/pytorch/question-answering/README.md"
    },
    {
        "context": "1. **[ViTMAE](https://huggingface.co/docs/transformers/model_doc/vit_mae)** (Meta AI 에서) Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick 의 [Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377) 논문과 함께 발표했습니다.\n1. **[ViTMatte](https://huggingface.co/docs/transformers/model_doc/vitmatte)** (HUST-VL 에서 제공)은 Jingfeng Yao, Xinggang Wang, Shusheng Yang, Baoyuan Wang.의 [ViTMatte: Boosting Image Matting with Pretrained Plain Vision Transformers](https://arxiv.org/abs/2305.15272)논문과 함께 발표했습니다.\n1. **[ViTMSN](https://huggingface.co/docs/transformers/model_doc/vit_msn)** (Meta AI 에서) Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes, Pascal Vincent, Armand Joulin, Michael Rabbat, Nicolas Ballas 의 [Masked Siamese Networks for Label-Efficient Learning](https://arxiv.org/abs/2204.07141) 논문과 함께 발표했습니다.\n1. **[VITS](https://huggingface.co/docs/transformers/model_doc/vits)** (Kakao Enterprise 에서 제공)은 Jaehyeon Kim, Jungil Kong, Juhee Son.의 [Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech](https://arxiv.org/abs/2106.06103)논문과 함께 발표했습니다.\n1. **[ViViT](https://huggingface.co/docs/transformers/model_doc/vivit)** (from Google Research) released with the paper [ViViT: A Video Vision Transformer](https://arxiv.org/abs/2103.15691) by Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lučić, Cordelia Schmid.\n1. **[Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/wav2vec2)** (Facebook AI 에서) Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli 의 [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477) 논문과 함께 발표했습니다.",
        "question": "Who are the authors of the paper \"ViTMSN: Masked Siamese Networks for Label-Efficient Learning\"?\n\n",
        "answer": " The authors of the paper \"ViTMSN: Masked Siamese Networks for Label-Efficient Learning\" are Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick.",
        "source_doc": "huggingface/transformers/blob/main/README_ko.md"
    },
    {
        "context": "```py\n>>> from transformers import AutoImageProcessor\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n```\n\nFirst, let's add some image augmentation. You can use any library you prefer, but in this tutorial, we'll use torchvision's [`transforms`](https://pytorch.org/vision/stable/transforms.html) module. If you're interested in using another data augmentation library, learn how in the [Albumentations](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_albumentations.ipynb) or [Kornia notebooks](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_kornia.ipynb).\n\n1. Here we use [`Compose`](https://pytorch.org/vision/master/generated/torchvision.transforms.Compose.html) to chain together a couple of\ntransforms - [`RandomResizedCrop`](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html) and [`ColorJitter`](https://pytorch.org/vision/main/generated/torchvision.transforms.ColorJitter.html).\nNote that for resizing, we can get the image size requirements from the `image_processor`. For some models, an exact height and\nwidth are expected, for others only the `shortest_edge` is defined.\n\n```py\n>>> from torchvision.transforms import RandomResizedCrop, ColorJitter, Compose\n\n>>> size = (\n...     image_processor.size[\"shortest_edge\"]\n...     if \"shortest_edge\" in image_processor.size\n...     else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n... )\n\n>>> _transforms = Compose([RandomResizedCrop(size), ColorJitter(brightness=0.5, hue=0.5)])\n```",
        "question": "(your factoid question)\n",
        "answer": "\n\nThe `from_pretrained` method takes in a model name and a pretrained model path and returns an `AutoImageProcessor` class with the pretrained model loaded.\n\n```vbnet\n>>> from torchvision.transforms import Compose\n\n>>> transforms = Compose([RandomResizedCrop, ColorJitter(brightness=0.5, hue=0.5)])\n\n>>> _transforms = Compose([RandomResizedCrop",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/preprocessing.md"
    },
    {
        "context": "1. **[Nougat](https://huggingface.co/docs/transformers/model_doc/nougat)** (from Meta AI) released with the paper [Nougat: Neural Optical Understanding for Academic Documents](https://arxiv.org/abs/2308.13418) by Lukas Blecher, Guillem Cucurull, Thomas Scialom, Robert Stojnic.\n1. **[Nyströmformer](https://huggingface.co/docs/transformers/model_doc/nystromformer)** (from the University of Wisconsin - Madison) released with the paper [Nyströmformer: A Nyström-Based Algorithm for Approximating Self-Attention](https://arxiv.org/abs/2102.03902) by Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, Vikas Singh.\n1. **[OneFormer](https://huggingface.co/docs/transformers/model_doc/oneformer)** (from SHI Labs) released with the paper [OneFormer: One Transformer to Rule Universal Image Segmentation](https://arxiv.org/abs/2211.06220) by Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov, Humphrey Shi.\n1. **[OpenLlama](https://huggingface.co/docs/transformers/model_doc/open-llama)** (from [s-JoL](https://huggingface.co/s-JoL)) released on GitHub (now removed).\n1. **[OPT](https://huggingface.co/docs/transformers/master/model_doc/opt)** (from Meta AI) released with the paper [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068) by Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen et al.\n1. **[OWL-ViT](https://huggingface.co/docs/transformers/model_doc/owlvit)** (from Google AI) released with the paper [Simple Open-Vocabulary Object Detection with Vision Transformers](https://arxiv.org/abs/2205.06230) by Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby.",
        "question": "Why was the OPT model released on GitHub (now removed)?\n\n",
        "answer": " The OPT model was released on GitHub (now removed) is a pre-trained transformer language model.",
        "source_doc": "huggingface/transformers/blob/main/README_zh-hant.md"
    },
    {
        "context": "### Breaking Changes:\n\nNo changes to highlight.\n\n### Full Changelog:\n\n- Prevent in-place updates of `generic_update` by shallow copying by [@gitgithan](https://github.com/gitgithan) in [PR 3405](https://github.com/gradio-app/gradio/pull/3405) to fix [#3282](https://github.com/gradio-app/gradio/issues/3282)\n- Persist file names of files uploaded through any Gradio component by [@abidlabs](https://github.com/abidlabs) in [PR 3412](https://github.com/gradio-app/gradio/pull/3412)\n- Fix markdown embedded component in docs by [@aliabd](https://github.com/aliabd) in [PR 3410](https://github.com/gradio-app/gradio/pull/3410)\n- Clean up event listeners code by [@aliabid94](https://github.com/aliabid94) in [PR 3420](https://github.com/gradio-app/gradio/pull/3420)\n- Fix css issue with spaces logo by [@aliabd](https://github.com/aliabd) in [PR 3422](https://github.com/gradio-app/gradio/pull/3422)\n- Makes a few fixes to the `JSON` component (show_label parameter, icons) in [@abidlabs](https://github.com/abidlabs) in [PR 3451](https://github.com/gradio-app/gradio/pull/3451)\n\n### Contributors Shoutout:\n\nNo changes to highlight.\n\n## 3.20.1\n\n### New Features:\n\n- Add `height` kwarg to style in `gr.Chatbot()` component by [@dawoodkhan82](https://github.com/dawoodkhan82) in [PR 3369](https://github.com/gradio-app/gradio/pull/3369)\n\n```python\nchatbot = gr.Chatbot().style(height=500)\n```\n\n### Bug Fixes:\n\n- Ensure uploaded images are always shown in the sketch tool by [@pngwn](https://github.com/pngwn) in [PR 3386](https://github.com/gradio-app/gradio/pull/3386)\n- Fixes bug where when if fn is a non-static class member, then self should be ignored as the first param of the fn by [@or25](https://github.com/or25) in [PR #3227](https://github.com/gradio-app/gradio/pull/3227)\n\n### Documentation Changes:\n\nNo changes to highlight.\n\n### Testing and Infrastructure Changes:\n\nNo changes to highlight.\n\n### Breaking Changes:\n\nNo changes to highlight.\n\n### Full Changelog:\n\nNo changes to highlight.",
        "question": "(your factoid question)\n",
        "answer": " (your answer to the factoid question)\n\nNow here is the context.\n\nContext: ### Breaking Changes:\n\nNo changes to highlight.\n\n### Full Changelog:\n\n- Prevent in-place updates of `generic_update` by shallow copying by [@gitgithan](https://github.com/gitgithan) in [PR 3405](https://github.com/gradio-app/gradio/pull/3405) to fix [#3282](https://github.com/gradio-app/gradio/issues/3282)\n- Persist file names of files uploaded through any Gradio component by [@abidlabs](https://github.com/abidlabs) in [PR 3412](https://github.com/gradio-app/gradio/pull/3412)\n- Fix markdown embedded component in docs by [@aliabd](https://github.com/aliabd) in [PR 3410](https://github.com/gradio-app/gradio/pull/3410)\n- Clean up event listeners code by [@aliabid94](https://github.com/aliabid94) in [PR 3420](https://github.com/gradio-app/gradio/pull/3420)\n- Fix css issue with spaces logo by [@aliabd](https://github.com/aliabd) in [PR 3422](https://github.com/gradio-app/gradio/pull/3422)\n- Makes a few fixes to the `JSON` component (show_label parameter, icons) in [@abidlabs](https://github.com/abidlabs) in [PR 3451](https://github.com/gradio-app/gradio/pull/3451)\n\n### Contributors Shoutout:\n\nNo changes to highlight.\n\n## 3.20.1\n\n### New Features:\n\n- Add `height` kwarg to style in `gr.Chatbot()` component by [@dawoodkhan82](https://github.com/dawoodkhan82) in [PR 3369](https://github.com/gradio-app/gradio/pull/3369)\n\n```python\nchatbot = gr.Chatbot().style(height=500)\n```\n\n### Bug Fixes:\n\n- Ensure uploaded images are always shown in the sketch tool by [@pngwn](https://github.com/pngwn) in [PR 3386](https://github.com/gradio-app/gradio/pull/3386)\n- Fixes bug where when if fn is a non-static class member, then self should be ignored as the first param of the fn by [@or25](https://github.com/or25) in [PR #3227](https://github.com/gradio-app/gradio/pull/3227)\n\n### Documentation Changes:\n\nNo changes to highlight.\n\n### Testing and Infrastructure Changes:\n\nNo changes to highlight.\n\n### Breaking Changes:\n\nNo changes to highlight.\n\n### Full Changelog:\n\nNo changes to highlight.\n\nOutput:::\n\nNo changes to highlight.\n\n### Full Changelog:\n\nNo changes to highlight.\n\n### Full Changelog:\n\nNo changes to highlight.\n\n### Full Changelog:\n\nNo changes to highlight.\n\n### Full Changelog:\n\nNo changes to highlight.\n\n### Full Changelog:\n\nNo changes to highlight.\n\n### Full Changelog:\n\nNo changes to highlight.\n\n### Full Changelog:\n\nNo changes to highlight.\n\n### Full Changelog:\n\nNo changes to highlight.\n\n### Full Changelog:\n\nNo changes to highlight.\n\n### Full Changelog:\n\nNo changes to highlight.\n\n### Full Changelog:\n\nNo changes to highlight.\n\n### Full Changelog:\n\nNo changes to highlight.\n\n### Full Changelog:\n\nNo changes to highlight.\n\n### Full Changelog:\n\nNo changes to highlight.\n\n### Full Changelog:\n\nNo changes to highlight.\n\n### Full Changelog:\n\nNo changes to highlight.\n\n### Full Changelog:\n\nNo changes to highlight.\n\n### Full Changelog:\n\nNo changes to highlight.\n\n### Full Changelog:\n\nNo changes to highlight.\n\n### Full Changelog:\n\nNo changes to highlight.\n\n### Full Changelog:\n\nNo changes to highlight.\n\n### Full Changelog:\n\nNo changes to highlight.\n\n### Full Changelog:\n\nNo changes to highlight.\n\n### Full Changelog:\n\nNo changes to highlight.\n\n### Full Changelog:\n\nNo changes to highlight.\n\n### Full Changelog:\n\nNo changes to highlight.\n\n###",
        "source_doc": "gradio-app/gradio/blob/main/gradio/CHANGELOG.md"
    },
    {
        "context": "```python\nmodel.eval()\ni = 33\ninputs = tokenizer(f'{text_column} : {dataset[\"test\"][i][\"Tweet text\"]} Label : ', return_tensors=\"pt\")\nprint(dataset[\"test\"][i][\"Tweet text\"])\nprint(inputs)\n\nwith torch.no_grad():\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    outputs = model.generate(\n        input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_new_tokens=10, eos_token_id=3\n    )\n    print(outputs)\n    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))\n```\n\nYou can push model to hub or save model locally. \n\n- Option1: Pushing the model to Hugging Face Hub\n```python\nmodel.push_to_hub(\n    f\"{dataset_name}_{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\".replace(\"/\", \"_\"),\n    token = \"hf_...\"\n)\n```\ntoken (`bool` or `str`, *optional*):\n    `token` is to be used for HTTP Bearer authorization when accessing remote files. If `True`, will use the token generated\n    when running `huggingface-cli login` (stored in `~/.huggingface`). Will default to `True` if `repo_url`\n    is not specified.\n    Or you can get your token from https://huggingface.co/settings/token\n```\n- Or save model locally\n```python\npeft_model_id = f\"{dataset_name}_{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\".replace(\"/\", \"_\")\nmodel.save_pretrained(peft_model_id)\n```\n\n\n```python\n# saving model\npeft_model_id = f\"{dataset_name}_{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\".replace(\n    \"/\", \"_\"\n)\nmodel.save_pretrained(peft_model_id)\n```\n\n\n```python\nckpt = f\"{peft_model_id}/adapter_model.bin\"\n!du -h $ckpt\n```\n\n\n```python\nfrom peft import PeftModel, PeftConfig\n\npeft_model_id = f\"{dataset_name}_{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\".replace(\n    \"/\", \"_\"\n)",
        "question": "What is the purpose of the `save_pretrained` function?\n\n",
        "answer": " The `save_pretrained` function saves a pretrained model locally, which can be useful for",
        "source_doc": "huggingface/peft/blob/main/examples/causal_language_modeling/peft_prompt_tuning_clm.ipynb"
    },
    {
        "context": "@gradio/dataframe\n\n## 0.4.3\n\n### Patch Changes\n\n- Updated dependencies [[`846d52d`](https://github.com/gradio-app/gradio/commit/846d52d1c92d429077382ce494eea27fd062d9f6), [`828fb9e`](https://github.com/gradio-app/gradio/commit/828fb9e6ce15b6ea08318675a2361117596a1b5d), [`f3abde8`](https://github.com/gradio-app/gradio/commit/f3abde80884d96ad69b825020c46486d9dd5cac5), [`73268ee`](https://github.com/gradio-app/gradio/commit/73268ee2e39f23ebdd1e927cb49b8d79c4b9a144)]:\n  - @gradio/markdown@0.6.0\n  - @gradio/statustracker@0.4.3\n  - @gradio/atoms@0.4.1\n  - @gradio/upload@0.5.6\n  - @gradio/button@0.2.13\n\n## 0.4.2\n\n### Patch Changes\n\n- Updated dependencies []:\n  - @gradio/button@0.2.12\n  - @gradio/upload@0.5.5\n\n## 0.4.1\n\n### Patch Changes\n\n- Updated dependencies [[`5d51fbc`](https://github.com/gradio-app/gradio/commit/5d51fbce7826da840a2fd4940feb5d9ad6f1bc5a)]:\n  - @gradio/upload@0.5.4\n  - @gradio/button@0.2.11\n\n## 0.4.0\n\n### Features\n\n- [#6603](https://github.com/gradio-app/gradio/pull/6603) [`6b1401c`](https://github.com/gradio-app/gradio/commit/6b1401c514c2ec012b0a50c72a6ec81cb673bf1d) - chore(deps): update dependency marked to v11. Thanks [@renovate](https://github.com/apps/renovate)!\n\n## 0.3.11\n\n### Patch Changes\n\n- Updated dependencies []:\n  - @gradio/atoms@0.3.1\n  - @gradio/statustracker@0.4.1\n  - @gradio/upload@0.5.2\n  - @gradio/markdown@0.4.1\n  - @gradio/button@0.2.9\n\n## 0.3.10\n\n### Patch Changes\n\n- Updated dependencies [[`6d3fecfa4`](https://github.com/gradio-app/gradio/commit/6d3fecfa42dde1c70a60c397434c88db77289be6)]:\n  - @gradio/markdown@0.4.0\n\n## 0.3.9\n\n### Patch Changes\n\n- Updated dependencies [[`46f13f496`](https://github.com/gradio-app/gradio/commit/46f13f4968c8177e318c9d75f2eed1ed55c2c042)]:\n  - @gradio/markdown@0.3.4\n  - @gradio/button@0.2.8\n  - @gradio/upload@0.5.1\n\n## 0.3.8\n\n### Patch Changes",
        "question": "(your factoid question)\n",
        "answer": " The dependencies of the Gradio",
        "source_doc": "gradio-app/gradio/blob/main/js/dataframe/CHANGELOG.md"
    },
    {
        "context": "Getting Started\n\n## Welcome\n\nWelcome to the `timm` documentation, a lean set of docs that covers the basics of `timm`.\n\nFor a more comprehensive set of docs (currently under development), please visit [timmdocs](http://timm.fast.ai) by [Aman Arora](https://github.com/amaarora).\n\n## Install\n\nThe library can be installed with pip:\n\n```\npip install timm\n```\n\nI update the PyPi (pip) packages when I'm confident there are no significant model regressions from previous releases. If you want to pip install the bleeding edge from GitHub, use:\n```\npip install git+https://github.com/rwightman/pytorch-image-models.git\n```\n\n!!! info \"Conda Environment\"\n    All development and testing has been done in Conda Python 3 environments on Linux x86-64 systems, specifically 3.7, 3.8, 3.9, 3.10\n    \n    Little to no care has been taken to be Python 2.x friendly and will not support it. If you run into any challenges running on Windows, or other OS, I'm definitely open to looking into those issues so long as it's in a reproducible (read Conda) environment.\n    \n    PyTorch versions 1.9, 1.10, 1.11 have been tested with the latest versions of this code.\n    \n    I've tried to keep the dependencies minimal, the setup is as per the PyTorch default install instructions for Conda:\n    ```\n    conda create -n torch-env\n    conda activate torch-env\n    conda install pytorch torchvision cudatoolkit=11.3 -c pytorch\n    conda install pyyaml\n    ```\n\n## Load a Pretrained Model\n\nPretrained models can be loaded using `timm.create_model`\n\n```python\nimport timm\n\nm = timm.create_model('mobilenetv3_large_100', pretrained=True)\nm.eval()\n```\n\n## List Models with Pretrained Weights\n```python\nimport timm\nfrom pprint import pprint\nmodel_names = timm.list_models(pretrained=True)\npprint(model_names)\n>>> ['adv_inception_v3',\n 'cspdarknet53',\n 'cspresnext50',\n 'densenet121',\n 'densenet161',\n 'densenet169',\n 'densenet201',\n 'densenetblur121d',\n 'dla34',\n 'dla46_c',\n...\n]\n```",
        "question": "What are the names of the pretrained models available in the timm library?\n",
        "answer": " The pretrained models available in the timm library are:\n\n'adv_inception_v3',\n 'cspdarknet53',\n 'cspresnext50',\n 'densenet121',\n 'densenet169',\n 'densenet201',\n 'densenetblur121d',\n 'dla34',\n 'dla46_c',\n 'dla46_mobilenet_v2',\n 'dla46_mobilenet_v2',\n 'dla46_mobilenet_v3',\n 'dla46_resnet',\n 'dla46_resnext50',\n 'dla46_resnext50',\n 'dla46_resnext101',\n 'dla46_resnext50',\n 'dla46_resnext101',\n 'dla46_resnext50',\n 'dla46_resnext101',\n 'dla46_resnext152',\n 'dla46_resnext269',\n 'dla46_resnext50',\n 'dla46_resnext50',\n 'dla46_resnext101',\n 'dla46_resnext152',\n 'dla46_resnext269',\n 'dla46_resnext50',\n 'dla46_resnext101',\n 'dla46_resnext152',\n 'dla46_resnext",
        "source_doc": "huggingface/pytorch-image-models/blob/main/docs/index.md"
    },
    {
        "context": "## Acknowledgements\n\nWe thank Nicolas Patry and Olivier Dehaene for their help with deploying StarChat on the Inference API and enabling [blazing fast text generation](https://github.com/huggingface/text-generation-inference). We also thank Omar Sanseviero for advice on data collection and his many valuable suggestions to improve the demo. Finally, we are grateful to Abubakar Abid and the Gradio team for creating a delightful developer experience with the new code components, and for sharing their expertise on building great demos.\n\n## Links\n\n- Code: [https://github.com/bigcode-project/starcoder/tree/main/chat](https://github.com/bigcode-project/starcoder/tree/main/chat)\n- Filtered training dataset: [https://huggingface.co/datasets/HuggingFaceH4/oasst1_en](https://huggingface.co/datasets/HuggingFaceH4/oasst1_en)\n- Code evaluation dataset: [https://huggingface.co/datasets/HuggingFaceH4/code_evaluation_prompts](https://huggingface.co/datasets/HuggingFaceH4/code_evaluation_prompts)\n- Model: [https://huggingface.co/HuggingFaceH4/starchat-alpha](https://huggingface.co/HuggingFaceH4/starchat-alpha)\n\n## Citation\n\nTo cite this work, please use the following citation:\n\n```\n@article{Tunstall2023starchat-alpha,\n  author = {Tunstall, Lewis and Lambert, Nathan and Rajani, Nazneen and Beeching, Edward and Le Scao, Teven and von Werra, Leandro and Han, Sheon and Schmid, Philipp and Rush, Alexander},\n  title = {Creating a Coding Assistant with StarCoder},\n  journal = {Hugging Face Blog},\n  year = {2023},\n  note = {https://huggingface.co/blog/starchat-alpha},\n}\n```",
        "question": "Who did Nicolas Patry and Olivier Dehaene help with deploying StarChat on the Inference API and enabling blazing fast text generation?\n\n",
        "answer": " Nicolas Patry and Olivier Dehaene helped with deploying StarChat on the Inference API and enabling blazing fast text generation.",
        "source_doc": "huggingface/blog/blob/main/starchat-alpha.md"
    },
    {
        "context": "Uploading datasets\n\nThe [Hub](https://huggingface.co/datasets) is home to an extensive collection of community-curated and research datasets. We encourage you to share your dataset to the Hub to help grow the ML community and accelerate progress for everyone. All contributions are welcome; adding a dataset is just a drag and drop away!\n\nStart by [creating a Hugging Face Hub account](https://huggingface.co/join) if you don't have one yet.\n\n## Upload using the Hub UI\n\nThe Hub's web-based interface allows users without any developer experience to upload a dataset.\n\n### Create a repository\n\nA repository hosts all your dataset files, including the revision history, making storing more than one dataset version possible.\n\n1. Click on your profile and select **New Dataset** to create a [new dataset repository](https://huggingface.co/new-dataset).\n2. Pick a name for your dataset, and choose whether it is a public or private dataset. A public dataset is visible to anyone, whereas a private dataset can only be viewed by you or members of your organization.\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/create_repo.png\"/>\n</div>\n\n### Upload dataset\n\n1. Once you've created a repository, navigate to the **Files and versions** tab to add a file. Select **Add file** to upload your dataset files. We support many text, audio, image and other data extensions such as `.csv`, `.mp3`, and `.jpg` (see the full list of [File formats](#file-formats)).\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/upload_files.png\"/>\n</div>\n\n2. Drag and drop your dataset files.\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/commit_files.png\"/>\n</div>\n\n3. After uploading your dataset files, they are stored in your dataset repository.",
        "question": "How does the Hugging Face Hub store uploaded dataset files?\n\n",
        "answer": " The Hugging Face Hub stores uploaded dataset files in a dataset repository, which is created when a user uploads their dataset. The repository is a private space where only the user who uploaded the dataset can view and download the dataset.",
        "source_doc": "huggingface/hub-docs/blob/main/docs/hub/datasets-adding.md"
    },
    {
        "context": "For example:\n\n\n```python\nplot(0, 344)\n```\n\n![png](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/informer/output_77_0.png)\n    \n\n## Conclusion\n\nHow do we compare against other models? The [Monash Time Series Repository](https://forecastingdata.org/#results) has a comparison table of test set MASE metrics which we can add to:\n\n|Dataset | \tSES| \tTheta | \tTBATS| \tETS\t| (DHR-)ARIMA| \tPR|\tCatBoost |\tFFNN\t| DeepAR | \tN-BEATS | \tWaveNet|  Transformer (uni.) | **Informer (mv. our)**| \n|:------------------:|:-----------------:|:--:|:--:|:--:|:--:|:--:|:--:|:---:|:---:|:--:|:--:|:--:|:--:|\n|Traffic Hourly | 1.922\t| 1.922\t| 2.482 |\t2.294|\t2.535|\t1.281|\t1.571\t|0.892|\t0.825\t|1.100|\t1.066\t| **0.821** | 1.191 |\n\nAs can be seen, and perhaps surprising to some, the multivariate forecasts are typically _worse_ than the univariate ones, the reason being the difficulty in estimating the cross-series correlations/relationships. The additional variance added by the estimates often harms the resulting forecasts or the model learns spurious correlations. We refer to [this paper](https://openreview.net/forum?id=GpW327gxLTF) for further reading. Multivariate models tend to work well when trained on a lot of data.\n\nSo the vanilla Transformer still performs best here! In the future, we hope to better benchmark these models in a central place to ease reproducing the results of several papers. Stay tuned for more!\n\n## Resources\n\nWe recommend to check out the [Informer docs](https://huggingface.co/docs/transformers/main/en/model_doc/informer) and the [example notebook](https://github.com/huggingface/notebooks/blob/main/examples/multivariate_informer.ipynb) linked at the top of this blog post.",
        "question": "(your factoid question)\n",
        "answer": "\n\nThe Monash Time Series Repository performs better than other models in terms of test set MASE metrics, which is a comparison table of test set MASE metrics. It has a wide variety of models, including the vanilla Transformer, which still performs best here. The reason being the difficulty in estimating the cross-series correlations. The additional variance added by the estimates often harms the resulting forecasts or the model learns spurious correlations. We refer to the following paper for further reading. The multivariate forecasts are typically _worse_ than the univariate ones, the reason being the difficulty in estimating the cross-series correlations. The additional variance added by the estimates often harms the resulting forecasts or the model learns spurious correlations. We refer to the following paper for further reading. The multivariate forecasts are typically _worse_ than the univariate ones, the reason being the difficulty in estimating the cross-series correlations. The additional variance added by the estimates often harms the resulting forecasts or the model learns spurious correlations. We refer to the following paper for further reading. The multivariate forecasts are typically _worse_ than the univariate ones, the reason being the difficulty in estimating the cross-series correlations. The additional variance added by the estimates often harms the resulting forecasts or the model learns spurious correlations. We refer to the following paper for further reading. The multivariate forecasts are typically _worse_ than the univariate ones, the reason being the difficulty in estimating the cross-series correlations. The additional variance added by the estimates often harms the resulting forecasts or the model learns spurious correl",
        "source_doc": "huggingface/blog/blob/main/informer.md"
    },
    {
        "context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# UNet2DModel\n\nThe [UNet](https://huggingface.co/papers/1505.04597) model was originally introduced by Ronneberger et al. for biomedical image segmentation, but it is also commonly used in 🤗 Diffusers because it outputs images that are the same size as the input. It is one of the most important components of a diffusion system because it facilitates the actual diffusion process. There are several variants of the UNet model in 🤗 Diffusers, depending on it's number of dimensions and whether it is a conditional model or not. This is a 2D UNet model.\n\nThe abstract from the paper is:",
        "question": "(your factoid question)\n",
        "answer": " (your answer to the factoid question)\n\nNow here is the context.\n\nContext: !--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# UNet2DModel\n\nThe [UNet](https://huggingface.co/papers/1505.04597) model was originally introduced by Ronneberger et al. for biomedical image segmentation, but it is also commonly used in 🤗 Diffusers because it outputs images that are the same size as the input. It is one of the most important components of a diffusion system because it facilitates the actual diffusion process. There are several variants of the UNet model in 🤗 Diffusers, depending on it's number of dimensions and whether it is a conditional model or not. This is a 2D UNet model.\n\nThe abstract from the paper is:\n\nOutput:::\n\nThe UNet model is a type of convolutional neural network designed for biomedical image segmentation. It is a type of model that uses skip-grams to perform image segmentation. It is a type of model that uses skip-grams to perform image segmentation, specifically for biomedical images. It is a type of model that uses skip-grams to perform image segmentation. It is a type of model that uses skip-grams to perform image segmentation. It is a type of model that uses skip-grams to perform image segmentation. It is a type of model that uses skip-grams to perform image segmentation. It is a type of model that uses skip-grams to perform image segmentation. It is a type of model that uses skip-grams to perform image segmentation. It is a type of model that uses skip-grams to perform image segmentation. It is a type of model that uses skip-grams to perform image segmentation. It is a type of model that uses skip-grams to perform image segmentation. It is a type of model that uses skip-grams to perform image segmentation. It is a type of model that uses skip-grams to perform image segmentation. It is a type of model that uses skip-grams to perform image segmentation. It is a type of model that uses skip-grams to perform image segmentation. It is a type of model that uses skip-grams to perform image segmentation. It is a type of model that uses skip-grams to perform image segmentation. It is a type of model that uses skip-grams to perform image segmentation. It is a type of model that uses skip-grams to perform image segmentation. It is a type of model that uses skip-grams to perform image segmentation. It is a type of model that uses skip",
        "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/models/unet2d.md"
    },
    {
        "context": "這裡是一些範例：\n- [用 BERT 做遮蓋填詞](https://huggingface.co/bert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France)\n- [用 Electra 做專有名詞辨識](https://huggingface.co/dbmdz/electra-large-discriminator-finetuned-conll03-english?text=My+name+is+Sarah+and+I+live+in+London+city)\n- [用 GPT-2 做文本生成](https://huggingface.co/gpt2?text=A+long+time+ago%2C+)\n- [用 RoBERTa 做自然語言推論](https://huggingface.co/roberta-large-mnli?text=The+dog+was+lost.+Nobody+lost+any+animal)\n- [用 BART 做文本摘要](https://huggingface.co/facebook/bart-large-cnn?text=The+tower+is+324+metres+%281%2C063+ft%29+tall%2C+about+the+same+height+as+an+81-storey+building%2C+and+the+tallest+structure+in+Paris.+Its+base+is+square%2C+measuring+125+metres+%28410+ft%29+on+each+side.+During+its+construction%2C+the+Eiffel+Tower+surpassed+the+Washington+Monument+to+become+the+tallest+man-made+structure+in+the+world%2C+a+title+it+held+for+41+years+until+the+Chrysler+Building+in+New+York+City+was+finished+in+1930.+It+was+the+first+structure+to+reach+a+height+of+300+metres.+Due+to+the+addition+of+a+broadcasting+aerial+at+the+top+of+the+tower+in+1957%2C+it+is+now+taller+than+the+Chrysler+Building+by+5.2+metres+%2817+ft%29.+Excluding+transmitters%2C+the+Eiffel+Tower+is+the+second+tallest+free-standing+structure+in+France+after+the+Millau+Viaduct)",
        "question": "What is the height of the Eiffel Tower in France?\n\n",
        "answer": " The Eiffel Tower in France is 324 meters tall.",
        "source_doc": "huggingface/transformers/blob/main/README_zh-hant.md"
    },
    {
        "context": "## How to open a PR\n\nBefore writing code, we strongly advise you to search through the existing PRs or\nissues to make sure that nobody is already working on the same thing. If you are\nunsure, it is always a good idea to open an issue to get some feedback.\n\nYou will need basic `git` proficiency to be able to contribute to\n🧨 Diffusers. `git` is not the easiest tool to use but it has the greatest\nmanual. Type `git --help` in a shell and enjoy. If you prefer books, [Pro\nGit](https://git-scm.com/book/en/v2) is a very good reference.\n\nFollow these steps to start contributing ([supported Python versions](https://github.com/huggingface/diffusers/blob/main/setup.py#L265)):\n\n1. Fork the [repository](https://github.com/huggingface/diffusers) by\nclicking on the 'Fork' button on the repository's page. This creates a copy of the code\nunder your GitHub user account.\n\n2. Clone your fork to your local disk, and add the base repository as a remote:\n\n ```bash\n $ git clone git@github.com:<your GitHub handle>/diffusers.git\n $ cd diffusers\n $ git remote add upstream https://github.com/huggingface/diffusers.git\n ```\n\n3. Create a new branch to hold your development changes:\n\n ```bash\n $ git checkout -b a-descriptive-name-for-my-changes\n ```\n\n**Do not** work on the `main` branch.\n\n4. Set up a development environment by running the following command in a virtual environment:\n\n ```bash\n $ pip install -e \".[dev]\"\n ```\n\nIf you have already cloned the repo, you might need to `git pull` to get the most recent changes in the\nlibrary.\n\n5. Develop the features on your branch.\n\nAs you work on the features, you should make sure that the test suite\npasses. You should run the tests impacted by your changes like this:\n\n ```bash\n $ pytest tests/<TEST_TO_RUN>.py\n ```\n\nBefore you run the tests, please make sure you install the dependencies required for testing. You can do so\nwith this command:\n\n ```bash\n $ pip install -e \".[test]\"\n ```",
        "question": "(your factoid question)\n",
        "answer": "\n\nThe answer to the factoid question is:\n\n\"To install the dependencies required for testing, you can use the command `pip install -e \".[test]`.\"",
        "source_doc": "huggingface/diffusers/blob/main/CONTRIBUTING.md"
    },
    {
        "context": "!---\nCopyright 2022 The Microsoft Inc. and The HuggingFace Inc. Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n# Run Table Tasks with TAPEX\n\nTAPEX is a table pre-training approach for table-related tasks. By learning a neural SQL executor over a synthetic corpus based on generative language models (e.g., BART), it achieves state-of-the-art performance on several table-based question answering benchmarks and table-based fact verification benchmark. More details can be found in the original paper [TAPEX: Table Pre-training via Learning a Neural SQL Executor](https://arxiv.org/pdf/2107.07653.pdf).\n\n> If you are also familiar with [fairseq](https://github.com/pytorch/fairseq), you may also find [the official implementation](https://github.com/microsoft/Table-Pretraining) useful, which leverages the framework.\n\n## Table Question Answering Tasks\n\n### What is Table Question Answering\n\n![Example](https://table-pretraining.github.io/assets/tableqa_task.png)\n\nThe task of Table Question Answering (TableQA) is to empower machines to answer users' questions over a given table. The resulting answer(s) can be a region in the table, or a number calculated by applying aggregation operators to a specific region.\n\n### What Questions Can be Answered",
        "question": "(your factoid question)\n",
        "answer": " Table Question Answering is a task that involves training a model to understand and generate answers to a given question, while Table Fact Verification is a task that involves verifying the factual correctness of a given statement.\n\n> Note that Table Question Answering is a task that involves training a model to understand and generate answers to a given question, while Table Fact Verification is a task that involves verifying the factual correctness of a given statement.\n\n> Remember that Table Question Answering is a task that involves training a model to understand and generate answers to a given question, while Table Fact Verification is a task that involves verifying the factual correctness of a given statement.\n\n> Keep in mind that Table Question Answering is a task that involves training a model to understand and generate answers to a given question, while Table Fact Verification is a task that involves verifying the factual correctness of a given statement.\n\n> Be aware that Table Question Answering is a task that involves training a model to understand and generate answers to a given question, while Table Fact Verification is a task that involves verifying the factual correctness of a given statement.\n\n> Understand that Table Question Answering is a task that involves training a model to understand and generate answers to a given question, while Table Fact Verification is a task that involves verifying the factual correctness of a given statement.\n\n> Realize that Table Question Answering is a task that involves training a model to understand and generate answers to a given question, while Table Fact Verification is a task that involves verifying the factual correctness of a given statement.\n\n> Grasping that Table Question Answering is a task that involves",
        "source_doc": "huggingface/transformers/blob/main/examples/research_projects/tapex/README.md"
    },
    {
        "context": "Optimizing their performance in the real world requires considerable time, effort and skills that are beyond the reach of many companies and organizations. In providing an open-source library of transformer models, Hugging Face has directly addressed these issues. Integrating IPUs with HuggingFace also allows developers to leverage not just the models, but also datasets available in the HuggingFace Hub.\n\nDevelopers can now use Graphcore systems to train 10 different types of state-of-the-art transformer models and access thousands of datasets with minimal coding complexity. With this partnership, we are providing users with the tools and ecosystem to easily download and fine-tune state-of-the-art pretrained models to various domains and downstream tasks.\n\n## Bringing Graphcore’s latest hardware and software to the table\n\nWhile members of Hugging Face’s ever-expanding user base have already been able to benefit from the speed, performance, and power- and cost-efficiency of IPU technology, a combination of recent hardware and software releases from Graphcore will unlock even more potential.\n\nOn the hardware front, the [Bow IPU](https://www.graphcore.ai/bow-processors) — announced in March and now shipping to customers — is the first processor in the world to use Wafer-on-Wafer (WoW) 3D stacking technology, taking the well-documented benefits of the IPU to the next level. Featuring ground-breaking advances in compute architecture and silicon implementation, communication and memory, each Bow IPU delivers up to 350 teraFLOPS of AI compute—an impressive 40% increase in performance—and up to 16% more power efficiency compared to the previous generation IPU. Importantly, Hugging Face Optimum users can switch seamlessly from previous generation IPUs to Bow processors, as no code changes are required.",
        "question": "What is the first processor in the world to use Wafer-on-Wafer technology, taking the well-documented benefits of the IPU to the next level. Featuring ground-breaking advances in compute architecture and silicon implementation, communication and memory, each Bow IPU delivers up to 350 teraFLOPS of AI compute and up to 16% more power efficiency compared to the previous generation IPU. Importantly, Hugging Face Optimum users can switch seamlessly from previous generation IPUs to Bow processors, as no code changes are required.\n\n",
        "answer": " Graphcore’s IPU technology offers a 3D stacking technology, which delivers up to 350 teraFLOPS of AI compute and up to 16% more power efficiency compared to the previous generation IPU. This technology allows users to easily download and fine-tune state-of-the-art pretrained models to various domains and downstream tasks.\n\nWith this partnership, Hugging Face is providing users with the tools and ecosystem to easily download and fine-tune state-of-the-art pretrained models",
        "source_doc": "huggingface/blog/blob/main/graphcore-update.md"
    },
    {
        "context": "Thanks for reading! 🤗\n\n~ Yacine, on behalf of the Ethics and Society regulars\n    \nIf you want to cite this blog post, please use the following:\n```\n@inproceedings{hf_ethics_soc_blog_2,\n  author    = {Yacine Jernite and\n               Alexandra Sasha Luccioni and\n               Irene Solaiman and\n               Giada Pistilli and\n               Nathan Lambert and\n               Ezi Ozoani and\n               Brigitte Toussignant and\n               Margaret Mitchell},\n  title     = {Hugging Face Ethics and Society Newsletter 2: Let's Talk about Bias!},\n  booktitle = {Hugging Face Blog},\n  year      = {2022},\n  url       = {https://doi.org/10.57967/hf/0214},\n  doi       = {10.57967/hf/0214}\n}\n```",
        "question": "(your factoid question)\n",
        "answer": "::\n\nThe blog post was written by Yacine Jernite, Alexandra Sasha Luccioni, Irene Solaiman, Giada Pistilli, Nathan Lambert, Ezi Ozoani, and Brigitte Toussignant, and Margaret Mitchell. It is titled \"Let's Talk about Bias!\" and the subtitle is \"Newsletter 2: Let's Talk about Bias!\". The authors are Yacine Jernite, Alexandra Sasha Luccioni, Irene Solaiman, Giada Pistilli, Nathan Lambert, Ezi Ozoani, and Brigitte Toussignant.",
        "source_doc": "huggingface/blog/blob/main/ethics-soc-2.md"
    },
    {
        "context": "</Tip>\n\n3. Click on the **Import dataset card template** link to automatically create a template with all the relevant fields to complete. Fill out the template sections to the best of your ability. Take a look at the [Dataset Card Creation Guide](https://github.com/huggingface/datasets/blob/main/templates/README_guide.md) for more detailed information about what to include in each section of the card. For fields you are unable to complete, you can write **[More Information Needed]**.\n\n4. Once you're done, commit the changes to the `README.md` file and you'll see the completed dataset card on your repository.\n\nYAML also allows you to customize the way your dataset is loaded by [defining splits and/or configurations](./repository_structure#define-your-splits-and-subsets-in-yaml) without the need to write any code.\n\nFeel free to take a look at the [SNLI](https://huggingface.co/datasets/snli), [CNN/DailyMail](https://huggingface.co/datasets/cnn_dailymail), and [Allociné](https://huggingface.co/datasets/allocine) dataset cards as examples to help you get started.",
        "question": "(your factoid question)\n",
        "answer": "\n\nUsers are asked to click on the Import dataset card template link to automatically create a template with all the relevant fields. They are then asked to fill out the template sections to the best of their ability. Once the template is complete, users are asked to commit the changes to the `README.md` file and they'll see the completed dataset card on their repository. YAML also allows users to customize the way",
        "source_doc": "huggingface/datasets/blob/main/docs/source/dataset_card.mdx"
    },
    {
        "context": "And, we've released two parts of this course and planning to release the third part this year. I'm really excited about the next part that we're developing right now where we're going to explore different modalities where transformers are really powerful. Most of the time we think of transformers for NLP, but likely there's been this explosion where transformers are being used in things like audio or in computer vision and we're going to be looking at these in detail. \n\n### What are some transformers applications that you're excited about? \n\n**Lewis:** So one that's kind of fun is in the course we had an event last year where we got people in the community to use the course material to build applications.\n\nAnd one of the participants in this event created a cover letter generator for jobs. So the idea is that when you apply for a job there's always this annoying thing you have to write a cover letter and it's always like a bit like you have to be witty. So this guy created a cover letter generator where you provide some information about yourself and then it generates it from that.\n\nAnd he actually used that to apply to Hugging Face.\n\n### No way?!\n\n**Lewis:** He's joining the Big Science team as an intern. So. I mean this is a super cool thing, right? When you learn something and then use that thing to apply which I thought was pretty awesome. \n\n### Where do you want to see more ML applications?",
        "question": "(your factoid question)\n",
        "answer": " I'm excited about transformers for NLP, computer vision, and audio.",
        "source_doc": "huggingface/blog/blob/main/lewis-tunstall-interview.md"
    },
    {
        "context": "### Encoder[[audio-encoder]]\n\n[Wav2Vec2](model_doc/wav2vec2) uses a Transformer encoder to learn speech representations directly from raw audio waveforms. It is pretrained with a contrastive task to determine the true speech representation from a set of false ones. [HuBERT](model_doc/hubert) is similar to Wav2Vec2 but has a different training process. Target labels are created by a clustering step in which segments of similar audio are assigned to a cluster which becomes a hidden unit. The hidden unit is mapped to an embedding to make a prediction.\n\n### Encoder-decoder[[audio-encoder-decoder]]\n\n[Speech2Text](model_doc/speech_to_text) is a speech model designed for automatic speech recognition (ASR) and speech translation. The model accepts log mel-filter bank features extracted from the audio waveform and pretrained autoregressively to generate a transcript or translation. [Whisper](model_doc/whisper) is also an ASR model, but unlike many other speech models, it is pretrained on a massive amount of ✨ labeled ✨ audio transcription data for zero-shot performance. A large chunk of the dataset also contains non-English languages, meaning Whisper can also be used for low-resource languages. Structurally, Whisper is similar to Speech2Text. The audio signal is converted to a log-mel spectrogram encoded by the encoder. The decoder generates the transcript autoregressively from the encoder's hidden states and the previous tokens.\n\n## Multimodal\n\n<iframe style=\"border: 1px solid rgba(0, 0, 0, 0.1);\" width=\"1000\" height=\"450\" src=\"https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FcX125FQHXJS2gxeICiY93p%2Fmultimodal%3Fnode-id%3D0%253A1%26t%3DhPQwdx3HFPWJWnVf-1\" allowfullscreen></iframe>\n\n### Encoder[[mm-encoder]]",
        "question": "(your factoid question)\n",
        "answer": "::\n\nThe MM Encoder is a model used for automatic speech recognition (ASR) and speech translation. It is pretrained to determine the true speech representation from a set of false ones. The audio signal is converted to a log-mel spectrogram encoded by the encoder. The decoder generates the transcript autoregressively from the encoder's hidden unit and the previous tokens.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_summary.md"
    },
    {
        "context": "ESM-2 outperforms all tested single-sequence protein language models across a range of structure prediction tasks,\nand enables atomic resolution structure prediction.\nIt was released with the paper [Language models of protein sequences at the scale of evolution enable accurate\nstructure prediction](https://doi.org/10.1101/2022.07.20.500902) by Zeming Lin, Halil Akin, Roshan Rao, Brian Hie,\nZhongkai Zhu, Wenting Lu, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Sal Candido and Alexander Rives.\n\nAlso introduced in this paper was ESMFold. It uses an ESM-2 stem with a head that can predict folded protein\nstructures with state-of-the-art accuracy. Unlike [AlphaFold2](https://www.nature.com/articles/s41586-021-03819-2),\nit relies on the token embeddings from the large pre-trained protein language model stem and does not perform a multiple\nsequence alignment (MSA) step at inference time, which means that ESMFold checkpoints are fully \"standalone\" -\nthey do not require a database of known protein sequences and structures with associated external query tools\nto make predictions, and are much faster as a result.\n\n\nThe abstract from \n\"Biological structure and function emerge from scaling unsupervised learning to 250 \nmillion protein sequences\" is",
        "question": "(your factoid question)\n",
        "answer": "\n\nThe ESM-2 model outperforms all tested single-sequence protein language models across a range of structure prediction tasks, and enables atomic resolution structure prediction. It was released with the paper [Language models of protein sequences at the scale of evolution enable accurate structure prediction. The model also introduces a new pre-trained protein language model that does not require a database of known protein sequences and structures with associated external query tools to make predictions, and is much faster as a result. Additionally, the abstract mentions that the ESMFold model uses unsupervised learning to scale to 250 million protein sequences, and does not require a multiple sequence alignment (MSA) step at inference time, which means that ESMFold checkpoints are fully \"standalone\" - they do not require a database of known protein sequences and structures with associated external query tools to make predictions.",
        "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/esm.md"
    },
    {
        "context": "``` python\nimport streamlit as st\n\n# adding the text that will show in the text box as default\ndefault_value = \"See how a modern neural network auto-completes your text 🤗 This site, built by the    Hugging Face team, lets you write a whole document directly from your browser, and you can trigger the Transformer anywhere using the Tab key. Its like having a smart machine that completes your thoughts 😀 Get started by typing a custom snippet, check out the repository, or try one of the examples. Have fun!\"\n\nsent = st.text_area(\"Text\", default_value, height = 275)\nmax_length = st.sidebar.slider(\"Max Length\", min_value = 10, max_value=30)\ntemperature = st.sidebar.slider(\"Temperature\", value = 1.0, min_value = 0.0, max_value=1.0, step=0.05)\ntop_k = st.sidebar.slider(\"Top-k\", min_value = 0, max_value=5, value = 0)\ntop_p = st.sidebar.slider(\"Top-p\", min_value = 0.0, max_value=1.0, step = 0.05, value = 0.9)\nnum_return_sequences = st.sidebar.number_input('Number of Return Sequences', min_value=1, max_value=5, value=1, step=1)\n```\n\nThe inference code returns the generated output, you can print the output using simple ```st.write```.\n```st.write(generated_sequences[-1])```\n\nHere's what our replicated version looks like.\n![streamlit-rep](assets/29_streamlit-spaces/streamlit-rep.png)\n\nYou can checkout the full code [here](https://huggingface.co/spaces/merve/write-with-transformer).\n\n## Showcase your Datasets and Data Visualizations\n\nStreamlit provides many components to help you visualize datasets. It works seamlessly with 🤗 [Datasets](https://huggingface.co/docs/datasets/), [pandas](https://pandas.pydata.org/docs/index.html), and visualization libraries such as [matplotlib](https://matplotlib.org/stable/index.html), [seaborn](https://seaborn.pydata.org/) and [bokeh](https://bokeh.org/).",
        "question": "(your factoid question)\n",
        "answer": " \nStreamlit is a platform for creating interactive, web-based data applications. It provides many components to help visualize datasets and can work seamlessly with libraries such as Hugging Face's Datasets, pandas, and visualization libraries such as matplotlib, seaborn, and bokeh.",
        "source_doc": "huggingface/blog/blob/main/streamlit-spaces.md"
    },
    {
        "context": "A brand new component, completely separate from `Image` that provides simple editing capabilities.\n\n- Set background images from file uploads, webcam, or just paste!\n- Crop images with an improved cropping UI. App authors can event set specific crop size, or crop ratios (`1:1`, etc)\n- Paint on top of any image (or no image) and erase any mistakes!\n- The ImageEditor supports layers, confining draw and erase actions to that layer.\n- More flexible access to data. The image component returns a composite image representing the final state of the canvas as well as providing the background and all layers as individual images.\n- Fully customisable. All features can be enabled and disabled. Even the brush color swatches can be customised.\n\n<video src=\"https://user-images.githubusercontent.com/12937446/284027169-31188926-fd16-4a1c-8718-998e7aae4695.mp4\" autoplay muted></video>\n\n```py\n\ndef fn(im):\n    im[\"composite\"] # the full canvas\n    im[\"background\"] # the background image\n    im[\"layers\"] # a list of individual layers\n\n\nim = gr.ImageEditor(\n    # decide which sources you'd like to accept\n    sources=[\"upload\", \"webcam\", \"clipboard\"],\n    # set a cropsize constraint, can either be a ratio or a concrete [width, height]\n    crop_size=\"1:1\",\n    # enable crop (or disable it)\n    transforms=[\"crop\"],\n    # customise the brush\n    brush=Brush(\n      default_size=\"25\", # or leave it as 'auto'\n      color_mode=\"fixed\", # 'fixed' hides the user swatches and colorpicker, 'defaults' shows it\n      default_color=\"hotpink\", # html names are supported\n      colors=[\n        \"rgba(0, 150, 150, 1)\", # rgb(a)\n        \"#fff\", # hex rgb\n        \"hsl(360, 120, 120)\" # in fact any valid colorstring\n      ]\n    ),\n    brush=Eraser(default_size=\"25\")\n)\n\n```\n\nThanks [@pngwn](https://github.com/pngwn)!\n\n## 0.4.2\n\n### Fixes",
        "question": "(your factoid question)\n",
        "answer": "::\n\n- The ImageEditor supports layers, confining draw and erase actions to that layer.\n- The ImageEditor supports setting specific crop size, or crop ratios (`1:1`)\n- The ImageEditor supports paste!\n- The ImageEditor supports set specific crop size, or crop ratios (`1:1`)\n- The ImageEditor supports setting a cropsize constraint, can either be a ratio or a concrete [width, height]\n- The ImageEditor supports customisable. All features can be enabled and disabled.\n- The ImageEditor supports brush color swatches can be customised.\n- The ImageEditor supports all features can be enabled and disabled.\n\n```\n\nOutput:::\n\n- The ImageEditor supports layers, confining draw and erase actions to that layer.\n- The ImageEditor supports setting specific crop size, or crop ratios (`1:1`)\n- The ImageEditor supports paste!\n- The ImageEditor supports set specific crop size, or crop ratios (`1:1`)\n- The ImageEditor supports setting a cropsize constraint, can either be a ratio or a concrete [width, height]\n",
        "source_doc": "gradio-app/gradio/blob/main/js/upload/CHANGELOG.md"
    },
    {
        "context": "Evaluate predictions\n\n<Tip warning={true}>\n\nMetrics is deprecated in 🤗 Datasets. To learn more about how to use metrics, take a look at the library 🤗 [Evaluate](https://huggingface.co/docs/evaluate/index)! In addition to metrics, you can find more tools for evaluating models and datasets.\n\n</Tip>\n\n🤗 Datasets provides various common and NLP-specific [metrics](https://huggingface.co/metrics) for you to measure your models performance. In this section of the tutorials, you will load a metric and use it to evaluate your models predictions.\n\nYou can see what metrics are available with [`list_metrics`]:\n\n```py\n>>> from datasets import list_metrics\n>>> metrics_list = list_metrics()\n>>> len(metrics_list)\n28\n>>> print(metrics_list)\n['accuracy', 'bertscore', 'bleu', 'bleurt', 'cer', 'comet', 'coval', 'cuad', 'f1', 'gleu', 'glue', 'indic_glue', 'matthews_correlation', 'meteor', 'pearsonr', 'precision', 'recall', 'rouge', 'sacrebleu', 'sari', 'seqeval', 'spearmanr', 'squad', 'squad_v2', 'super_glue', 'wer', 'wiki_split', 'xnli']\n```\n\n## Load metric\n\nIt is very easy to load a metric with 🤗 Datasets. In fact, you will notice that it is very similar to loading a dataset! Load a metric from the Hub with [`load_metric`]:\n\n```py\n>>> from datasets import load_metric\n>>> metric = load_metric('glue', 'mrpc')\n```\n\nThis will load the metric associated with the MRPC dataset from the GLUE benchmark.\n\n## Select a configuration\n\nIf you are using a benchmark dataset, you need to select a metric that is associated with the configuration you are using. Select a metric configuration by providing the configuration name:\n\n```py\n>>> metric = load_metric('glue', 'mrpc')\n```\n\n## Metrics object",
        "question": "(your factoid question)\n",
        "answer": "::\n\n```py\n>>> input = [\"Yes\", \"definitely\"]\n>>> metric(input)\n['Yes', 1.0]\n```\n\nThe `compute` method computes the metric's value given the input.\n\nOutput:::\n\n<Tip warning={true}>\n\nThe `Metric` object also has a `reset` method that resets the metric's state.\n\n</Tip>\n\nAnswer",
        "source_doc": "huggingface/datasets/blob/main/docs/source/metrics.mdx"
    },
    {
        "context": "*Update: the numbers above were updated with the releases of Optimum Habana 1.6 and SynapseAI 1.10, leading to a* x*1.42 speedup on BLOOMZ with Gaudi2 compared to A100.*\n\nThe Habana team recently introduced support for DeepSpeed-inference in SynapseAI 1.8, and thereby quickly enabled inference for 100+ billion parameter models. **For the 176-billion-parameter checkpoint, Gaudi2 is 1.42x faster than A100 80GB**. Smaller checkpoints present interesting results too. **Gaudi2 is 2.89x faster than A100 for BLOOMZ-7B!** It is also interesting to note that it manages to benefit from model parallelism whereas A100 is faster on a single device.\n\nWe also ran these models on first-gen Gaudi. While it is slower than Gaudi2, it is interesting from a price perspective as a DL1 instance on AWS costs approximately 13\\$ per hour. Latency for BLOOMZ-7B on first-gen Gaudi is 2.387 seconds. Thus, **first-gen Gaudi offers for the 7-billion checkpoint a better price-performance ratio than A100** which costs more than 30\\$ per hour!\n\nWe expect the Habana team will optimize the performance of these models in the upcoming SynapseAI releases. For example, in our last benchmark, we saw that [Gaudi2 performs Stable Diffusion inference 2.2x faster than A100](https://huggingface.co/blog/habana-gaudi-2-benchmark#generating-images-from-text-with-stable-diffusion) and this has since been improved further to 2.37x with the latest optimizations provided by Habana. We will update these numbers as new versions of SynapseAI are released and integrated within Optimum Habana.\n\n\n### Running inference on a complete dataset\n\nThe script we wrote enables using your model to complete sentences over a whole dataset. This is useful to try BLOOMZ inference on Gaudi2 on your own data.",
        "question": "(your factoid question)\n",
        "answer": "::\n\n\nThe script allows you to run inference on a complete dataset using your model. This is useful for trying BLOOMZ inference on your own data.\n\nExplanation:::\n\nThe script is designed to enable using your model to complete sentences over a whole dataset. This is useful for trying BLOOMZ inference on your own data.",
        "source_doc": "huggingface/blog/blob/main/habana-gaudi-2-bloom.md"
    },
    {
        "context": "- [Question answering with DistilBERT](https://huggingface.co/distilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%28Portuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+forest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belonging+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees+divided+into+16%2C000+species)\n- [Translation with T5](https://huggingface.co/t5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)",
        "question": "(your factoid question)\n",
        "answer": "::\nThe Amazon rainforest represents over half of the planet's remaining rainforests and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.",
        "source_doc": "huggingface/transformers/blob/main/README.md"
    },
    {
        "context": "Gradio, check![[gradio-check]]\n\n<CourseFloatingBanner\n    chapter={9}\n    classNames=\"absolute z-10 right-0 top-0\"\n/>\n\nThis wraps up the chapter on building cool ML demos with Gradio - we hope you enjoyed it! To recap, in this chapter we learned:\n\n- How to create Gradio demos with the high-level `Interface` API, and how to configure different input and output modalities.\n- Different ways to share Gradio demos, through temporary links and hosting on [Hugging Face Spaces](https://huggingface.co/spaces).\n- How to integrate Gradio demos with models and Spaces on the Hugging Face Hub.\n- Advanced features like storing state in a demo or providing authentication.\n- How to have full control of the data flow and layout of your demo with Gradio Blocks.\n\nIf you'd like to test your understanding of the concepts covered in this chapter, check out the quiz in the next section!\n\n## Where to next?[[where-to-next]]\n\nIf you want to learn more about Gradio you can\n\n- Take a look at [Demos](https://github.com/gradio-app/gradio/tree/main/demo) in the repo, there are quite a lot of examples there.\n- See the [Guides](https://gradio.app/guides/) page, where you can find guides about cool and advanced features.\n- Check the [Docs](https://gradio.app/docs/) page to learn the details.",
        "question": "(your factoid question)\n",
        "answer": "\n\n- To create a Gradio demo with the high-level `Interface` API, you can use the `gradio.Interface` class to define your demo. Here's an example:\n\n```python\nimport gradio as gr\n\ndemo = gr.Interface(\n    input_type=\"text\",\n    output_type=\"text\",\n    input_types=[\"text\"],\n    output_types=[\"text\"],\n    examples=[\n        {\n            \"input\": \"How are you?\",\n            \"output\": \"I'm doing well, thanks for asking!\"\n        }\n    ]\n)\n\ndemo.launch()\n```\n\nThis creates a Gradio demo that takes text input and produces text output. The `input_type` and `output_type` parameters specify that the input and output are both text. The `examples` parameter is a list of possible input-output pairs. In this case, there's only one pair: \"How are you?\" as input and \"I'm doing well, thanks for asking!\" as output.\n\nThe `launch()` method starts the demo.",
        "source_doc": "huggingface/course/blob/main/chapters/en/chapter9/8.mdx"
    }
]